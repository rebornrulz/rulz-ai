{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zX6H255Agj68"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-research/text-to-text-transfer-transfrormer/blob/main/notebooks/t5-deploy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yo_HOomXe1f2"
      },
      "source": [
        "##### Copyright 2020 The T5 Authors\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Rz9fAJ8PexKB"
      },
      "outputs": [],
      "source": [
        "# Copyright 2020 The T5 Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "geoZEiaLdGfR"
      },
      "source": [
        "# T5 SavedModel Export and Inference\n",
        "\n",
        "This notebook guides you through the process of exporting a [T5](https://github.com/google-research/text-to-text-transformer) `SavedModel` for inference. It uses the fine-tuned checkpoints in the [T5 Closed Book QA](https://github.com/google-research/google-research/tree/main/t5_closed_book_qa) repository for the [Natural Questions](https://ai.google.com/research/NaturalQuestions/) task as an example, but the same process will work for any model trained with the `t5` library.\n",
        "\n",
        "For more general usage of the `t5` library, please see the main [github repo](https://github.com/google-research/text-to-text-transfer-transformer) and fine-tuning [colab notebook](https://goo.gle/t5-colab).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WtS5hODBKtR_"
      },
      "source": [
        "## Install T5 Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UHCx-R4M-D0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Cannot install t5==0.0.0, t5==0.1.0, t5==0.1.1, t5==0.1.2, t5==0.1.3, t5==0.1.4, t5==0.1.5, t5==0.1.6, t5==0.1.7, t5==0.2.0, t5==0.3.0, t5==0.3.2, t5==0.4.0, t5==0.4.1, t5==0.5.0, t5==0.6.0, t5==0.6.1, t5==0.6.2, t5==0.6.3, t5==0.6.4, t5==0.7.0, t5==0.7.1, t5==0.8.0, t5==0.8.1, t5==0.9.0, t5==0.9.1, t5==0.9.2 and t5==0.9.3 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mCloning into 'google-research'...\n",
            "remote: Enumerating objects: 19294, done.\u001b[K\n",
            "remote: Counting objects: 100% (19294/19294), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15281/15281), done.\u001b[K\n",
            "remote: Total 19294 (delta 3158), reused 12954 (delta 2933), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (19294/19294), 421.62 MiB | 15.21 MiB/s, done.\n",
            "Resolving deltas: 100% (3158/3158), done.\n",
            "Updating files: 100% (18602/18602), done.\n",
            "env: PYTHONPATH=\"/content/google-research/:/content/google-research/t5_closed_book_qa:${PYTHONPATH}\"\n"
          ]
        }
      ],
      "source": [
        "!pip install -q t5\n",
        "!git clone https://github.com/google-research/google-research.git --depth=1\n",
        "# Add closed-book qa library to Python path.\n",
        "%env PYTHONPATH=\"/content/google-research/:/content/google-research/t5_closed_book_qa:${PYTHONPATH}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IFuyCiHpLCh7"
      },
      "source": [
        "## Export `SavedModel` to local storage\n",
        "\n",
        "NOTE: This will take a while for XL and XXL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "-Y7QSepo9a8H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: t5_mesh_transformer\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/small_ssm_nq'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/rulz-ai/Rulz-AI/colabs/wandb-model-registry/t5-deploy.ipynb Cell 8\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rulz-ai/Rulz-AI/colabs/wandb-model-registry/t5-deploy.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m saved_model_dir \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/content/\u001b[39m\u001b[39m{\u001b[39;00mMODEL\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rulz-ai/Rulz-AI/colabs/wandb-model-registry/t5-deploy.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mt5_mesh_transformer    --module_import=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mt5_cbqa.tasks\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m    --model_dir=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgs://t5-data/pretrained_models/cbqa/\u001b[39m\u001b[39m{MODEL}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m    --use_model_api    --mode=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexport_predict\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m    --export_dir=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{saved_model_dir}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rulz-ai/Rulz-AI/colabs/wandb-model-registry/t5-deploy.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m saved_model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(saved_model_dir, \u001b[39mmax\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(saved_model_dir)))\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/small_ssm_nq'"
          ]
        }
      ],
      "source": [
        "MODEL = \"small_ssm_nq\" #@param[\"small_ssm_nq\", \"t5.1.1.xl_ssm_nq\", \"t5.1.1.xxl_ssm_nq\"]\n",
        "\n",
        "import os\n",
        "\n",
        "saved_model_dir = f\"/content/{MODEL}\"\n",
        "\n",
        "!t5_mesh_transformer \\\n",
        "  --module_import=\"t5_cbqa.tasks\" \\\n",
        "  --model_dir=\"gs://t5-data/pretrained_models/cbqa/{MODEL}\" \\\n",
        "  --use_model_api \\\n",
        "  --mode=\"export_predict\" \\\n",
        "  --export_dir=\"{saved_model_dir}\"\n",
        "\n",
        "saved_model_path = os.path.join(saved_model_dir, max(os.listdir(saved_model_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JUH5BkcYK3At"
      },
      "source": [
        "## Load `SavedModel` and create helper functions for inference\n",
        "\n",
        "NOTE: This will take a while for XL and XXL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xiBSnGuu-em0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text  # Required to run exported model.\n",
        "\n",
        "model = tf.saved_model.load(saved_model_path, [\"serve\"])\n",
        "\n",
        "def predict_fn(x):\n",
        " return model.signatures['serving_default'](tf.constant(x))['outputs'].numpy()\n",
        "\n",
        "def answer(question):\n",
        "  return predict_fn([question])[0].decode('utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HSlRnhz7VpTu"
      },
      "source": [
        "## Ask some questions\n",
        "\n",
        "We must prefix each question with the `nq question:` prompt since T5 is a multitask model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CE1bO4hw--Zh"
      },
      "outputs": [],
      "source": [
        "for question in [\"nq question: where is google's headquarters\",\n",
        "                 \"nq question: what is the most populous country in the world\",\n",
        "                 \"nq question: name a member of the beatles\",\n",
        "                 \"nq question: how many teeth do humans have\"]:\n",
        "    print(answer(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BlTOrC7iaCZD"
      },
      "source": [
        "## Package in Docker image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ndK6zIryaKTX"
      },
      "source": [
        "```bash\n",
        "MODEL_NAME=model-name\n",
        "SAVED_MODEL_PATH=/path/to/export/dir\n",
        "\n",
        "# Download the TensorFlow Serving Docker image and repo:\n",
        "docker pull tensorflow/serving:nightly\n",
        "\n",
        "# First, run a serving image as a daemon:\n",
        "docker run -d --name serving_base tensorflow/serving:nightly\n",
        "\n",
        "# Next, copy your `SavedModel` to the container's model folder:\n",
        "docker cp $SAVED_MODEL_PATH serving_base:/models/$MODEL_NAME\n",
        "\n",
        "# Now, commit the container that's serving your model:\n",
        "docker commit --change \"ENV MODEL_NAME ${MODEL_NAME}\" serving_base $MODEL_NAME\n",
        "\n",
        "# Finally, save the image to a tar file:\n",
        "docker save $MODEL_NAME -o $MODEL_NAME.tar\n",
        "\n",
        "# You can now stop `serving_base`:\n",
        "docker kill serving_base\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WYQiZke3nXD_"
      },
      "source": [
        "```bash\n",
        "docker run -t --rm -p 8501:8501 --name $MODEL_NAME-server $MODEL_NAME &\n",
        "\n",
        "curl -d '{\"inputs\": [\"nq question: what is the most populous country?\"]}' \\\n",
        "    -X POST http://localhost:8501/v1/models/$MODEL_NAME:predict\n",
        "\n",
        "docker stop $MODEL_NAME-server\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Yo_HOomXe1f2"
      ],
      "name": "t5-deploy",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
