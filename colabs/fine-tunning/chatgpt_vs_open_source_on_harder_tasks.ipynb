{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "**Note: the text in [the blog post](https://medium.com/@marcotcr/exploring-chatgpt-vs-open-source-models-on-slightly-harder-tasks-aa0395c31610) diverges from this blog post (it's better), but this notebook has the code and examples for it.**  \n",
    "\n",
    "Open-source LLMs like [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) and [MPT](https://www.mosaicml.com/blog/mpt-7b#building-with-mosaicml-platform) are popping up all over the place.  \n",
    "There's been [discussion](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) about how these models compare to commercial LLMs like ChatGPT or Bard, but most of the comparison has been around answers to simple questions.  \n",
    "\n",
    "As an example, the folks at [LMSYSOrg](https://lmsys.org/) did [an interesting analysis](https://lmsys.org/blog/2023-03-30-vicuna/) (+1 for being automated and reproducible) comparing Vicuna-13B to ChatGPT on various short questions, which is great as a comparison of the models as simple chatbots. However, many interesting ways of using these LLMs typically require complex instructions and/or multi-turn conversations, and some prompt engineering.\n",
    "We think that in the 'real world', most people will want to compare different LLM offerings on _their_ problem, with a variety of different prompts.  \n",
    "\n",
    "This blog post is an example of what such an exploration might look like, comparing two open source models (Vicuna-13B, MPT-7b-Chat) with ChatGPT on tasks of varying complexity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup: Solving equations\n",
    "By way of warmup, let's start with a toy task (solving simple polynomial equations), where we can check the output for correctness and shouldn't need much prompt engineering.  \n",
    "This will be similar to the Math category in [here](https://lmsys.org/blog/2023-03-30-vicuna/), with the difference that we will evaluate models as correct / incorrect on ground truth, rather than using GPT-4 to rate the output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple function that generates a polynomial with distinct integer roots.\n",
    "This will give us both the input and the ground truth output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roots [-8, 8]\n",
      "x^2 - 64.0 = 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def random_polynomial(n_roots, low=1, high=100):\n",
    "    roots = [np.random.randint(low, high) for _ in range(n_roots)]\n",
    "    # unique roots only\n",
    "    while len(set(roots)) != n_roots:\n",
    "        roots = [np.random.randint(low, high) for _ in range(n_roots)]\n",
    "    poly = np.polynomial.polynomial.Polynomial.fromroots(roots)\n",
    "    a = poly.coef.copy()\n",
    "    a = a[::-1]\n",
    "    text = \"\"\n",
    "    for i, coef in enumerate(a):\n",
    "        if coef == 0:\n",
    "            continue\n",
    "        sign = \" + \" if coef > 0 else \" - \"\n",
    "        if i == 0:\n",
    "            sign = \"\"\n",
    "        elif coef < 0:\n",
    "            coef = -coef\n",
    "        if i == len(a) - 1:\n",
    "            text += f\"{sign}{coef}\"\n",
    "        else:\n",
    "            if coef == 1:\n",
    "                coef = \"\"\n",
    "            power = f\"^{len(a) - i - 1}\" if len(a) - i - 1 > 1 else \"\"\n",
    "            text += f\"{sign}{coef}x{power}\"\n",
    "    text += \" = 0\"\n",
    "    return roots, text\n",
    "\n",
    "\n",
    "roots, equation = random_polynomial(2, low=-10, high=10)\n",
    "print(\"Roots\", roots)\n",
    "print(equation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the models (we use [`guidance`](https://github.com/microsoft/guidance) throughout for easy comparison and control):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'guidance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mguidance\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n\u001b[0;32m      4\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/marcotcr/.cache/huggingface/hub/vicuna-13b\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'guidance'"
     ]
    }
   ],
   "source": [
    "import guidance\n",
    "import transformers\n",
    "\n",
    "path = \"/home/marcotcr/.cache/huggingface/hub/vicuna-13b\"\n",
    "mpt = guidance.llms.transformers.MPTChat(\"mosaicml/mpt-7b-chat\", device=1)\n",
    "vicuna = guidance.llms.transformers.Vicuna(path, device_map=\"auto\")\n",
    "chatgpt = guidance.llms.OpenAI(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick digression on different syntaxes**: each of these models have a their own _chat syntax_, e.g. here is how the same conversation would look like in Vicuna and MPT (where `[generated response]` is where the model would put its output):  \n",
    "\n",
    "Vicuna:  \n",
    "```\n",
    "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.  \n",
    "USER: Can you please solve the following equation? x^2 + 2x + 1 = 0  \n",
    "ASSISTANT: [generated response] </s>\n",
    "```\n",
    "\n",
    "MPT:  \n",
    "```\n",
    "<|im_start|>system\n",
    "- You are a helpful assistant chatbot trained by MosaicML.  \n",
    "- You answer questions.\n",
    "- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.\"\"\"\n",
    "<|im_end|>\n",
    "<|im_start|>user Can you please solve the following equation? x^2 + 2x + 1 = 0<|im_end|>\n",
    "<|im_start|>assistant [generated response]<|im_end>\n",
    "```\n",
    "\n",
    "To avoid the tediousness translating between these, `guidance` supports a unified chat syntax that gets translated to the model-specific syntax when calling the model.  \n",
    "Here is the prompt we'll use for all models (note how we use ``{{system}}``, ``{{user}}`` and ``{{assistant}}`` tags rather than model-specific separators):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'guidance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m find_roots \u001b[39m=\u001b[39m guidance(\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m{{~#system~}}\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m{{llm.default_system_prompt}}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m{{~/system}}\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mPlease find the roots of the following equation: {{equation}}\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mThink step by step, find the roots, and then say:\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mROOTS = [root1, root2...]\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mFor example, if the roots are 1.3 and 2.2, say ROOTS = [1.3, 2.2].\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mMake sure to use real numbers, not fractions.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m{{~/user}}\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[39m{{#assistant~}}\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m{{gen 'answer' temperature=0}}\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m{{~/assistant~}}\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'guidance' is not defined"
     ]
    }
   ],
   "source": [
    "find_roots = guidance(\n",
    "    \"\"\"\n",
    "{{~#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "Please find the roots of the following equation: {{equation}}\n",
    "Think step by step, find the roots, and then say:\n",
    "ROOTS = [root1, root2...]\n",
    "For example, if the roots are 1.3 and 2.2, say ROOTS = [1.3, 2.2].\n",
    "Make sure to use real numbers, not fractions.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}\"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the prompt on a very simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_polynomial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m roots, equation \u001b[39m=\u001b[39m random_polynomial(\u001b[39m2\u001b[39m, low\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m, high\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(roots)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(equation)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random_polynomial' is not defined"
     ]
    }
   ],
   "source": [
    "roots, equation = random_polynomial(2, low=-3, high=3)\n",
    "print(roots)\n",
    "print(equation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_roots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m answer_gpt \u001b[39m=\u001b[39m find_roots(llm\u001b[39m=\u001b[39mchatgpt, equation\u001b[39m=\u001b[39mequation)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'find_roots' is not defined"
     ]
    }
   ],
   "source": [
    "answer_gpt = find_roots(llm=chatgpt, equation=equation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_roots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m answer_vicuna \u001b[39m=\u001b[39m find_roots(llm\u001b[39m=\u001b[39mvicuna, equation\u001b[39m=\u001b[39mequation)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'find_roots' is not defined"
     ]
    }
   ],
   "source": [
    "answer_vicuna = find_roots(llm=vicuna, equation=equation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_roots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m answer_mpt \u001b[39m=\u001b[39m find_roots(llm\u001b[39m=\u001b[39mmpt, equation\u001b[39m=\u001b[39mequation)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'find_roots' is not defined"
     ]
    }
   ],
   "source": [
    "answer_mpt = find_roots(llm=mpt, equation=equation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT got it right, while Vicuna and MPT got it wrong (Vicuna didn't even follow the specified format).\n",
    "Let's write a simple regex to parse the output, so we can evaluate this on a few more expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer_gpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m     roots \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39msorted\u001b[39m(roots))\n\u001b[0;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mall(np\u001b[39m.\u001b[39mabs(gt \u001b[39m-\u001b[39m roots) \u001b[39m<\u001b[39m \u001b[39m1e-3\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mChatGPT: \u001b[39m\u001b[39m\"\u001b[39m, matches_groundtruth(parse_roots(answer_gpt[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]), roots))\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mVicuna: \u001b[39m\u001b[39m\"\u001b[39m, matches_groundtruth(parse_roots(answer_vicuna[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]), roots))\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMPT: \u001b[39m\u001b[39m\"\u001b[39m, matches_groundtruth(parse_roots(answer_mpt[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]), roots))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'answer_gpt' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def parse_roots(text):\n",
    "    roots = re.search(r\"ROOTS = \\[(.*)\\]\", text)\n",
    "    if not roots:\n",
    "        return []\n",
    "    roots = roots.group(1).split(\",\")\n",
    "    try:\n",
    "        roots = [float(r) for r in roots]\n",
    "    except:\n",
    "        roots = []\n",
    "    return roots\n",
    "\n",
    "\n",
    "def matches_groundtruth(roots, groundtruth):\n",
    "    if len(roots) != len(groundtruth):\n",
    "        return False\n",
    "    gt = np.array(sorted(groundtruth))\n",
    "    roots = np.array(sorted(roots))\n",
    "    return np.all(np.abs(gt - roots) < 1e-3)\n",
    "\n",
    "\n",
    "print(\"ChatGPT: \", matches_groundtruth(parse_roots(answer_gpt[\"answer\"]), roots))\n",
    "print(\"Vicuna: \", matches_groundtruth(parse_roots(answer_vicuna[\"answer\"]), roots))\n",
    "print(\"MPT: \", matches_groundtruth(parse_roots(answer_mpt[\"answer\"]), roots))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the prompt on quadratic equations with roots between -20 and 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_polynomial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m correct \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mChatGPT\u001b[39m\u001b[39m\"\u001b[39m: [], \u001b[39m\"\u001b[39m\u001b[39mVicuna\u001b[39m\u001b[39m\"\u001b[39m: [], \u001b[39m\"\u001b[39m\u001b[39mMPT\u001b[39m\u001b[39m\"\u001b[39m: []}\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     roots, equation \u001b[39m=\u001b[39m random_polynomial(\u001b[39m2\u001b[39m, low\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m, high\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m      5\u001b[0m     answer_gpt \u001b[39m=\u001b[39m find_roots(llm\u001b[39m=\u001b[39mchatgpt, equation\u001b[39m=\u001b[39mequation, silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m     answer_vicuna \u001b[39m=\u001b[39m find_roots(llm\u001b[39m=\u001b[39mvicuna, equation\u001b[39m=\u001b[39mequation, silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random_polynomial' is not defined"
     ]
    }
   ],
   "source": [
    "answers = {\"ChatGPT\": [], \"Vicuna\": [], \"MPT\": []}\n",
    "correct = {\"ChatGPT\": [], \"Vicuna\": [], \"MPT\": []}\n",
    "for _ in range(20):\n",
    "    roots, equation = random_polynomial(2, low=-20, high=20)\n",
    "    answer_gpt = find_roots(llm=chatgpt, equation=equation, silent=True)\n",
    "    answer_vicuna = find_roots(llm=vicuna, equation=equation, silent=True)\n",
    "    answer_mpt = find_roots(llm=mpt, equation=equation, silent=True)\n",
    "    answers[\"ChatGPT\"].append(answer_gpt)\n",
    "    answers[\"Vicuna\"].append(answer_vicuna)\n",
    "    answers[\"MPT\"].append(answer_mpt)\n",
    "    correct[\"ChatGPT\"].append(\n",
    "        matches_groundtruth(parse_roots(answer_gpt[\"answer\"]), roots)\n",
    "    )\n",
    "    correct[\"Vicuna\"].append(\n",
    "        matches_groundtruth(parse_roots(answer_vicuna[\"answer\"]), roots)\n",
    "    )\n",
    "    correct[\"MPT\"].append(matches_groundtruth(parse_roots(answer_mpt[\"answer\"]), roots))\n",
    "\n",
    "print(\"Frequency of correct answers:\")\n",
    "print(\"ChatGPT: \", np.mean(correct[\"ChatGPT\"]))\n",
    "print(\"Vicuna: \", np.mean(correct[\"Vicuna\"]))\n",
    "print(\"MPT: \", np.mean(correct[\"MPT\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT gets the right roots 80% of the time, while Vicuna and MPT never get them right. Let's see a few errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m answers[\u001b[39m\"\u001b[39;49m\u001b[39mVicuna\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "answers[\"Vicuna\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m answers[\u001b[39m\"\u001b[39;49m\u001b[39mMPT\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "answers[\"MPT\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna does the math wrong, while MPT does not even attempt to solve it step by step.  \n",
    "ChatGPT also makes some mistakes, often involving some math substep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m error \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(np\u001b[39m.\u001b[39marray(correct[\u001b[39m\"\u001b[39m\u001b[39mChatGPT\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m answers[\u001b[39m\"\u001b[39m\u001b[39mChatGPT\u001b[39m\u001b[39m\"\u001b[39m][error]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "error = np.where(np.array(correct[\"ChatGPT\"]) == False)[0][0]\n",
    "answers[\"ChatGPT\"][error]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Vicuna and MPT failed on quadratic equations, let's look at even simpler equations, such as `x - 10 = 0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'guidance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m find_solution \u001b[39m=\u001b[39m guidance(\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m{{#system~}}\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m{{llm.default_system_prompt}}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m{{~/system}}\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mPlease find the solution to the following equation: {{equation}}\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mThink step by step, find the solution, and then say:\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mSOLUTION = [value]\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mFor example, if the solution is x=3, say SOLUTION = [3].\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mMake sure to use real numbers, not fractions.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m{{/user}}\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[39m{{#assistant~}}\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m{{gen 'answer' temperature=0 max_tokens=300}}\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m{{~/assistant~}}\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'guidance' is not defined"
     ]
    }
   ],
   "source": [
    "find_solution = guidance(\n",
    "    \"\"\"\n",
    "{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "Please find the solution to the following equation: {{equation}}\n",
    "Think step by step, find the solution, and then say:\n",
    "SOLUTION = [value]\n",
    "For example, if the solution is x=3, say SOLUTION = [3].\n",
    "Make sure to use real numbers, not fractions.\n",
    "{{/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0 max_tokens=300}}\n",
    "{{~/assistant~}}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_solution(text):\n",
    "    solution = re.search(r\"SOLUTION = \\[(.*)\\]\", text)\n",
    "    if not solution:\n",
    "        return []\n",
    "    try:\n",
    "        return [float(solution.group(1))]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random_polynomial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[1;32m----> 5\u001b[0m     roots, equation \u001b[39m=\u001b[39m random_polynomial(\u001b[39m1\u001b[39m, low\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m, high\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m      6\u001b[0m     answer_gpt \u001b[39m=\u001b[39m find_solution(llm\u001b[39m=\u001b[39mchatgpt, equation\u001b[39m=\u001b[39mequation, silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m     answer_vicuna \u001b[39m=\u001b[39m find_solution(llm\u001b[39m=\u001b[39mvicuna, equation\u001b[39m=\u001b[39mequation, silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random_polynomial' is not defined"
     ]
    }
   ],
   "source": [
    "correct1d = {\"ChatGPT\": [], \"Vicuna\": [], \"MPT\": []}\n",
    "answers1d = {\"ChatGPT\": [], \"Vicuna\": [], \"MPT\": []}\n",
    "for i in range(20):\n",
    "    print(i)\n",
    "    roots, equation = random_polynomial(1, low=-20, high=20)\n",
    "    answer_gpt = find_solution(llm=chatgpt, equation=equation, silent=False)\n",
    "    answer_vicuna = find_solution(llm=vicuna, equation=equation, silent=False)\n",
    "    answer_mpt = find_solution(llm=mpt, equation=equation, silent=False)\n",
    "    answers1d[\"ChatGPT\"].append(answer_gpt)\n",
    "    answers1d[\"Vicuna\"].append(answer_vicuna)\n",
    "    answers1d[\"MPT\"].append(answer_mpt)\n",
    "    correct1d[\"ChatGPT\"].append(\n",
    "        matches_groundtruth(parse_solution(answer_gpt[\"answer\"]), roots)\n",
    "    )\n",
    "    correct1d[\"Vicuna\"].append(\n",
    "        matches_groundtruth(parse_solution(answer_vicuna[\"answer\"]), roots)\n",
    "    )\n",
    "    correct1d[\"MPT\"].append(\n",
    "        matches_groundtruth(parse_solution(answer_mpt[\"answer\"]), roots)\n",
    "    )\n",
    "\n",
    "print(\"Frequency of correct answers:\")\n",
    "print(\"ChatGPT: \", np.mean(correct1d[\"ChatGPT\"]))\n",
    "print(\"Vicuna: \", np.mean(correct1d[\"Vicuna\"]))\n",
    "print(\"MPT: \", np.mean(correct1d[\"MPT\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, MPT still fails to solve these. Vicuna still makes some mistakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m error \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(np\u001b[39m.\u001b[39marray(correct1d[\u001b[39m\"\u001b[39m\u001b[39mVicuna\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m answers1d[\u001b[39m\"\u001b[39m\u001b[39mVicuna\u001b[39m\u001b[39m\"\u001b[39m][error]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "error = np.where(np.array(correct1d[\"Vicuna\"]) == False)[0][0]\n",
    "answers1d[\"Vicuna\"][error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m error \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(np\u001b[39m.\u001b[39marray(correct1d[\u001b[39m\"\u001b[39m\u001b[39mMPT\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m answers1d[\u001b[39m\"\u001b[39m\u001b[39mMPT\u001b[39m\u001b[39m\"\u001b[39m][error]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "error = np.where(np.array(correct1d[\"MPT\"]) == False)[0][0]\n",
    "answers1d[\"MPT\"][error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m error \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(np\u001b[39m.\u001b[39marray(correct1d[\u001b[39m\"\u001b[39m\u001b[39mMPT\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[0;32m      2\u001b[0m answers1d[\u001b[39m\"\u001b[39m\u001b[39mMPT\u001b[39m\u001b[39m\"\u001b[39m][error]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "error = np.where(np.array(correct1d[\"MPT\"]) == False)[0][1]\n",
    "answers1d[\"MPT\"][error]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**  \n",
    "This was a very toy task, but served as an example of how to compare models with different chat syntax using the same prompt.  \n",
    "For this particular combination of toy task and prompt, ChatGPT far surpasses Vicuna and MPT in terms of accuracy (measured exactly, because we have ground truth).  \n",
    "Let's now turn to more realistic tasks, where evaluating accuracy is not as straightforward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting snippets + answering questions about meetings\n",
    "Let's say we want our LLM to answer questions (with the relevant conversation segments for grounding) about meeting transcripts.  \n",
    "This is an application where some users might prefer to use open-source LLMs rather than commercial ones, for privacy reasons (maybe I don't want to send all of my meeting data to OpenAI).  \n",
    "Here is a toy meeting transcript to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_transcript = \"\"\"John: Alright, so we're all here to discuss the offer we received from Microsoft to buy our startup. What are your thoughts on this?\n",
    "Lucy: Well, I think it's a great opportunity for us. Microsoft is a huge company with a lot of resources, and they could really help us take our product to the next level.\n",
    "Steven: I agree with Lucy. Microsoft has a lot of experience in the tech industry, and they could provide us with the support we need to grow our business.\n",
    "John: I see your point, but I'm a little hesitant about selling our startup. We've put a lot of time and effort into building this company, and I'm not sure if I'm ready to let it go just yet.\n",
    "Lucy: I understand where you're coming from, John, but we have to think about the future of our company. If we sell to Microsoft, we'll have access to their resources and expertise, which could help us grow our business even more.\n",
    "Steven: Right, and let's not forget about the financial benefits. Microsoft is offering us a lot of money for our startup, which could help us invest in new projects and expand our team.\n",
    "John: I see your point, but I still have some reservations. What if Microsoft changes our product or our company culture? What if we lose control over our own business?\n",
    "Steven: You know what, I hadn't thought about this before, but maybe John is right. It would be a shame if our culture changed.\n",
    "Lucy: Those are valid concerns, but we can negotiate the terms of the deal to ensure that we retain some control over our company. And as for the product and culture, we can work with Microsoft to make sure that our vision is still intact.\n",
    "John: But won't we change just by virtue of being absorbed into a big company? I mean, we're a small startup with a very specific culture. Microsoft is a huge corporation with a very different culture. I'm not sure if the two can coexist.\n",
    "Steven: But John, didn't we always plan on being acquired? Won't this be a problem whenever?\n",
    "Lucy: Right\n",
    "John: I just don't want to lose what we've built here.\n",
    "Steven: I share this concern too\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by just trying to get ChatGPT to solve the task for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"How does Steven feel about selling?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'guidance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_attempt1 \u001b[39m=\u001b[39m guidance(\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"{{#system~}}\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m{{llm.default_system_prompt}}\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m{{~/system}}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mYou will read a meeting transcript, then extract the relevant segments to answer the following question:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mQuestion: {{query}}\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mHere is a meeting transcript:\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m{{transcript}}\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mPlease answer the following question:\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mQuestion: {{query}}\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mExtract from the transcript the most relevant segments for the answer, and then answer the question.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m{{/user}}\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[39m{{#assistant~}}\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m{{gen 'answer' temperature=0}}\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m{{~/assistant~}}\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m qa_attempt1(llm\u001b[39m=\u001b[39mchatgpt, transcript\u001b[39m=\u001b[39mmeeting_transcript, query\u001b[39m=\u001b[39mquery1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'guidance' is not defined"
     ]
    }
   ],
   "source": [
    "qa_attempt1 = guidance(\n",
    "    \"\"\"{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: {{query}}\n",
    "Here is a meeting transcript:\n",
    "----\n",
    "{{transcript}}\n",
    "----\n",
    "Please answer the following question:\n",
    "Question: {{query}}\n",
    "Extract from the transcript the most relevant segments for the answer, and then answer the question.\n",
    "{{/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}\"\"\"\n",
    ")\n",
    "qa_attempt1(llm=chatgpt, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the response is plausible, ChatGPT did not extract any conversation segments to ground the answer (and thus fails our specification). Let's try a different prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'guidance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_attempt2 \u001b[39m=\u001b[39m guidance(\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"{{#system~}}\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m{{llm.default_system_prompt}}\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m{{~/system}}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mYou will read a meeting transcript, then extract the relevant segments to answer the following question:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mQuestion: {{query}}\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mHere is a meeting transcript:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m{{transcript}}\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mConsider the following question:\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mQuestion: {{query}}\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mNow follow these steps:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m1. Extract from the transcript the most relevant segments for the answer\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m2. Answer the question.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m{{/user}}\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m{{#assistant~}}\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m{{gen 'answer' temperature=0}}\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m{{~/assistant~}}\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m qa_attempt2(llm\u001b[39m=\u001b[39mchatgpt, transcript\u001b[39m=\u001b[39mmeeting_transcript, query\u001b[39m=\u001b[39mquery1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'guidance' is not defined"
     ]
    }
   ],
   "source": [
    "qa_attempt2 = guidance(\n",
    "    \"\"\"{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: {{query}}\n",
    "Here is a meeting transcript:\n",
    "----\n",
    "{{transcript}}\n",
    "----\n",
    "Consider the following question:\n",
    "Question: {{query}}\n",
    "Now follow these steps:\n",
    "1. Extract from the transcript the most relevant segments for the answer\n",
    "2. Answer the question.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}\"\"\"\n",
    ")\n",
    "qa_attempt2(llm=chatgpt, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better, but maybe we want to specify the output format a little more, e.g. let's say we want each segment to have a summary, and to keep the character names. \n",
    "Let's try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'guidance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_attempt3 \u001b[39m=\u001b[39m guidance(\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"{{#system~}}\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m{{llm.default_system_prompt}}\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m{{~/system}}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mYou will read a meeting transcript, then extract the relevant segments to answer the following question:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mQuestion: {{query}}\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mHere is a meeting transcript:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m{{transcript}}\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mBased on the above, please answer the following question:\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mQuestion: {{query}}\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mPlease extract from the transcript whichever conversation segments are most relevant for the answer, and then answer the question.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mNote that conversation segments can be of any length, e.g. including multiple conversation turns.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mPlease extract at most 3 segments. If you need less than three segments, you can leave the rest blank.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[39mAs an example of output format, here is a fictitious answer to a question about another meeting transcript.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mCONVERSATION SEGMENTS:\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mSegment 1: Peter and John discuss the weather.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mPeter: John, how is the weather today?\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mJohn: It's raining.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mSegment 2: Peter insults John\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mPeter: John, you are a bad person.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mSegment 3: Blank\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mANSWER: Peter and John discussed the weather and Peter insulted John.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m{{/user}}\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m{{#assistant~}}\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m{{gen 'answer' temperature=0}}\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m{{~/assistant~}}\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m qa_attempt3(llm\u001b[39m=\u001b[39mchatgpt, transcript\u001b[39m=\u001b[39mmeeting_transcript, query\u001b[39m=\u001b[39mquery1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'guidance' is not defined"
     ]
    }
   ],
   "source": [
    "qa_attempt3 = guidance(\n",
    "    \"\"\"{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: {{query}}\n",
    "Here is a meeting transcript:\n",
    "----\n",
    "{{transcript}}\n",
    "----\n",
    "Based on the above, please answer the following question:\n",
    "Question: {{query}}\n",
    "Please extract from the transcript whichever conversation segments are most relevant for the answer, and then answer the question.\n",
    "Note that conversation segments can be of any length, e.g. including multiple conversation turns.\n",
    "Please extract at most 3 segments. If you need less than three segments, you can leave the rest blank.\n",
    "\n",
    "As an example of output format, here is a fictitious answer to a question about another meeting transcript.\n",
    "CONVERSATION SEGMENTS:\n",
    "Segment 1: Peter and John discuss the weather.\n",
    "Peter: John, how is the weather today?\n",
    "John: It's raining.\n",
    "Segment 2: Peter insults John\n",
    "Peter: John, you are a bad person.\n",
    "Segment 3: Blank\n",
    "ANSWER: Peter and John discussed the weather and Peter insulted John.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}\"\"\"\n",
    ")\n",
    "qa_attempt3(llm=chatgpt, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT did extract relevant segments, but it did not follow our output format (it did not summarize each segment, nor did it have the participant's names). Let's try again, with more explicit instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'guidance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_attempt4 \u001b[39m=\u001b[39m guidance(\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"{{#system~}}\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m{{llm.default_system_prompt}}\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m{{~/system}}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mYou will read a meeting transcript, then extract the relevant segments to answer the following question:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mQuestion: {{query}}\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mHere is a meeting transcript:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m{{transcript}}\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mBased on the above, please answer the following question:\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mQuestion: {{query}}\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mPlease extract from the transcript whichever conversation segments are most relevant for the answer, and then answer the question.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mNote that conversation segments can be of any length, e.g. including multiple conversation turns.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mPlease extract at most 3 segments. If you need less than three segments, you can leave the rest blank.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[39mYour output should have the following structure:\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mCONVERSATION SEGMENTS:\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mSegment 1: a summary of the first conversation segment\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m(segment here)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mSegment 2: a summary of the second conversation segment\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m(segment here)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mSegment 3: a summary of the third conversation segment\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m(segment here)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mANSWER: the answer to the question, supported by the segments above.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[39mAs an example of output format, here is a fictitious answer to a question about another meeting transcript.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39mCONVERSATION SEGMENTS:\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mSegment 1: Peter and John discuss the weather.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mPeter: John, how is the weather today?\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mJohn: It's raining.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mSegment 2: Peter insults John\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39mPeter: John, you are a bad person.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mSegment 3: Blank\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mANSWER: Peter and John discussed the weather and Peter insulted John.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39m{{/user}}\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m{{#assistant~}}\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m{{gen 'answer' temperature=0}}\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m{{~/assistant~}}\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     42\u001b[0m qa_attempt4(llm\u001b[39m=\u001b[39mchatgpt, transcript\u001b[39m=\u001b[39mmeeting_transcript, query\u001b[39m=\u001b[39mquery1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'guidance' is not defined"
     ]
    }
   ],
   "source": [
    "qa_attempt4 = guidance(\n",
    "    \"\"\"{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: {{query}}\n",
    "Here is a meeting transcript:\n",
    "----\n",
    "{{transcript}}\n",
    "----\n",
    "Based on the above, please answer the following question:\n",
    "Question: {{query}}\n",
    "Please extract from the transcript whichever conversation segments are most relevant for the answer, and then answer the question.\n",
    "Note that conversation segments can be of any length, e.g. including multiple conversation turns.\n",
    "Please extract at most 3 segments. If you need less than three segments, you can leave the rest blank.\n",
    "\n",
    "Your output should have the following structure:\n",
    "CONVERSATION SEGMENTS:\n",
    "Segment 1: a summary of the first conversation segment\n",
    "(segment here)\n",
    "Segment 2: a summary of the second conversation segment\n",
    "(segment here)\n",
    "Segment 3: a summary of the third conversation segment\n",
    "(segment here)\n",
    "ANSWER: the answer to the question, supported by the segments above.\n",
    "\n",
    "As an example of output format, here is a fictitious answer to a question about another meeting transcript.\n",
    "CONVERSATION SEGMENTS:\n",
    "Segment 1: Peter and John discuss the weather.\n",
    "Peter: John, how is the weather today?\n",
    "John: It's raining.\n",
    "Segment 2: Peter insults John\n",
    "Peter: John, you are a bad person.\n",
    "Segment 3: Blank\n",
    "ANSWER: Peter and John discussed the weather and Peter insulted John.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}\"\"\"\n",
    ")\n",
    "qa_attempt4(llm=chatgpt, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we got ChatGPT to use the format we wanted.\n",
    "The root of the problem we're facing is that the OpenAI API does not allow us to do partial output completion (i.e. we can't specify how the assistant begins to answer), and thus it's hard for us to **guide** the output.  \n",
    "If, instead, we use one of the open source models, we can guide the output more clearly, forcing the model to use our structure.  \n",
    "For example, here is how we might modify `qa_attempt3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'guidance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_guided \u001b[39m=\u001b[39m guidance(\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"{{#system~}}\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m{{llm.default_system_prompt}}\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m{{~/system}}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mYou will read a meeting transcript, then extract the relevant segments to answer the following question:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mQuestion: {{query}}\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m{{transcript}}\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mBased on the above, please answer the following question:\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mQuestion: {{query}}\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mPlease extract the three segment from the transcript that are the most relevant for the answer, and then answer the question.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mNote that conversation segments can be of any length, e.g. including multiple conversation turns. If you need less than three segments, you can leave the rest blank.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mAs an example of output format, here is a fictitious answer to a question about another meeting transcript:\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mCONVERSATION SEGMENTS:\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mSegment 1: Peter and John discuss the weather.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mPeter: John, how is the weather today?\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mJohn: It's raining.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mSegment 2: Peter insults John\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mPeter: John, you are a bad person.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mSegment 3: Blank\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mANSWER: Peter and John discussed the weather and Peter insulted John.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m{{/user}}\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[39m{{#assistant~}}\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39mCONVERSATION SEGMENTS:\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mSegment 1: {{gen 'segment1' temperature=0}}\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mSegment 2: {{gen 'segment2' temperature=0}}\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mSegment 3: {{gen 'segment3' temperature=0}}\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mANSWER: {{gen 'answer' temperature=0}}\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m{{~/assistant~}}\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m qa_guided(llm\u001b[39m=\u001b[39mvicuna, transcript\u001b[39m=\u001b[39mmeeting_transcript, query\u001b[39m=\u001b[39mquery1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'guidance' is not defined"
     ]
    }
   ],
   "source": [
    "qa_guided = guidance(\n",
    "    \"\"\"{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: {{query}}\n",
    "----\n",
    "{{transcript}}\n",
    "----\n",
    "Based on the above, please answer the following question:\n",
    "Question: {{query}}\n",
    "Please extract the three segment from the transcript that are the most relevant for the answer, and then answer the question.\n",
    "Note that conversation segments can be of any length, e.g. including multiple conversation turns. If you need less than three segments, you can leave the rest blank.\n",
    "\n",
    "As an example of output format, here is a fictitious answer to a question about another meeting transcript:\n",
    "CONVERSATION SEGMENTS:\n",
    "Segment 1: Peter and John discuss the weather.\n",
    "Peter: John, how is the weather today?\n",
    "John: It's raining.\n",
    "Segment 2: Peter insults John\n",
    "Peter: John, you are a bad person.\n",
    "Segment 3: Blank\n",
    "ANSWER: Peter and John discussed the weather and Peter insulted John.\n",
    "{{/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "CONVERSATION SEGMENTS:\n",
    "Segment 1: {{gen 'segment1' temperature=0}}\n",
    "Segment 2: {{gen 'segment2' temperature=0}}\n",
    "Segment 3: {{gen 'segment3' temperature=0}}\n",
    "ANSWER: {{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}\"\"\"\n",
    ")\n",
    "qa_guided(llm=vicuna, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this guidance, we get the right format the first time (and all the time). Let's see how MPT does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_guided' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_guided(llm\u001b[39m=\u001b[39mmpt, transcript\u001b[39m=\u001b[39mmeeting_transcript, query\u001b[39m=\u001b[39mquery1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qa_guided' is not defined"
     ]
    }
   ],
   "source": [
    "qa_guided(llm=mpt, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While MPT follows the format, it ignores the question and takes snippets from the format example rather than from the real transcript.  \n",
    "From now on, we'll just compare ChatGPT and Vicuna."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"Who wants to sell the company?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_attempt4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_attempt4(\n\u001b[0;32m      2\u001b[0m     llm\u001b[39m=\u001b[39mchatgpt,\n\u001b[0;32m      3\u001b[0m     system_prompt\u001b[39m=\u001b[39mchatgpt_system,\n\u001b[0;32m      4\u001b[0m     transcript\u001b[39m=\u001b[39mmeeting_transcript,\n\u001b[0;32m      5\u001b[0m     query\u001b[39m=\u001b[39mquery2,\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qa_attempt4' is not defined"
     ]
    }
   ],
   "source": [
    "qa_attempt4(\n",
    "    llm=chatgpt,\n",
    "    system_prompt=chatgpt_system,\n",
    "    transcript=meeting_transcript,\n",
    "    query=query2,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ugh, it seems that we didn't fix the formatting issue with ChatGPT yet. Not only does it not summarize the segments, it also doesn't really answer the question. Let's try again, putting a one-shot example as a conversation round this time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'guidance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_attempt5 \u001b[39m=\u001b[39m guidance(\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"{{#system~}}\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m{{llm.default_system_prompt}}\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m{{~/system}}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mYou will read a meeting transcript, then extract the relevant segments to answer the following question:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mQuestion: What were the main things that happened in the meeting?\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mHere is a meeting transcript:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mPeter: Hey\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mJohn: Hey\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mPeter: John, how is the weather today?\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mJohn: It's raining.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mPeter: That's too bad. I was hoping to go for a walk later.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mJohn: Yeah, it's a shame.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mPeter: John, you are a bad person.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mBased on the above, please answer the following question:\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mQuestion: {{query}}\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mPlease extract from the transcript whichever conversation segments are most relevant for the answer, and then answer the question.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mNote that conversation segments can be of any length, e.g. including multiple conversation turns.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mPlease extract at most 3 segments. If you need less than three segments, you can leave the rest blank.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m{{/user}}\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m{{#assistant~}}\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mCONVERSATION SEGMENTS:\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mSegment 1: Peter and John discuss the weather.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mPeter: John, how is the weather today?\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mJohn: It's raining.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39mSegment 2: Peter insults John\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mPeter: John, you are a bad person.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mSegment 3: Blank\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mANSWER: Peter and John discussed the weather and Peter insulted John.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m{{~/assistant~}}\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mYou will read a meeting transcript, then extract the relevant segments to answer the following question:\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mQuestion: {{query}}\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39mHere is a meeting transcript:\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m{{transcript}}\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m----\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39mBased on the above, please answer the following question:\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39mQuestion: {{query}}\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mPlease extract from the transcript whichever conversation segments are most relevant for the answer, and then answer the question.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mNote that conversation segments can be of any length, e.g. including multiple conversation turns.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39mPlease extract at most 3 segments. If you need less than three segments, you can leave the rest blank.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39m{{/user}}\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39m{{#assistant~}}\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39m{{gen 'answer' temperature=0}}\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39m{{~/assistant~}}\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'guidance' is not defined"
     ]
    }
   ],
   "source": [
    "qa_attempt5 = guidance(\n",
    "    \"\"\"{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: What were the main things that happened in the meeting?\n",
    "Here is a meeting transcript:\n",
    "----\n",
    "Peter: Hey\n",
    "John: Hey\n",
    "Peter: John, how is the weather today?\n",
    "John: It's raining.\n",
    "Peter: That's too bad. I was hoping to go for a walk later.\n",
    "John: Yeah, it's a shame.\n",
    "Peter: John, you are a bad person.\n",
    "----\n",
    "Based on the above, please answer the following question:\n",
    "Question: {{query}}\n",
    "Please extract from the transcript whichever conversation segments are most relevant for the answer, and then answer the question.\n",
    "Note that conversation segments can be of any length, e.g. including multiple conversation turns.\n",
    "Please extract at most 3 segments. If you need less than three segments, you can leave the rest blank.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "CONVERSATION SEGMENTS:\n",
    "Segment 1: Peter and John discuss the weather.\n",
    "Peter: John, how is the weather today?\n",
    "John: It's raining.\n",
    "Segment 2: Peter insults John\n",
    "Peter: John, you are a bad person.\n",
    "Segment 3: Blank\n",
    "ANSWER: Peter and John discussed the weather and Peter insulted John.\n",
    "{{~/assistant~}}\n",
    "{{#user~}}\n",
    "You will read a meeting transcript, then extract the relevant segments to answer the following question:\n",
    "Question: {{query}}\n",
    "Here is a meeting transcript:\n",
    "----\n",
    "{{transcript}}\n",
    "----\n",
    "Based on the above, please answer the following question:\n",
    "Question: {{query}}\n",
    "Please extract from the transcript whichever conversation segments are most relevant for the answer, and then answer the question.\n",
    "Note that conversation segments can be of any length, e.g. including multiple conversation turns.\n",
    "Please extract at most 3 segments. If you need less than three segments, you can leave the rest blank.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "{{gen 'answer' temperature=0}}\n",
    "{{~/assistant~}}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_attempt5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_attempt5(llm\u001b[39m=\u001b[39mchatgpt, transcript\u001b[39m=\u001b[39mmeeting_transcript, query\u001b[39m=\u001b[39mquery2)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qa_attempt5' is not defined"
     ]
    }
   ],
   "source": [
    "qa_attempt5(llm=chatgpt, transcript=meeting_transcript, query=query2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works this time (it also works on the original query):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_attempt5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_attempt5(llm\u001b[39m=\u001b[39mchatgpt, transcript\u001b[39m=\u001b[39mmeeting_transcript, query\u001b[39m=\u001b[39mquery1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qa_attempt5' is not defined"
     ]
    }
   ],
   "source": [
    "qa_attempt5(llm=chatgpt, transcript=meeting_transcript, query=query1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how Vicuna does on this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_guided' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_guided(llm\u001b[39m=\u001b[39mvicuna, transcript\u001b[39m=\u001b[39mmeeting_transcript, query\u001b[39m=\u001b[39mquery2)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qa_guided' is not defined"
     ]
    }
   ],
   "source": [
    "qa_guided(llm=vicuna, transcript=meeting_transcript, query=query2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna just worked.  \n",
    "Let's now try both of these prompts on a different meeting transcript, the beginning of [an interview](https://www.rev.com/blog/transcripts/elon-musk-interview-with-the-bbc-4-11-23-transcript) with Elon Musk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full transcript: https://www.rev.com/blog/transcripts/elon-musk-interview-with-the-bbc-4-11-23-transcript\n",
    "transcript2 = \"\"\"Interviewer: In Sachs that used to be in content moderation. And we’ve spoken to people very recently who were involved in moderation. And they just say there’s not enough people to police this stuff. Particularly around hate speech in the company. Is that something that you-\n",
    "Elon Musk: What hate speech are you talking about? I mean, you use Twitter?\n",
    "Interviewer: Right.\n",
    "Elon Musk: Do you see a rise in hate speech? Just your personal anecdote, do you? I don’t.\n",
    "Interviewer: Personally, my For You, I would see I get more of that kind of content. Yeah. Personally. But I’m not going to talk for the rest of Twitter.\n",
    "Elon Musk: You see more hate speech personally?\n",
    "Interviewer: I would say see more hateful content in that.\n",
    "Elon Musk: Content you don’t like, or hateful. Describe a hateful thing?\n",
    "Interviewer: Yeah, I mean just content that will solicit a reaction. Something that may include something that is slightly racist or slightly sexist. Those kinds of things.\n",
    "Elon Musk: So you think if it’s something is slightly sexist it should be banned?\n",
    "Interviewer: No, I’m not saying anything. I’m saying-\n",
    "Elon Musk: I’m just curious. I’m trying to understand what you mean by “hateful content.” And I’m asking for specific examples. And you just said that if something is slightly sexist, that’s hateful content. Does that mean that it should be banned?\n",
    "Interviewer: Well, you’ve asked me whether my feed, whether it’s got less or more. I’d say it’s got slightly more.\n",
    "Elon Musk: That’s why I’m asking for examples. Can you name one example?\n",
    "Interviewer: I honestly don’t…\n",
    "Elon Musk: You can’t name a single example?\n",
    "Interviewer: I’ll tell you why. Because I don’t actually use that For You feed anymore. Because I just don’t particularly like it. Actually a lot of people are quite similar. I only look at my following.\n",
    "Elon Musk: You said you’ve seen more hateful content, but you can’t name a single example. Not even one.\n",
    "Interviewer: I’m not sure I’ve used that feed for the last three or four weeks. And I honestly couldn’t-\n",
    "Elon Musk: Then how could you see the hateful content?\n",
    "Interviewer: Because I’ve been using it. I’ve been using Twitter since you’ve taken it over for the last six months.\n",
    "Elon Musk: Then you must have at some point seen the For You hateful content. I’m asking for one example.\n",
    "Interviewer: Right.\n",
    "Elon Musk: And you can’t give a single one.\n",
    "Interviewer: And I’m saying-\n",
    "Elon Musk: Then I say, sir, that you don’t know what you’re talking about.\n",
    "Interviewer: Really?\n",
    "Elon Musk: Yes. Because you can’t give a single example of hateful content. Not even one tweet. And yet you claimed that the hateful content was high. That’s false.\n",
    "Interviewer: No. What I claimed-\n",
    "Elon Musk: You just lied.\n",
    "Interviewer: No, no. What I claimed was there are many organizations that say that that kind of information is on the rise. Now whether it has on my feed or not...\"\"\"\n",
    "query3 = \"The interviewer says his claim was not about his personal feed. Is this true?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_attempt5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_attempt5(llm\u001b[39m=\u001b[39mchatgpt, transcript\u001b[39m=\u001b[39mtranscript2, query\u001b[39m=\u001b[39mquery3)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qa_attempt5' is not defined"
     ]
    }
   ],
   "source": [
    "qa_attempt5(llm=chatgpt, transcript=transcript2, query=query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_guided' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_guided(llm\u001b[39m=\u001b[39mvicuna, transcript\u001b[39m=\u001b[39mtranscript2, query\u001b[39m=\u001b[39mquery3)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qa_guided' is not defined"
     ]
    }
   ],
   "source": [
    "qa_guided(llm=vicuna, transcript=transcript2, query=query3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, both work fine. Another question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_attempt5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query4 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDoes Elon Musk insult the interviewer?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m qa_attempt5(llm\u001b[39m=\u001b[39mchatgpt, transcript\u001b[39m=\u001b[39mtranscript2, query\u001b[39m=\u001b[39mquery4)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qa_attempt5' is not defined"
     ]
    }
   ],
   "source": [
    "query4 = \"Does Elon Musk insult the interviewer?\"\n",
    "qa_attempt5(llm=chatgpt, transcript=transcript2, query=query4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_guided' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_guided(llm\u001b[39m=\u001b[39mvicuna, transcript\u001b[39m=\u001b[39mtranscript2, query\u001b[39m=\u001b[39mquery4)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qa_guided' is not defined"
     ]
    }
   ],
   "source": [
    "qa_guided(llm=vicuna, transcript=transcript2, query=query4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna, has the right format and even the right segments, but it surprisingly generates a completely wrong answer, when it says \"Elon musk does not accuse him of lying or insult him in any way\".   \n",
    "We tried a variety of other questions and conversations, and the overall pattern was that Vicuna was comparable to ChatGPT on most questions, but got the answer wrong more often than ChatGPT did."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Using Bash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to get these LLMs to iteratively use a bash shell to solve individual tasks.\n",
    "Whenever they issue a command, we run it and paste the output back into the prompt, until the task is solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'termios'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# A bash session with state\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpty\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msubprocess\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\pty.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtty\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# names imported directly for test mocking purposes\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mos\u001b[39;00m \u001b[39mimport\u001b[39;00m close, waitpid\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\Lib\\tty.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"Terminal utilities.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Author: Steen Lumholt.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtermios\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      7\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39msetraw\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msetcbreak\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[39m# Indexes for termios list.\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'termios'"
     ]
    }
   ],
   "source": [
    "# A bash session with state\n",
    "import pty\n",
    "from subprocess import Popen\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "class BashSession:\n",
    "    def __init__(self):\n",
    "        self.master_fd, self.slave_fd = pty.openpty()\n",
    "        self.p = Popen(\n",
    "            \"bash\",\n",
    "            preexec_fn=os.setsid,\n",
    "            stdin=self.slave_fd,\n",
    "            stdout=self.slave_fd,\n",
    "            stderr=self.slave_fd,\n",
    "            universal_newlines=True,\n",
    "        )\n",
    "        self.run(\"ls\")\n",
    "\n",
    "    def run(self, command):\n",
    "        command = command + \"\\n\"\n",
    "        os.write(self.master_fd, command.encode())\n",
    "        time.sleep(0.2)\n",
    "        return \"\\n\".join(os.read(self.master_fd, 10240).decode().split(\"\\n\")[1:-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do ChatGPT. Again, since we can't specify the output format, we rely on a description of the format and on a one-shot example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'guidance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m terminal \u001b[39m=\u001b[39m guidance(\n\u001b[0;32m      4\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"{{#system~}}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m{{llm.default_system_prompt}}\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m{{~/system}}\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mPlease complete the following task:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mTask: list the files in the current directory\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mYou can give me one bash command to run at a time, using the syntax:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mCOMMAND: command\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mI will run the commands on my terminal, and paste the output back to you. Once you are done with the task, please type DONE.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m{{/user}}\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m{{#assistant~}}\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mCOMMAND: ls\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m{{~/assistant~}}\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mOutput: guidance project\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m{{/user}}\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m{{#assistant~}}\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mThe files or folders in the current directory are:\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m- guidance\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m- project\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mDONE\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m{{~/assistant~}}\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mPlease complete the following task:\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mTask: {{task}}\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39mYou can give me one bash command to run at a time, using the syntax:\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mCOMMAND: command\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mI will run the commands on my terminal, and paste the output back to you. Once you are done with the task, please type DONE.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m{{/user}}\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m{{#geneach 'commands'}}\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m{{#assistant~}}\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m{{gen 'this.command' temperature=0}}\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39m{{~/assistant~}}\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39mOutput: {{set 'this.output' (await 'output')}}\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m{{~/user}}\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m{{/geneach}}\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_task_chatgpt\u001b[39m(task):\n\u001b[0;32m     45\u001b[0m     t \u001b[39m=\u001b[39m terminal(llm\u001b[39m=\u001b[39mchatgpt, task\u001b[39m=\u001b[39mtask)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'guidance' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "terminal = guidance(\n",
    "    \"\"\"{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "{{#user~}}\n",
    "Please complete the following task:\n",
    "Task: list the files in the current directory\n",
    "You can give me one bash command to run at a time, using the syntax:\n",
    "COMMAND: command\n",
    "I will run the commands on my terminal, and paste the output back to you. Once you are done with the task, please type DONE.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "COMMAND: ls\n",
    "{{~/assistant~}}\n",
    "{{#user~}}\n",
    "Output: guidance project\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "The files or folders in the current directory are:\n",
    "- guidance\n",
    "- project\n",
    "DONE\n",
    "{{~/assistant~}}\n",
    "{{#user~}}\n",
    "Please complete the following task:\n",
    "Task: {{task}}\n",
    "You can give me one bash command to run at a time, using the syntax:\n",
    "COMMAND: command\n",
    "I will run the commands on my terminal, and paste the output back to you. Once you are done with the task, please type DONE.\n",
    "{{/user}}\n",
    "{{#geneach 'commands'}}\n",
    "{{#assistant~}}\n",
    "{{gen 'this.command' temperature=0}}\n",
    "{{~/assistant~}}\n",
    "{{#user~}}\n",
    "Output: {{set 'this.output' (await 'output')}}\n",
    "{{~/user}}\n",
    "{{/geneach}}\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def run_task_chatgpt(task):\n",
    "    t = terminal(llm=chatgpt, task=task)\n",
    "    session = BashSession()\n",
    "    for _ in range(10):\n",
    "        # Extract command\n",
    "        command = re.findall(r\"COMMAND: (.*)\", t[\"commands\"][-1][\"command\"])\n",
    "        if not command or \"DONE\" in t[\"commands\"][-1][\"command\"]:\n",
    "            break\n",
    "        command = command[0]\n",
    "        output = session.run(command)\n",
    "        t = t(output=output)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_task_chatgpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m run_task_chatgpt(task)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_task_chatgpt' is not defined"
     ]
    }
   ],
   "source": [
    "run_task_chatgpt(task)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try a simple task.  \n",
    "We created a dummy repo in `~/work/project`, with file `license.txt` (not the standard `LICENSE` file name).  \n",
    "Without communicating this to ChatGPT, let's see if it can figure it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_task_chatgpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m task \u001b[39m=\u001b[39m (\n\u001b[0;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFind out what license the open source project located in ~/work/project is using.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m )\n\u001b[1;32m----> 4\u001b[0m run_task_chatgpt(task)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_task_chatgpt' is not defined"
     ]
    }
   ],
   "source": [
    "task = (\n",
    "    \"Find out what license the open source project located in ~/work/project is using.\"\n",
    ")\n",
    "run_task_chatgpt(task)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, ChatGPT follows a very natural sequence, and solves the task.\n",
    "\n",
    "For the open source models, we write a simpler (guided) prompt where there is a sequence of command-output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'guidance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m guided_terminal \u001b[39m=\u001b[39m guidance(\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"{{#system~}}\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m{{llm.default_system_prompt}}\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m{{~/system}}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mPlease complete the following task:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mTask: list the files in the current directory\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mYou can run bash commands using the syntax:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mCOMMAND: command\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mOUTPUT: output\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mOnce you are done with the task, use the COMMAND: DONE.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m{{/user}}\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m{{#assistant~}}\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mCOMMAND: ls\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mOUTPUT: guidance project\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mCOMMAND: DONE \u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m{{~/assistant~}}\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m{{#user~}}\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mPlease complete the following task:\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mTask: {{task}}\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mYou can run bash commands using the syntax:\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mCOMMAND: command\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mOUTPUT: output\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mOnce you are done with the task, use the COMMAND: DONE.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m{{~/user}}\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m{{~#assistant~}}\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m{{#geneach 'commands'~}}\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mCOMMAND: {{gen 'this.command' stop='\\\\n'}}\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39mOUTPUT: {{shell this.command}}{{~/geneach}}\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m{{~/assistant~}}\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mStatefulShellOpenSource\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'guidance' is not defined"
     ]
    }
   ],
   "source": [
    "guided_terminal = guidance(\n",
    "    \"\"\"{{#system~}}\n",
    "{{llm.default_system_prompt}}\n",
    "{{~/system}}\n",
    "{{#user~}}\n",
    "Please complete the following task:\n",
    "Task: list the files in the current directory\n",
    "You can run bash commands using the syntax:\n",
    "COMMAND: command\n",
    "OUTPUT: output\n",
    "Once you are done with the task, use the COMMAND: DONE.\n",
    "{{/user}}\n",
    "{{#assistant~}}\n",
    "COMMAND: ls\n",
    "OUTPUT: guidance project\n",
    "COMMAND: DONE \n",
    "{{~/assistant~}}\n",
    "{{#user~}}\n",
    "Please complete the following task:\n",
    "Task: {{task}}\n",
    "You can run bash commands using the syntax:\n",
    "COMMAND: command\n",
    "OUTPUT: output\n",
    "Once you are done with the task, use the COMMAND: DONE.\n",
    "{{~/user}}\n",
    "{{~#assistant~}}\n",
    "{{#geneach 'commands'~}}\n",
    "COMMAND: {{gen 'this.command' stop='\\\\n'}}\n",
    "OUTPUT: {{shell this.command}}{{~/geneach}}\n",
    "{{~/assistant~}}\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class StatefulShellOpenSource:\n",
    "    def __init__(self):\n",
    "        self.session = BashSession()\n",
    "\n",
    "    def __call__(self, command):\n",
    "        if \"DONE\" in command:\n",
    "            raise StopIteration\n",
    "        output = self.session.run(command)\n",
    "        return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StatefulShellOpenSource' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m shell \u001b[39m=\u001b[39m StatefulShellOpenSource()\n\u001b[0;32m      2\u001b[0m task \u001b[39m=\u001b[39m (\n\u001b[0;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFind out what license the open source project located in ~/work/project is using.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m t \u001b[39m=\u001b[39m guided_terminal(llm\u001b[39m=\u001b[39mvicuna, task\u001b[39m=\u001b[39mtask, shell\u001b[39m=\u001b[39mshell)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StatefulShellOpenSource' is not defined"
     ]
    }
   ],
   "source": [
    "shell = StatefulShellOpenSource()\n",
    "task = (\n",
    "    \"Find out what license the open source project located in ~/work/project is using.\"\n",
    ")\n",
    "t = guided_terminal(llm=vicuna, task=task, shell=shell)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna is not able to solve the task this time. Let's see how MPT does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StatefulShellOpenSource' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m shell \u001b[39m=\u001b[39m StatefulShellOpenSource()\n\u001b[0;32m      2\u001b[0m t \u001b[39m=\u001b[39m guided_terminal(llm\u001b[39m=\u001b[39mmpt, task\u001b[39m=\u001b[39mtask, shell\u001b[39m=\u001b[39mshell)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StatefulShellOpenSource' is not defined"
     ]
    }
   ],
   "source": [
    "shell = StatefulShellOpenSource()\n",
    "t = guided_terminal(llm=mpt, task=task, shell=shell)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, MPT works this time while Vicuna doesn't.\n",
    "\n",
    "Besides privacy (we're not sending the session transcript to OpenAI), open source-models have a significant advantage: the whole prompt is a single LLM run (and we even [accelerate](https://github.com/microsoft/guidance#guidance-acceleration-notebook) it by not having it geneate the output structure tokens like `COMMAND:`).   \n",
    "\n",
    "In contrast, we have to make a new call to ChatGPT for each command, which is slower and more expensive.\n",
    "\n",
    "Let's try a different bash task with ChatGPT and Vicuna:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_task_chatgpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m task \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFind all jupyter notebook files in ~/work/guidance that are currently untracked by git\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m run_task_chatgpt(task)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_task_chatgpt' is not defined"
     ]
    }
   ],
   "source": [
    "task = \"Find all jupyter notebook files in ~/work/guidance that are currently untracked by git\"\n",
    "run_task_chatgpt(task)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we run into a problem with ChatGPT not following our specified output structure (and thus making it impossible for us to use inside a program, without a human in the loop).  \n",
    "We fix this __particular__ problem by changing the message when there is no output below, but we can't fix the general problem of not being able to _force_ ChatGPT to follow our specified output structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'terminal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m t \u001b[39m=\u001b[39m terminal(llm\u001b[39m=\u001b[39mchatgpt, task\u001b[39m=\u001b[39mtask)\n\u001b[0;32m      2\u001b[0m session \u001b[39m=\u001b[39m BashSession()\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[39m# Extract command\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'terminal' is not defined"
     ]
    }
   ],
   "source": [
    "    t = terminal(llm=chatgpt, task=task)\n",
    "    session = BashSession()\n",
    "    for _ in range(10):\n",
    "        # Extract command\n",
    "        command = re.findall(r'COMMAND: (.*)', t['commands'][-1]['command'])\n",
    "        if not command or 'DONE' in t['commands'][-1]['command']:\n",
    "            break\n",
    "        command = command[0]\n",
    "        output = session.run(command)\n",
    "        if not output:\n",
    "            output = 'No output'\n",
    "        t = t(output=output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ChatGPT _was_ able to solve the problem after this small modification. Let's see how Vicuna does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StatefulShellOpenSource' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m shell \u001b[39m=\u001b[39m StatefulShellOpenSource()\n\u001b[0;32m      2\u001b[0m t \u001b[39m=\u001b[39m guided_terminal(llm\u001b[39m=\u001b[39mvicuna, task\u001b[39m=\u001b[39mtask, shell\u001b[39m=\u001b[39mshell)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StatefulShellOpenSource' is not defined"
     ]
    }
   ],
   "source": [
    "shell = StatefulShellOpenSource()\n",
    "t = guided_terminal(llm=vicuna, task=task, shell=shell)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vicuna follows our output structure, but unfortunately runs the wrong command to do the task.  \n",
    "We observed the same pattern for other tasks as well, and found ChatGPT to be more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StatefulShellOpenSource' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m shell \u001b[39m=\u001b[39m StatefulShellOpenSource()\n\u001b[0;32m      2\u001b[0m t \u001b[39m=\u001b[39m guided_terminal(llm\u001b[39m=\u001b[39mmpt, task\u001b[39m=\u001b[39mtask, shell\u001b[39m=\u001b[39mshell)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StatefulShellOpenSource' is not defined"
     ]
    }
   ],
   "source": [
    "shell = StatefulShellOpenSource()\n",
    "t = guided_terminal(llm=mpt, task=task, shell=shell)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, MPT fails this time, but calling the same command again and again (we had to interrupt to stop execution.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the examples above, we tried various other inputs for both tasks (question answering and bash), and also tried a variety of other tasks involving summarization, question answering, and \"creative\" generation.  \n",
    "We also tried various toy string manipulation tasks, where we can evaluate accuracy automatically.  \n",
    "Here is a summary of our findings:\n",
    "- **Quality on task**: For _every_ task we tried, **ChatGPT is still stronger** than Vicuna on the task itself. MPT performed poorly on almost all tasks (perhaps we are using it wrong?), while Vicuna was often close to ChatGPT (sometimes very close, sometimes much worse as in the last example task above).  \n",
    "- **Ease of use**: It is much more painful to get ChatGPT to follow a specified output format, and thus it is harder to use it inside a program (without a human in the loop). Further, we always have to write regex parsers for the output (as opposed to Vicuna, where parsing a prompt with [clear syntax](https://towardsdatascience.com/the-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5) is trivial).  \n",
    "We are typically able to solve the structure problem adding more few-shot examples, but it is tedious to write them, and sometimes ChatGPT goes off-script anyway (we also end up with prompts that are longer, clumsier, and _uglier_).  \n",
    "**Being able to specify the output structure is a significant benefit of open-source models**, which sometimes make them a better option even if they are a little worse on the task itself. Vicuna is often close enough to ChatGPT, and for easier tasks they may be indistinguishable.\n",
    "- **Efficiency**: having the model locally means we can solve tasks in a single LLM run (`guidance` keeps the LLM state while the program is executing), which is faster and cheaper. This is particularly true when any substeps involve calling other APIs or functions (like search, terminal, etc), which always requires a new call to the OpenAI API. `guidance` also accelerates generation by not having the model generate the output structure tokens, which sometimes makes a big difference.\n",
    "\n",
    "Now, it may be that these findings don't generalize, and are instead specific to the tasks and inputs we tried (or to the kinds of prompts we tend to write).   \n",
    "That may very well be true, but we think that anyone who tries to use LLMs for real-world tasks will have to go through a similar process to figure out which LLM makes more sense for their use case / preferred prompt style.  \n",
    "(This is of course not including considerations of cost, privacy, model versioning, etc)\n",
    "\n",
    "In summary, our assessment is that MPT is not ready for real-world use yet (unless we're using it wrong), and that Vicuna is a viable alternative to ChatGPT (3.5) for many tasks (in part due to the ability to specify the output structure, since ChatGPT seems to still have stronger on-task performance).  \n",
    "We should acknowledge that we are biased by having used OpenAI models a lot in the past few years, having written various papers that depend on GPT-3 (e.g. [here](https://aclanthology.org/2022.acl-long.230.pdf), [here](https://arxiv.org/pdf/2303.09014.pdf)), and a big [paper](https://arxiv.org/pdf/2303.12712.pdf) that is basically saying \"GPT-4 is awesome, here are a bunch of cool examples\".  \n",
    "While Vicuna is somewhat comparable to ChatGPT (3.5), we believe GPT-4 is a _much_ stronger model, and are excited to see if open source models can approach *that*. `guidance` plays quite well with OpenAI models, but it really shines when you can specify output structure and accelerate generation, which right now you can only do with open source models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**: this post was written jointly by Marco Tulio Ribeiro and Scott Lundberg. It strictly represents our personal opinions, and not those of our employer (Microsoft)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guidance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
