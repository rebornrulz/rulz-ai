{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMXj+VhC5aEQTRwA9CAUjgA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rebornrulz/Rulz-AI/blob/master/colabs/sdk1.1/prompt_ide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r20v6PMFRjYw"
      },
      "outputs": [],
      "source": [
        "\"\"\"PromptIDE SDK version 1.1.\"\"\"\n",
        "\n",
        "import asyncio\n",
        "import contextlib\n",
        "import contextvars\n",
        "import dataclasses\n",
        "import random\n",
        "import time\n",
        "import uuid\n",
        "from typing import Any, Optional, Sequence, Union\n",
        "\n",
        "_USER = 1\n",
        "_MODEL = 2\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class Token:\n",
        "    \"\"\"A token is an element of our vocabulary that has a unique index and string representation.\n",
        "\n",
        "    A token can either be sampled from a model or provided by the user (i.e. prompted). If the token\n",
        "    comes from the mode, we may have additional metadata such as its sampling probability, the\n",
        "    attention pattern used when sampling the token, and alternative tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # The integer representation of the token. Corresponds to its index in the vocabulary.\n",
        "    token_id: int\n",
        "    # The string representation of the token. Corresponds to its value in the vocabulary.\n",
        "    token_str: str\n",
        "    # If this token was sampled, the token sampling probability. 0 if not sampled.\n",
        "    prob: float\n",
        "    # If this token was sampled, alternative tokens that could have been sampled instead.\n",
        "    top_k: list[\"Token\"]\n",
        "    # If this token was sampled with the correct options, the token's attention pattern. The array\n",
        "    # contains one value for every token in the context.\n",
        "    attn_weights: list[float]\n",
        "    # 1 if this token was created by a user and 2 if it was created by model.\n",
        "    token_type: int\n",
        "\n",
        "    @classmethod\n",
        "    def from_proto_dict(cls, values: dict) -> \"Token\":\n",
        "        \"\"\"Converts the protobuffer dictionary to a `Token` instance.\"\"\"\n",
        "        return Token(\n",
        "            token_id=values[\"finalLogit\"][\"tokenId\"],\n",
        "            token_str=values[\"finalLogit\"][\"stringToken\"],\n",
        "            prob=values[\"finalLogit\"][\"prob\"],\n",
        "            top_k=[\n",
        "                Token.from_proto_dict(\n",
        "                    {\"finalLogit\": l, \"topK\": [], \"attention\": [], \"tokenType\": _MODEL}\n",
        "                )\n",
        "                for l in values[\"topK\"]\n",
        "            ],\n",
        "            attn_weights=values[\"attention\"],\n",
        "            token_type=values[\"tokenType\"],\n",
        "        )\n",
        "\n",
        "\n",
        "async def user_input(text: str) -> str | None:\n",
        "    \"\"\"Asks the user to enter something into the text field shown in the completion dialog.\n",
        "\n",
        "    Args:\n",
        "        text: The placeholder text displayed in the text field before the user enters a response.\n",
        "\n",
        "    Returns:\n",
        "        A string if the user actually entered some text and `None` if the user pressed `cancel`.\n",
        "    \"\"\"\n",
        "    args = pyodide.ffi.create_proxy(str(text))\n",
        "    response = await js.userInput(args)\n",
        "    response = response.to_py()\n",
        "\n",
        "    if \"cancelled\" in response:\n",
        "        return None\n",
        "    return response[\"text\"]\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class SampleResult:\n",
        "    \"\"\"Holds the results of a sampling call.\"\"\"\n",
        "\n",
        "    # The actual request made to the sampling API. Note that these fields may be unstable and are\n",
        "    # subject to change in the future.\n",
        "    request: dict = dataclasses\n",
        "\n",
        "    # The number of tokens sampled.\n",
        "    tokens: list[Token] = dataclasses.field(default_factory=list)\n",
        "    # When sampling was started.\n",
        "    start_time: float = dataclasses.field(default_factory=time.time)\n",
        "    # Time when the first token was added.\n",
        "    first_token_time: Optional[float] = None\n",
        "    # When sampling finished.\n",
        "    end_time: Optional[float] = None\n",
        "\n",
        "    def as_string(self) -> str:\n",
        "        \"\"\"Returns a string representation of this context.\"\"\"\n",
        "        return \"\".join(t.token_str for t in self.tokens)\n",
        "\n",
        "    def append(self, token: Token):\n",
        "        \"\"\"Adds a token to the result and reports progress in the terminal.\"\"\"\n",
        "        self.tokens.append(token)\n",
        "        self.end_time = time.time()\n",
        "        if len(self.tokens) == 1:\n",
        "            self.first_token_time = time.time()\n",
        "            duration = (self.first_token_time - self.start_time) * 1000\n",
        "            print(f\"Sampled first token after {duration:1.2f}ms.\")\n",
        "        elif (len(self.tokens) + 1) % 10 == 0:\n",
        "            self.print_progress()\n",
        "\n",
        "    def print_progress(self):\n",
        "        \"\"\"Prints the sampling progress to stdout.\"\"\"\n",
        "        if len(self.tokens) > 1:\n",
        "            duration = self.end_time - self.first_token_time\n",
        "            speed = (len(self.tokens) - 1) / duration\n",
        "            print(f\"Sampled {len(self.tokens)} tokens. \" f\"{speed:1.2f} tokens/s\")\n",
        "\n",
        "\n",
        "def _parse_input_token(token: Union[int, str]) -> dict:\n",
        "    \"\"\"Converts the argument to an `InputToken` proto.\"\"\"\n",
        "    if isinstance(token, int):\n",
        "        return {\"tokenId\": token}\n",
        "    else:\n",
        "        return {\"stringToken\": token}\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Context:\n",
        "    \"\"\"A context is a sequence of tokens that are used as prompt when sampling from the model.\"\"\"\n",
        "\n",
        "    # The context ID.\n",
        "    context_id: str = dataclasses.field(default_factory=lambda: str(uuid.uuid4()))\n",
        "    # The body of this context is a sequence of tokens and child-contexts. The reasons we use a\n",
        "    # joint body field instead of separate fields is that we want to render the child contexts\n",
        "    # relative to the tokens of the parent context.\n",
        "    body: list[Union[Token, \"Context\"]] = dataclasses.field(default_factory=list)\n",
        "    # The parent context if this is not the root context.\n",
        "    parent: Optional[\"Context\"] = None\n",
        "    # The seed used for the next call to `sample`.\n",
        "    next_rng_seed: int = 0\n",
        "    # Name of the model to use. The model name is tied to the context because different models can\n",
        "    # use different tokenizers.\n",
        "    model_name: str = \"\"\n",
        "\n",
        "    # If this context has been manually entered, the reset token to reset the global context\n",
        "    # variable.\n",
        "    _reset_token: Any = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Sends this context to the UI thread to be displayed in the rendering dialogue.\"\"\"\n",
        "        if self.parent is not None:\n",
        "            self.parent.body.append(self)\n",
        "\n",
        "        request = {\n",
        "            \"contextId\": self.context_id,\n",
        "            \"parent\": self.parent.context_id if self.parent else \"\",\n",
        "        }\n",
        "        asyncio.get_event_loop().run_until_complete(\n",
        "            js.createContext(pyodide.ffi.create_proxy(request))\n",
        "        )\n",
        "\n",
        "    def select_model(self, model_name: str):\n",
        "        \"\"\"Selects the model name for this context.\n",
        "\n",
        "        The model name can only be set before any tokens have been added to this context.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the model to use.\n",
        "        \"\"\"\n",
        "        if self.tokens:\n",
        "            raise RuntimeError(\n",
        "                \"Cannot change the model name of a non-empty context. A context \"\n",
        "                \"stores token sequences and different models may use different \"\n",
        "                \"tokenizers. Hence, using tokens across models leads to undefined \"\n",
        "                \"behavior. If you want to use multiple models in the same prompt, \"\n",
        "                \"consider using a @prompt_fn.\"\n",
        "            )\n",
        "        self.model_name = model_name\n",
        "\n",
        "    async def _tokenize(self, text: str) -> list[dict]:\n",
        "        \"\"\"Same as `tokenize` but returns the raw proto dicts.\"\"\"\n",
        "        # Nothing to do if the text is empty.\n",
        "        if not text:\n",
        "            return []\n",
        "        print(f\"Tokenizing prompt with {len(text)} characters.\")\n",
        "        result = await js.tokenize(\n",
        "            pyodide.ffi.create_proxy(\n",
        "                {\n",
        "                    \"text\": text,\n",
        "                    \"modelName\": self.model_name,\n",
        "                }\n",
        "            )\n",
        "        )\n",
        "        result = result.to_py()\n",
        "        compression = (1 - len(result) / len(text)) * 100\n",
        "        print(\n",
        "            f\"Tokenization done. {len(result)} tokens detected (Compression of {compression:.1f}%).\"\n",
        "        )\n",
        "\n",
        "        return result\n",
        "\n",
        "    async def tokenize(self, text: str) -> list[Token]:\n",
        "        \"\"\"Tokenizes the given text and returns a list of individual tokens.\n",
        "\n",
        "        Args:\n",
        "            text: Text to tokenize.\n",
        "\n",
        "        Returns:\n",
        "            List of tokens. The log probability on the logit is initialized to 0.\n",
        "        \"\"\"\n",
        "        result = await self._tokenize(text)\n",
        "        return [Token.from_proto_dict(d) for d in result]\n",
        "\n",
        "    @property\n",
        "    def tokens(self) -> Sequence[Token]:\n",
        "        \"\"\"Returns the tokens stored in this context.\"\"\"\n",
        "        return [t for t in self.body if isinstance(t, Token)]\n",
        "\n",
        "    @property\n",
        "    def children(self) -> Sequence[\"Context\"]:\n",
        "        \"\"\"Returns all child contexts.\"\"\"\n",
        "        return [c for c in self.body if isinstance(c, Context)]\n",
        "\n",
        "    def as_string(self) -> str:\n",
        "        \"\"\"Returns a string representation of this context.\"\"\"\n",
        "        return \"\".join(t.token_str for t in self.tokens)\n",
        "\n",
        "    def as_token_ids(self) -> list[int]:\n",
        "        \"\"\"Returns a list of token IDs stored in this context.\"\"\"\n",
        "        return [t.token_id for t in self.tokens]\n",
        "\n",
        "    async def prompt(self, text: str, strip: bool = False) -> Sequence[Token]:\n",
        "        \"\"\"Tokenizes the argument and adds the tokens to the context.\n",
        "\n",
        "        Args:\n",
        "            text: String to tokenize and add to the context.\n",
        "            strip: If true, any whitespace surrounding `prompt` will be stripped.\n",
        "\n",
        "        Returns:\n",
        "            Tokenized string.\n",
        "        \"\"\"\n",
        "        if strip:\n",
        "            text = text.strip()\n",
        "        token_protos = await self._tokenize(text)\n",
        "\n",
        "        request = {\n",
        "            \"contextId\": self.context_id,\n",
        "            \"tokens\": token_protos,\n",
        "        }\n",
        "        await js.pushTokens(pyodide.ffi.create_proxy(request))\n",
        "\n",
        "        tokens = [Token.from_proto_dict(t) for t in token_protos]\n",
        "        self.body.extend(tokens)\n",
        "        return tokens\n",
        "\n",
        "    def randomize_rng_seed(self) -> int:\n",
        "        \"\"\"Samples a new RNG seed and returns it.\"\"\"\n",
        "        self.next_rng_seed = random.randint(0, 100000)\n",
        "        return self.next_rng_seed\n",
        "\n",
        "    def create_context(self) -> \"Context\":\n",
        "        \"\"\"Creates a new context and adds it as child context.\"\"\"\n",
        "        child = Context(\n",
        "            parent=self, next_rng_seed=self._get_next_rng_seed(), model_name=self.model_name\n",
        "        )\n",
        "        return child\n",
        "\n",
        "    def _get_next_rng_seed(self) -> int:\n",
        "        \"\"\"Returns the next RNG seed.\"\"\"\n",
        "        self.next_rng_seed += 1\n",
        "        return self.next_rng_seed - 1\n",
        "\n",
        "    async def sample(\n",
        "        self,\n",
        "        max_len: int = 256,\n",
        "        temperature: float = 1.0,\n",
        "        nucleus_p: float = 0.7,\n",
        "        stop_tokens: Optional[list[str]] = None,\n",
        "        stop_strings: Optional[list[str]] = None,\n",
        "        rng_seed: Optional[int] = None,\n",
        "        add_to_context: bool = True,\n",
        "        return_attention: bool = False,\n",
        "        allowed_tokens: Optional[Sequence[Union[int, str]]] = None,\n",
        "        disallowed_tokens: Optional[Sequence[Union[int, str]]] = None,\n",
        "        augment_tokens: bool = True,\n",
        "    ) -> SampleResult:\n",
        "        \"\"\"Generates a model response based on the current prompt.\n",
        "\n",
        "        The current prompt consists of all text that has been added to the prompt either since the\n",
        "        beginning of the program or since the last call to `clear_prompt`.\n",
        "\n",
        "        Args:\n",
        "            max_len: Maximum number of tokens to generate.\n",
        "            temperature: Temperature of the final softmax operation. The lower the temperature, the\n",
        "                lower the variance of the token distribution. In the limit, the distribution collapses\n",
        "                onto the single token with the highest probability.\n",
        "            nucleus_p: Threshold of the Top-P sampling technique: We rank all tokens by their\n",
        "                probability and then only actually sample from the set of tokens that ranks in the\n",
        "                Top-P percentile of the distribution.\n",
        "            stop_tokens: A list of strings, each of which will be mapped independently to a single\n",
        "                token. If a string does not map cleanly to one token, it will be silently ignored.\n",
        "                If the network samples one of these tokens, sampling is stopped and the stop token\n",
        "                *is not* included in the response.\n",
        "            stop_strings: A list of strings. If any of these strings occurs in the network output,\n",
        "                sampling is stopped but the string that triggered the stop *will be* included in the\n",
        "                response. Note that the response may be longer than the stop string. For example, if\n",
        "                the stop string is \"Hel\" and the network predicts the single-token response \"Hello\",\n",
        "                sampling will be stopped but the response will still read \"Hello\".\n",
        "            rng_seed: See of the random number generator used to sample from the model outputs.\n",
        "            add_to_context: If true, the generated tokens will be added to the context.\n",
        "            return_attention: If true, returns the attention mask. Note that this can significantly\n",
        "                increase the response size for long sequences.\n",
        "            allowed_tokens: If set, only these tokens can be sampled. Invalid input tokens are\n",
        "                ignored. Only one of `allowed_tokens` and `disallowed_tokens` must be set.\n",
        "            disallowed_tokens: If set, these tokens cannot be sampled. Invalid input tokens are\n",
        "                ignored. Only one of `allowed_tokens` and `disallowed_tokens` must be set.\n",
        "            augment_tokens: If true, strings passed to `stop_tokens`, `allowed_tokens` and\n",
        "                `disallowed_tokens` will be augmented to include both the passed token and the\n",
        "                version with leading whitespace. This is useful because most words have two\n",
        "                corresponding vocabulary entries: one with leading whitespace and one without.\n",
        "\n",
        "        Returns:\n",
        "            The generated text.\n",
        "        \"\"\"\n",
        "        if max_len is None and not stop_tokens:\n",
        "            raise ValueError(\"Must provide either max_len or stop_tokens when calling `generate`.\")\n",
        "\n",
        "        if rng_seed is None:\n",
        "            rng_seed = self._get_next_rng_seed()\n",
        "\n",
        "        if max_len is not None:\n",
        "            print(\n",
        "                f\"Generating {max_len} tokens [seed={rng_seed}, temperature={temperature}, \"\n",
        "                f\"nucleus_p={nucleus_p}, stop_tokens={stop_tokens}, stop_strings={stop_strings}].\"\n",
        "            )\n",
        "\n",
        "        if augment_tokens:\n",
        "            if stop_tokens:\n",
        "                stop_tokens = stop_tokens + [f\"▁{t}\" for t in stop_tokens]\n",
        "            if allowed_tokens:\n",
        "                allowed_tokens = list(allowed_tokens) + [\n",
        "                    f\"▁{t}\" for t in allowed_tokens if isinstance(t, str) and not t.startswith(\"▁\")\n",
        "                ]\n",
        "            if disallowed_tokens:\n",
        "                disallowed_tokens = list(disallowed_tokens) + [\n",
        "                    f\"▁{t}\"\n",
        "                    for t in disallowed_tokens\n",
        "                    if isinstance(t, str) and not t.startswith(\"▁\")\n",
        "                ]\n",
        "\n",
        "        request = {\n",
        "            \"prompt\": self.as_token_ids(),\n",
        "            \"settings\": {\n",
        "                \"maxLen\": max_len or 0,\n",
        "                \"temperature\": temperature,\n",
        "                \"nucleusP\": nucleus_p,\n",
        "                \"stopTokens\": stop_tokens or [],\n",
        "                \"stopStrings\": stop_strings or [],\n",
        "                \"rngSeed\": rng_seed,\n",
        "                \"allowedTokens\": [_parse_input_token(t) for t in allowed_tokens or []],\n",
        "                \"disallowedTokens\": [_parse_input_token(t) for t in disallowed_tokens or []],\n",
        "            },\n",
        "            \"returnAttention\": return_attention,\n",
        "            \"modelName\": self.model_name,\n",
        "        }\n",
        "\n",
        "        args = pyodide.ffi.create_proxy(request)\n",
        "        iterator = js.generate(args)\n",
        "\n",
        "        result = SampleResult(request)\n",
        "\n",
        "        while True:\n",
        "            obj = await iterator.next()\n",
        "            if obj.done:\n",
        "                break\n",
        "\n",
        "            token_proto = obj.value.to_py()\n",
        "            result.append(Token.from_proto_dict(token_proto))\n",
        "\n",
        "            if add_to_context:\n",
        "                self.body.append(result.tokens[-1])\n",
        "\n",
        "            # Sync the token to the UI thread.\n",
        "            request = {\n",
        "                \"contextId\": self.context_id,\n",
        "                \"tokens\": [token_proto],\n",
        "            }\n",
        "            await js.pushTokens(pyodide.ffi.create_proxy(request))\n",
        "\n",
        "        result.print_progress()\n",
        "        return result\n",
        "\n",
        "    def clone(self) -> \"Context\":\n",
        "        \"\"\"Clones the current prompt.\"\"\"\n",
        "        # We can't use deepcopy here because we need to make sure the clone is correctly synced to\n",
        "        # the UI thread.\n",
        "        clone = Context(\n",
        "            # We only clone the tokens, not the child contexts.\n",
        "            body=list(self.tokens),\n",
        "            parent=self,\n",
        "            next_rng_seed=self.next_rng_seed,\n",
        "        )\n",
        "        self.body.append(clone)\n",
        "        return clone\n",
        "\n",
        "    async def set_title(self, title: str):\n",
        "        \"\"\"Sets the title of the context, which is shown in the UI.\"\"\"\n",
        "        request = {\n",
        "            \"contextId\": self.context_id,\n",
        "            \"title\": title,\n",
        "        }\n",
        "        await js.setContextTitle(pyodide.ffi.create_proxy(request))\n",
        "\n",
        "    def __enter__(self):\n",
        "        \"\"\"Uses this context as the current context.\"\"\"\n",
        "        if self._reset_token is not None:\n",
        "            raise RuntimeError(\"Cannot enter a context twice.\")\n",
        "        self._reset_token = _current_ctx.set(self)\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"Exits the context and resets the global state.\"\"\"\n",
        "        _current_ctx.reset(self._reset_token)\n",
        "        self._reset_token = None\n",
        "\n",
        "\n",
        "def get_context() -> Context:\n",
        "    \"\"\"Returns the current context.\"\"\"\n",
        "    if _force_ctx.get() is not None:\n",
        "        return _force_ctx.get()\n",
        "    return _current_ctx.get()\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def force_context(ctx: Context):\n",
        "    \"\"\"Overrides the current context with the provided one.\"\"\"\n",
        "    token = _force_ctx.set(ctx)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        _force_ctx.reset(token)\n",
        "\n",
        "\n",
        "# The following functions operate on the current context.\n",
        "\n",
        "\n",
        "def as_string() -> str:\n",
        "    \"\"\"See `Context.as_string`.\"\"\"\n",
        "    return get_context().as_string()\n",
        "\n",
        "\n",
        "def select_model(model_name: str):\n",
        "    \"\"\"See `Context.select_model`.\"\"\"\n",
        "    return get_context().select_model(model_name)\n",
        "\n",
        "\n",
        "def as_token_ids() -> list[int]:\n",
        "    \"\"\"See `Context.as_token_ids`.\"\"\"\n",
        "    return get_context().as_token_ids()\n",
        "\n",
        "\n",
        "async def prompt(text: str, strip: bool = False) -> Sequence[Token]:\n",
        "    \"\"\"See `Context.prompt`.\"\"\"\n",
        "    return await get_context().prompt(text, strip)\n",
        "\n",
        "\n",
        "def randomize_rng_seed() -> int:\n",
        "    \"\"\"See `Context.randomize_rng_seed`.\"\"\"\n",
        "    return get_context().randomize_rng_seed()\n",
        "\n",
        "\n",
        "def create_context() -> \"Context\":\n",
        "    \"\"\"See `Context.create_context()`.\"\"\"\n",
        "    return get_context().create_context()\n",
        "\n",
        "\n",
        "async def set_title(title: str):\n",
        "    \"\"\"See `Context.set_title`.\"\"\"\n",
        "    await get_context().set_title(title)\n",
        "\n",
        "\n",
        "async def sample(\n",
        "    max_len: int = 256,\n",
        "    temperature: float = 1.0,\n",
        "    nucleus_p: float = 0.7,\n",
        "    stop_tokens: Optional[list[str]] = None,\n",
        "    stop_strings: Optional[list[str]] = None,\n",
        "    rng_seed: Optional[int] = None,\n",
        "    add_to_context: bool = True,\n",
        "    return_attention: bool = False,\n",
        "    allowed_tokens: Optional[Sequence[Union[int, str]]] = None,\n",
        "    disallowed_tokens: Optional[Sequence[Union[int, str]]] = None,\n",
        "):\n",
        "    \"\"\"See `Context.sample`.\"\"\"\n",
        "    return await get_context().sample(\n",
        "        max_len,\n",
        "        temperature,\n",
        "        nucleus_p,\n",
        "        stop_tokens,\n",
        "        stop_strings,\n",
        "        rng_seed,\n",
        "        add_to_context,\n",
        "        return_attention,\n",
        "        allowed_tokens,\n",
        "        disallowed_tokens,\n",
        "    )\n",
        "\n",
        "\n",
        "def clone() -> \"Context\":\n",
        "    \"\"\"See `Context.clone`.\"\"\"\n",
        "    return get_context().clone()\n",
        "\n",
        "\n",
        "def prompt_fn(fn):\n",
        "    \"\"\"A context manager that executes `fn` in a fresh prompt context.\n",
        "\n",
        "    If a function is annotated with this context manager, a fresh prompt context is created that\n",
        "    the function operates on. This allows solving sub-problems with different prompt and\n",
        "    incorporating the solution to a sub problems into the original one.\n",
        "\n",
        "    Example:\n",
        "        ```\n",
        "        @prompt_fn\n",
        "        async def add(a, b):\n",
        "            prompt(f\"{a}+{b}=\")\n",
        "            result = await sample(max_len=10, stop_strings=[\" \"])\n",
        "            return result.as_string().split(\" \")[0]\n",
        "        ```\n",
        "\n",
        "    In order to get access to the context used by an annotated function, the function must return\n",
        "    it like this:\n",
        "\n",
        "    ```\n",
        "        @prompt_fn\n",
        "        def foo():\n",
        "            return get_context()\n",
        "    ```\n",
        "\n",
        "    You can override the context an annotated function uses. This is useful if you want to continue\n",
        "    operating on a context that was created by a function.\n",
        "\n",
        "    ```\n",
        "        @prompt_fn\n",
        "        async def bar():\n",
        "            async prompt(\"1+1=\")\n",
        "            return get_context()\n",
        "\n",
        "        @prompt_fn\n",
        "        async def foo():\n",
        "            await sample(max_len=24)\n",
        "\n",
        "        ctx = await bar()\n",
        "        with force_context(ctx):\n",
        "            foo()\n",
        "    ```\n",
        "\n",
        "    Args:\n",
        "        fn: An asynchronous function to execute in a newly created context.\n",
        "\n",
        "    Returns:\n",
        "        The wrapped function.\n",
        "    \"\"\"\n",
        "\n",
        "    async def _fn(*args, **kwargs):\n",
        "        with get_context().create_context() as ctx:\n",
        "            await ctx.set_title(fn.__name__)\n",
        "            return await fn(*args, **kwargs)\n",
        "\n",
        "    return _fn\n",
        "\n",
        "\n",
        "async def read_file(file_name: str) -> bytes:\n",
        "    \"\"\"Reads a file that the user has uploaded to the file manager.\n",
        "\n",
        "    Args:\n",
        "        file_name: Name of the file to read.\n",
        "\n",
        "    Returns:\n",
        "        The file's content as raw bytes array.\n",
        "    \"\"\"\n",
        "    result = await js.readFile(pyodide.ffi.create_proxy(file_name))\n",
        "    return result.to_py().tobytes()"
      ]
    }
  ]
}