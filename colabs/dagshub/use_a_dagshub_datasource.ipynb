{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb11af6efcea45e8bf9ada7918703b77": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f5963fc1fcae4b519636f726fef115dc",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m⠼\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠼</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "f5963fc1fcae4b519636f726fef115dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rebornrulz/rulz-ai/blob/master/colabs/dagshub/use_a_dagshub_datasource.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use the my-first-ds datasource"
      ],
      "metadata": {
        "id": "aXbzbUOWPL5e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2JvR9_vR4VyK",
        "outputId": "5cb963fa-a237-459c-a227-e4a163cc1012",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fusepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install the DagsHub client\n",
        "!pip install -q dagshub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dagshub.data_engine import datasources\n",
        "ds = datasources.get('rebornrulz/rulz-ai', 'my-first-ds')"
      ],
      "metadata": {
        "id": "vYv-arA5M154",
        "outputId": "1abbd894-a6fa-4393-aeec-cfbfaf69811e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160,
          "referenced_widgets": [
            "eb11af6efcea45e8bf9ada7918703b77",
            "f5963fc1fcae4b519636f726fef115dc"
          ]
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb11af6efcea45e8bf9ada7918703b77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=664c7e8b-b83e-47bf-82e6-bafa562d27c7&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=28873d02d479bdf39b9fd523efb0dc6c658964f391c6aee45cdff2f3d0452dfb\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query your data\n",
        "Datasources can be filtered using a pandas-like syntax.\n",
        "\n",
        "[For the full list of possible filters, see the docs](https://dagshub.com/docs/use_cases/data_engine/query_and_create_subsets)."
      ],
      "metadata": {
        "id": "OhdD9CXKRY2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import gzip\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os.path\n",
        "import time\n",
        "import webbrowser\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, TYPE_CHECKING, Union, Set, ContextManager\n",
        "\n",
        "import rich.progress\n",
        "from dataclasses_json import dataclass_json, config\n",
        "from pathvalidate import sanitize_filepath\n",
        "\n",
        "import dagshub.common.config\n",
        "from dagshub.common import rich_console\n",
        "from dagshub.common.analytics import send_analytics_event\n",
        "from dagshub.common.helpers import prompt_user, http_request, log_message\n",
        "from dagshub.common.rich_util import get_rich_progress\n",
        "from dagshub.common.util import lazy_load, multi_urljoin\n",
        "from dagshub.data_engine.client.models import PreprocessingStatus, MetadataFieldType, MetadataFieldSchema, \\\n",
        "    ScanOption, autogenerated_columns\n",
        "from dagshub.data_engine.model.datapoint import Datapoint\n",
        "from dagshub.data_engine.model.errors import WrongOperatorError, WrongOrderError, DatasetFieldComparisonError, \\\n",
        "    FieldNotFoundError\n",
        "from dagshub.data_engine.model.query import DatasourceQuery, _metadataTypeLookup, _metadataTypeLookupReverse\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    from dagshub.data_engine.model.query_result import QueryResult\n",
        "    from dagshub.data_engine.model.datasource_state import DatasourceState\n",
        "    import fiftyone as fo\n",
        "    import pandas\n",
        "else:\n",
        "    plugin_server_module = lazy_load(\"dagshub.data_engine.voxel_plugin_server.server\")\n",
        "    fo = lazy_load(\"fiftyone\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class DatapointMetadataUpdateEntry(json.JSONEncoder):\n",
        "    url: str\n",
        "    key: str\n",
        "    value: str\n",
        "    valueType: MetadataFieldType = field(\n",
        "        metadata=config(\n",
        "            encoder=lambda val: val.value\n",
        "        )\n",
        "    )\n",
        "    allowMultiple: bool = False\n",
        "\n",
        "\n",
        "class Datasource:\n",
        "\n",
        "    def __init__(self, datasource: \"DatasourceState\", query: Optional[DatasourceQuery] = None):\n",
        "        self._source = datasource\n",
        "        if query is None:\n",
        "            query = DatasourceQuery()\n",
        "        self._query = query\n",
        "\n",
        "        self.serialize_gql_query_input()\n",
        "\n",
        "    @property\n",
        "    def source(self) -> \"DatasourceState\":\n",
        "        return self._source\n",
        "\n",
        "    def clear_query(self):\n",
        "        \"\"\"\n",
        "        This function clears the query assigned to this datasource.\n",
        "        Once you clear the query, next time you try to get datapoints, you'll get all the datapoints in the datasource\n",
        "        \"\"\"\n",
        "        self._query = DatasourceQuery()\n",
        "\n",
        "    def __deepcopy__(self, memodict={}) -> \"Datasource\":\n",
        "        res = Datasource(self._source, self._query.__deepcopy__())\n",
        "        return res\n",
        "\n",
        "    def get_query(self):\n",
        "        return self._query\n",
        "\n",
        "    @property\n",
        "    def annotation_fields(self) -> List[str]:\n",
        "        # TODO: once the annotation type is implemented, expose those columns here\n",
        "        return [\"annotation\"]\n",
        "\n",
        "    def serialize_gql_query_input(self):\n",
        "        return {\n",
        "            \"query\": self._query.serialize_graphql(),\n",
        "        }\n",
        "\n",
        "    def sample(self, start: Optional[int] = None, end: Optional[int] = None):\n",
        "        if start is not None:\n",
        "            logger.warning(\"Starting slices is not implemented for now\")\n",
        "        return self._source.client.sample(self, end, include_metadata=True)\n",
        "\n",
        "    def head(self, size=100) -> \"QueryResult\":\n",
        "        \"\"\"\n",
        "        Executes the query and returns a QueryResult object containing first <size> datapoints\n",
        "\n",
        "        Args:\n",
        "            size: how many datapoints to get. Default is 100\n",
        "        \"\"\"\n",
        "        self._check_preprocess()\n",
        "        send_analytics_event(\"Client_DataEngine_DisplayTopResults\", repo=self.source.repoApi)\n",
        "        return self._source.client.head(self, size)\n",
        "\n",
        "    def all(self) -> \"QueryResult\":\n",
        "        \"\"\"\n",
        "        Executes the query and returns a QueryResult object containing all datapoints\n",
        "        \"\"\"\n",
        "        self._check_preprocess()\n",
        "        return self._source.client.get_datapoints(self)\n",
        "\n",
        "    def _check_preprocess(self):\n",
        "        self.source.get_from_dagshub()\n",
        "        if (self.source.preprocessing_status == PreprocessingStatus.IN_PROGRESS or\n",
        "            self.source.preprocessing_status == PreprocessingStatus.AUTO_SCAN_IN_PROGRESS):\n",
        "            logger.warning(\n",
        "                f\"Datasource {self.source.name} is currently in the progress of rescanning. \"\n",
        "                f\"Values might change if you requery later\")\n",
        "\n",
        "    def metadata_context(self) -> ContextManager[\"MetadataContextManager\"]:\n",
        "        \"\"\"\n",
        "        Returns a metadata context, that you can upload metadata through via update_metadata\n",
        "        Once the context is exited, all metadata is uploaded in one batch\n",
        "\n",
        "        with df.metadata_context() as ctx:\n",
        "            ctx.update_metadata([\"file1\", \"file2\"], {\"key1\": True, \"key2\": \"value\"})\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Need to have the context manager inside a wrapper to satisfy MyPy + PyCharm type hinter\n",
        "        @contextmanager\n",
        "        def func():\n",
        "            self.source.get_from_dagshub()\n",
        "            send_analytics_event(\"Client_DataEngine_addEnrichments\", repo=self.source.repoApi)\n",
        "            ctx = MetadataContextManager(self)\n",
        "            yield ctx\n",
        "            self._upload_metadata(ctx.get_metadata_entries())\n",
        "\n",
        "        return func()\n",
        "\n",
        "    def upload_metadata_from_dataframe(self, df: \"pandas.DataFrame\", path_column: Optional[Union[str, int]] = None):\n",
        "        \"\"\"\n",
        "        Uploads metadata from a pandas dataframe\n",
        "        path_column can either be a name of the column with the data or its index.\n",
        "        This will be the column from which the datapoints are extracted.\n",
        "        All the other columns are treated as metadata to upload\n",
        "        If path_column is not specified, the first column is used as the datapoints\n",
        "        \"\"\"\n",
        "        self.source.get_from_dagshub()\n",
        "        send_analytics_event(\"Client_DataEngine_addEnrichmentsWithDataFrame\", repo=self.source.repoApi)\n",
        "        self._upload_metadata(self._df_to_metadata(df, path_column, multivalue_fields=self._get_multivalue_fields()))\n",
        "\n",
        "    def _get_multivalue_fields(self) -> Set[str]:\n",
        "        res = set()\n",
        "        for col in self.source.metadata_fields:\n",
        "            if col.multiple:\n",
        "                res.add(col.name)\n",
        "        return res\n",
        "\n",
        "    def _df_to_metadata(self, df: \"pandas.DataFrame\", path_column: Optional[Union[str, int]] = None,\n",
        "                        multivalue_fields=set()) -> List[\n",
        "        DatapointMetadataUpdateEntry]:\n",
        "        res: List[DatapointMetadataUpdateEntry] = []\n",
        "        if path_column is None:\n",
        "            path_column = df.columns[0]\n",
        "        elif type(path_column) is str:\n",
        "            if path_column not in df.columns:\n",
        "                raise RuntimeError(f\"Column {path_column} does not exist in the dataframe\")\n",
        "        elif type(path_column) is int:\n",
        "            path_column = df.columns[path_column]\n",
        "\n",
        "        # objects are actually mixed and not guaranteed to be string, but this should cover most use cases\n",
        "        if df.dtypes[path_column] != \"object\":\n",
        "            raise RuntimeError(f\"Column {path_column} doesn't have strings\")\n",
        "\n",
        "        field_value_types = {f.name: f.valueType for f in self.fields}\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            datapoint = row[path_column]\n",
        "            for key, val in row.items():\n",
        "                if key == path_column:\n",
        "                    continue\n",
        "                key = str(key)\n",
        "                if key in autogenerated_columns:\n",
        "                    continue\n",
        "                if val is None:\n",
        "                    continue\n",
        "                # ONLY FOR PANDAS: since pandas doesn't distinguish between None and NaN, don't upload it\n",
        "                if type(val) is float and math.isnan(val):\n",
        "                    continue\n",
        "                if type(val) is list:\n",
        "                    if key not in multivalue_fields:\n",
        "                        multivalue_fields.add(key)\n",
        "                        # Promote all the existing uploading metadata to multivalue\n",
        "                        for update_entry in res:\n",
        "                            if update_entry.key == key:\n",
        "                                update_entry.allowMultiple = True\n",
        "                    for sub_val in val:\n",
        "                        value_type = field_value_types.get(key)\n",
        "                        if value_type is None:\n",
        "                            value_type = _metadataTypeLookup[type(sub_val)]\n",
        "                            field_value_types[key] = value_type\n",
        "                        # Don't override bytes if they're not bytes - probably just undownloaded values\n",
        "                        if value_type == MetadataFieldType.BLOB and type(sub_val) is not bytes:\n",
        "                            continue\n",
        "                        # Pandas quirk - integers are floats on the backend\n",
        "                        if value_type == MetadataFieldType.INTEGER:\n",
        "                            sub_val = int(sub_val)\n",
        "                        if type(sub_val) is bytes:\n",
        "                            sub_val = MetadataContextManager.wrap_bytes(sub_val)\n",
        "                        res.append(DatapointMetadataUpdateEntry(\n",
        "                            url=datapoint,\n",
        "                            key=key,\n",
        "                            value=str(sub_val),\n",
        "                            valueType=value_type,\n",
        "                            allowMultiple=True\n",
        "                        ))\n",
        "                else:\n",
        "                    value_type = field_value_types.get(key)\n",
        "                    if value_type is None:\n",
        "                        value_type = _metadataTypeLookup[type(val)]\n",
        "                        field_value_types[key] = value_type\n",
        "                    # Don't override bytes if they're not bytes - probably just undownloaded values\n",
        "                    if value_type == MetadataFieldType.BLOB and type(val) is not bytes:\n",
        "                        continue\n",
        "                    # Pandas quirk - integers are floats on the backend\n",
        "                    if value_type == MetadataFieldType.INTEGER:\n",
        "                        val = int(val)\n",
        "                    if type(val) is bytes:\n",
        "                        val = MetadataContextManager.wrap_bytes(val)\n",
        "                    res.append(DatapointMetadataUpdateEntry(\n",
        "                        url=datapoint,\n",
        "                        key=key,\n",
        "                        value=str(val),\n",
        "                        valueType=value_type,\n",
        "                        allowMultiple=key in multivalue_fields\n",
        "                    ))\n",
        "        return res\n",
        "\n",
        "    def delete_source(self, force: bool = False):\n",
        "        \"\"\"\n",
        "        Delete the record of this datasource\n",
        "        This will remove ALL the datapoints + metadata associated with the datasource\n",
        "        \"\"\"\n",
        "        prompt = f\"You are about to delete datasource \\\"{self.source.name}\\\" for repo \\\"{self.source.repo}\\\"\\n\" \\\n",
        "                 f\"This will remove the datasource and ALL datapoints \" \\\n",
        "                 f\"and metadata records associated with the source.\"\n",
        "        if not force:\n",
        "            user_response = prompt_user(prompt)\n",
        "            if not user_response:\n",
        "                print(\"Deletion cancelled\")\n",
        "                return\n",
        "        self.source.client.delete_datasource(self)\n",
        "\n",
        "    def scan_source(self, options: Optional[List[ScanOption]] = None):\n",
        "        \"\"\"\n",
        "        This function fires a call to the backend to rescan the datapoints.\n",
        "        Call this function whenever you uploaded new files and want them to appear when querying the datasource,\n",
        "        Or if you changed existing file contents and want their metadata to be updated automatically.\n",
        "\n",
        "        Notes about automatically scanned metadata:\n",
        "        1. Only new datapoints (files) will be added.\n",
        "           If files were removed from the source, their metadata will still remain,\n",
        "           and they will still be returned from queries on the datasource.\n",
        "           An API to actively remove metadata will be available soon.\n",
        "        2. Some metadata fields will be automatically scanned and updated by DagsHub based on this scan -\n",
        "           the list of automatic metadata fields is growing frequently!\n",
        "\n",
        "        :param options: List of scanning options. If not sure, leave empty.\n",
        "        \"\"\"\n",
        "        logger.debug(\"Rescanning datasource\")\n",
        "        self.source.client.scan_datasource(self, options=options)\n",
        "\n",
        "    def _upload_metadata(self, metadata_entries: List[DatapointMetadataUpdateEntry]):\n",
        "\n",
        "        progress = get_rich_progress(rich.progress.MofNCompleteColumn())\n",
        "\n",
        "        upload_batch_size = dagshub.common.config.dataengine_metadata_upload_batch_size\n",
        "        total_entries = len(metadata_entries)\n",
        "        total_task = progress.add_task(f\"Uploading metadata (batch size {upload_batch_size})...\",\n",
        "                                       total=total_entries)\n",
        "\n",
        "        with progress:\n",
        "            for start in range(0, total_entries, upload_batch_size):\n",
        "                entries = metadata_entries[start:start + upload_batch_size]\n",
        "                logger.debug(f\"Uploading {len(entries)} metadata entries...\")\n",
        "                self.source.client.update_metadata(self, entries)\n",
        "                progress.update(total_task, advance=upload_batch_size)\n",
        "            progress.update(total_task, completed=total_entries, refresh=True)\n",
        "\n",
        "        # Update the status from dagshub, so we get back the new metadata columns\n",
        "        self.source.get_from_dagshub()\n",
        "\n",
        "    def save_dataset(self, name: str):\n",
        "        \"\"\"\n",
        "        Save the dataset, which is a combination of datasource + query, on the backend.\n",
        "        That way you can persist and share your queries on the backend\n",
        "        You can get the dataset back by calling `datasources.get_dataset(repo, name)`\n",
        "        \"\"\"\n",
        "        send_analytics_event(\"Client_DataEngine_QuerySaved\", repo=self.source.repoApi)\n",
        "\n",
        "        self.source.client.save_dataset(self, name)\n",
        "        log_message(f\"Dataset {name} saved\")\n",
        "\n",
        "    def to_voxel51_dataset(self, **kwargs) -> \"fo.Dataset\":\n",
        "        \"\"\"\n",
        "        Creates a voxel51 dataset that can be used with `fo.launch_app()` to visualize it\n",
        "\n",
        "        Kwargs:\n",
        "            name (str): name of the dataset (by default uses the same name as the datasource)\n",
        "            force_download (bool): download the dataset even if the size of the files is bigger than 100MB\n",
        "            files_location (str|PathLike): path to the location where to download the local files\n",
        "                default: ~/dagshub_datasets/user/repo/ds_name/\n",
        "            redownload (bool): Redownload files, replacing the ones that might exist on the filesystem\n",
        "            voxel_annotations (List[str]) : List of columns from which to load voxel annotations serialized with\n",
        "                                        `to_json()`. This will override the labelstudio annotations\n",
        "        \"\"\"\n",
        "        return self.all().to_voxel51_dataset(**kwargs)\n",
        "\n",
        "    @property\n",
        "    def default_dataset_location(self) -> Path:\n",
        "        return Path(\n",
        "            sanitize_filepath(os.path.join(Path.home(), \"dagshub\", \"datasets\", self.source.repo, str(self.source.id))))\n",
        "\n",
        "    def visualize(self, **kwargs) -> \"fo.Session\":\n",
        "        return self.all().visualize(**kwargs)\n",
        "\n",
        "    @property\n",
        "    def fields(self) -> List[MetadataFieldSchema]:\n",
        "        return self.source.metadata_fields\n",
        "\n",
        "    def annotate(self) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Sends all datapoints in the datasource for annotation in Label Studio.\n",
        "        It's recommended to not send a huge amount of datapoints to be annotated at once, to avoid overloading\n",
        "        The Label Studio workspace.\n",
        "\n",
        "        :return: Link to open Label Studio in the browser\n",
        "        \"\"\"\n",
        "        return self.all().annotate()\n",
        "\n",
        "    def send_to_annotation(self):\n",
        "        \"\"\"\n",
        "        deprecated, see annotate()\n",
        "        \"\"\"\n",
        "        return self.annotate()\n",
        "\n",
        "    def send_datapoints_to_annotation(self, datapoints: Union[List[Datapoint], \"QueryResult\", List[Dict]],\n",
        "                                      open_project=True, ignore_warning=False) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Sends datapoints to annotations in Label Studio\n",
        "\n",
        "        :param datapoints: Either a list of Datapoints or dicts that have \"id\" and \"downloadurl\" fields.\n",
        "                     A QueryResult can also function as a list of Datapoint.\n",
        "        :param open_project: Specifies whether the link to the returned LS project should be opened from Python\n",
        "        :param ignore_warning: Suppress any non-lethal warnings that require user input\n",
        "        :return: Link to open Label Studio in the browser\n",
        "        \"\"\"\n",
        "        if len(datapoints) == 0:\n",
        "            logger.warning(\"No datapoints provided to be sent to annotation\")\n",
        "            return None\n",
        "        elif len(datapoints) > dagshub.common.config.recommended_annotate_limit and not ignore_warning:\n",
        "            force = prompt_user(f\"You are attempting to annotate {len(datapoints)} datapoints at once - it's \"\n",
        "                                f\"recommended to only annotate up to \"\n",
        "                                f\"{dagshub.common.config.recommended_annotate_limit} \"\n",
        "                                f\"datapoints at a time.\")\n",
        "            if not force:\n",
        "                return \"\"\n",
        "\n",
        "        req_data = {\n",
        "            \"datasource_id\": self.source.id,\n",
        "            \"datapoints\": []\n",
        "        }\n",
        "\n",
        "        for dp in datapoints:\n",
        "            req_dict = {}\n",
        "            if type(dp) is dict:\n",
        "                req_dict[\"id\"] = dp[\"datapoint_id\"]\n",
        "                req_dict[\"download_url\"] = dp[\"download_url\"]\n",
        "            else:\n",
        "                req_dict[\"id\"] = dp.datapoint_id\n",
        "                req_dict[\"download_url\"] = dp.download_url\n",
        "            req_data[\"datapoints\"].append(req_dict)\n",
        "\n",
        "        init_url = multi_urljoin(self.source.repoApi.data_engine_url, \"annotations/init\")\n",
        "        resp = http_request(\"POST\", init_url, json=req_data, auth=self.source.repoApi.auth)\n",
        "\n",
        "        if resp.status_code != 200:\n",
        "            logger.error(f\"Error while sending request for annotation: {resp.content}\")\n",
        "            return None\n",
        "        link = resp.json()[\"link\"]\n",
        "\n",
        "        # Do a raw print so it works in colab/jupyter\n",
        "        print(\"Open the following link to start working on your annotation project:\")\n",
        "        print(link)\n",
        "\n",
        "        if open_project:\n",
        "            webbrowser.open_new_tab(link)\n",
        "        return link\n",
        "\n",
        "    def _launch_annotation_workspace(self):\n",
        "        try:\n",
        "            start_workspace_url = multi_urljoin(self.source.repoApi.annotations_url, \"start\")\n",
        "            http_request(\"POST\", start_workspace_url, auth=self.source.repoApi.auth)\n",
        "        except:  # noqa\n",
        "            pass\n",
        "\n",
        "    def wait_until_ready(self, max_wait_time=300, fail_on_timeout=True):\n",
        "        \"\"\"\n",
        "       Blocks until the datasource preprocessing is complete\n",
        "\n",
        "       Args:\n",
        "           max_wait_time (int): Maximum time to wait in seconds\n",
        "           fail_on_timeout: Whether to raise a RuntimeError or continue if the scan does not complete on time\n",
        "       \"\"\"\n",
        "\n",
        "        # Start LS workspace to save time later in the flow\n",
        "        self._launch_annotation_workspace()\n",
        "\n",
        "        start = time.time()\n",
        "        if max_wait_time:\n",
        "            rich_console.log(f\"Maximum waiting time set to {int(max_wait_time / 60)} minutes\")\n",
        "        spinner = rich_console.status(\"Waiting for datasource preprocessing to complete...\")\n",
        "        with spinner:\n",
        "            while True:\n",
        "                self.source.get_from_dagshub()\n",
        "                if self.source.preprocessing_status == PreprocessingStatus.READY:\n",
        "                    return\n",
        "\n",
        "                if self.source.preprocessing_status == PreprocessingStatus.FAILED:\n",
        "                    raise RuntimeError(\"Datasource preprocessing failed\")\n",
        "\n",
        "                if max_wait_time is not None and (time.time() - start) > max_wait_time:\n",
        "                    if fail_on_timeout:\n",
        "                        raise RuntimeError(\n",
        "                            f\"Time limit of {max_wait_time} seconds reached before processing was completed.\")\n",
        "                    else:\n",
        "                        logger.warning(\n",
        "                            f\"Time limit of {max_wait_time} seconds reached before processing was completed.\")\n",
        "                        return\n",
        "\n",
        "                time.sleep(1)\n",
        "\n",
        "    def has_field(self, field_name: str):\n",
        "        reserved_searchable_fields = [\"path\"]\n",
        "        fields = (f.name for f in self.fields)\n",
        "        return field_name in reserved_searchable_fields or field_name in fields\n",
        "\n",
        "    def __repr__(self):\n",
        "        res = f\"Datasource {self.source.name}\"\n",
        "        res += f\"\\n\\tRepo: {self.source.repo}, path: {self.source.path}\"\n",
        "        res += f\"\\n\\t{self._query}\"\n",
        "        res += \"\\n\\tFields:\"\n",
        "        for f in self.fields:\n",
        "            res += f\"\\n\\t\\t{f}\"\n",
        "        return res + \"\\n\"\n",
        "\n",
        "    \"\"\" FUNCTIONS RELATED TO QUERYING\n",
        "    These are functions that overload operators on the DataSet, so you can do pandas-like filtering\n",
        "        ds = Dataset(...)\n",
        "        queried_ds = ds[ds[\"value\"] == 5]\n",
        "    \"\"\"\n",
        "\n",
        "    def __getitem__(self, other: Union[slice, str, \"Datasource\"]):\n",
        "        # Slicing - get items from the slice\n",
        "        if type(other) is slice:\n",
        "            return self.sample(other.start, other.stop)\n",
        "\n",
        "        # Otherwise we're doing querying\n",
        "        new_ds = self.__deepcopy__()\n",
        "        if type(other) is str:\n",
        "            if not self.has_field(other):\n",
        "                raise FieldNotFoundError(other)\n",
        "            new_ds._query = DatasourceQuery(other)\n",
        "            return new_ds\n",
        "        else:\n",
        "            # \"index\" is a datasource with a query - compose with \"and\"\n",
        "            # Example:\n",
        "            #   ds = Dataset()\n",
        "            #   filtered_ds = ds[ds[\"aaa\"] > 5]\n",
        "            #   filtered_ds2 = filtered_ds[filtered_ds[\"bbb\"] < 4]\n",
        "            if self._query.is_empty:\n",
        "                new_ds._query = other._query\n",
        "                return new_ds\n",
        "            else:\n",
        "                return other.__and__(self)\n",
        "\n",
        "    def __gt__(self, other: object):\n",
        "        self._test_not_comparing_other_ds(other)\n",
        "        if not isinstance(other, (int, float, str)):\n",
        "            raise NotImplementedError\n",
        "        return self.add_query_op(\"gt\", other)\n",
        "\n",
        "    def __ge__(self, other: object):\n",
        "        self._test_not_comparing_other_ds(other)\n",
        "        if not isinstance(other, (int, float, str)):\n",
        "            raise NotImplementedError\n",
        "        return self.add_query_op(\"ge\", other)\n",
        "\n",
        "    def __le__(self, other: object):\n",
        "        self._test_not_comparing_other_ds(other)\n",
        "        if not isinstance(other, (int, float, str)):\n",
        "            raise NotImplementedError\n",
        "        return self.add_query_op(\"le\", other)\n",
        "\n",
        "    def __lt__(self, other: object):\n",
        "        self._test_not_comparing_other_ds(other)\n",
        "        if not isinstance(other, (int, float, str)):\n",
        "            raise NotImplementedError\n",
        "        return self.add_query_op(\"lt\", other)\n",
        "\n",
        "    def __eq__(self, other: object):\n",
        "        self._test_not_comparing_other_ds(other)\n",
        "        if other is None:\n",
        "            return self.is_null()\n",
        "        if not isinstance(other, (int, float, str)):\n",
        "            raise NotImplementedError\n",
        "        return self.add_query_op(\"eq\", other)\n",
        "\n",
        "    def __ne__(self, other: object):\n",
        "        self._test_not_comparing_other_ds(other)\n",
        "        if other is None:\n",
        "            return self.is_not_null()\n",
        "        if not isinstance(other, (int, float, str)):\n",
        "            raise NotImplementedError\n",
        "        return self.add_query_op(\"eq\", other).add_query_op(\"not\")\n",
        "\n",
        "    def __invert__(self):\n",
        "        return self.add_query_op(\"not\")\n",
        "\n",
        "    def __contains__(self, item):\n",
        "        raise WrongOperatorError(\"Use `ds.contains(a)` for querying instead of `a in ds`\")\n",
        "\n",
        "    def contains(self, item: str):\n",
        "        if type(item) is not str:\n",
        "            return WrongOperatorError(f\"Cannot use contains with non-string value {item}\")\n",
        "        self._test_not_comparing_other_ds(item)\n",
        "        return self.add_query_op(\"contains\", item)\n",
        "\n",
        "    def is_null(self):\n",
        "        field = self._get_filtering_field()\n",
        "        value_type = _metadataTypeLookupReverse[field.valueType.value]\n",
        "        return self.add_query_op(\"isnull\", value_type())\n",
        "\n",
        "    def is_not_null(self):\n",
        "        return self.is_null().add_query_op(\"not\")\n",
        "\n",
        "    def _get_filtering_field(self) -> MetadataFieldSchema:\n",
        "        field_name = self.get_query().column_filter\n",
        "        if field_name is None:\n",
        "            raise RuntimeError(\"The current query filter is not a field\")\n",
        "        for col in self.source.metadata_fields:\n",
        "            if col.name == field_name:\n",
        "                return col\n",
        "        raise RuntimeError(f\"Field {field_name} doesn't exist in the current uploaded metadata\")\n",
        "\n",
        "    def __and__(self, other: \"Datasource\"):\n",
        "        return self.add_query_op(\"and\", other)\n",
        "\n",
        "    def __or__(self, other: \"Datasource\"):\n",
        "        return self.add_query_op(\"or\", other)\n",
        "\n",
        "    # Prevent users from messing up their queries due to operator order\n",
        "    # They always need to put the dataset query filters in parentheses, otherwise the binary and/or get executed before\n",
        "    def __rand__(self, other):\n",
        "        if type(other) is not Datasource:\n",
        "            raise WrongOrderError(type(other))\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __ror__(self, other):\n",
        "        if type(other) is not Datasource:\n",
        "            raise WrongOrderError(type(other))\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def add_query_op(self, op: str,\n",
        "                     other: Optional[Union[str, int, float, \"Datasource\", \"DatasourceQuery\"]] = None) -> \"Datasource\":\n",
        "        \"\"\"\n",
        "        Returns a new dataset with an added query param\n",
        "        \"\"\"\n",
        "        new_ds = self.__deepcopy__()\n",
        "        if type(other) is Datasource:\n",
        "            other = other.get_query()\n",
        "        new_ds._query.compose(op, other)\n",
        "        return new_ds\n",
        "\n",
        "    @staticmethod\n",
        "    def _test_not_comparing_other_ds(other):\n",
        "        if type(other) is Datasource:\n",
        "            raise DatasetFieldComparisonError()\n",
        "\n",
        "\n",
        "class MetadataContextManager:\n",
        "    def __init__(self, datasource: Datasource):\n",
        "        self._datasource = datasource\n",
        "        self._metadata_entries: List[DatapointMetadataUpdateEntry] = []\n",
        "        self._multivalue_fields = datasource._get_multivalue_fields()\n",
        "\n",
        "    def update_metadata(self, datapoints: Union[List[str], str], metadata: Dict[str, Any]):\n",
        "        if isinstance(datapoints, str):\n",
        "            datapoints = [datapoints]\n",
        "\n",
        "        field_value_types = {f.name: f.valueType for f in self._datasource.fields}\n",
        "\n",
        "        for dp in datapoints:\n",
        "            for k, v in metadata.items():\n",
        "                if v is None:\n",
        "                    continue\n",
        "                if k in autogenerated_columns:\n",
        "                    continue\n",
        "\n",
        "                if type(v) is list:\n",
        "                    if k not in self._multivalue_fields:\n",
        "                        self._multivalue_fields.add(k)\n",
        "                        # Promote all existing ones to multivalue\n",
        "                        for e in self._metadata_entries:\n",
        "                            if e.key == k:\n",
        "                                e.allowMultiple = True\n",
        "                    for sub_val in v:\n",
        "\n",
        "                        value_type = field_value_types.get(k)\n",
        "                        if value_type is None:\n",
        "                            value_type = _metadataTypeLookup[type(sub_val)]\n",
        "                            field_value_types[k] = value_type\n",
        "                        # Don't override bytes if they're not bytes - probably just undownloaded values\n",
        "                        if value_type == MetadataFieldType.BLOB and type(sub_val) is not bytes:\n",
        "                            continue\n",
        "\n",
        "                        if type(v) is bytes:\n",
        "                            sub_val = self.wrap_bytes(sub_val)\n",
        "                        self._metadata_entries.append(DatapointMetadataUpdateEntry(\n",
        "                            url=dp,\n",
        "                            key=k,\n",
        "                            value=str(sub_val),\n",
        "                            # todo: preliminary type check\n",
        "                            valueType=value_type,\n",
        "                            allowMultiple=k in self._multivalue_fields\n",
        "                        ))\n",
        "\n",
        "                else:\n",
        "\n",
        "                    value_type = field_value_types.get(k)\n",
        "                    if value_type is None:\n",
        "                        value_type = _metadataTypeLookup[type(v)]\n",
        "                        field_value_types[k] = value_type\n",
        "                    # Don't override bytes if they're not bytes - probably just undownloaded values\n",
        "                    if value_type == MetadataFieldType.BLOB and type(v) is not bytes:\n",
        "                        continue\n",
        "\n",
        "                    if type(v) is bytes:\n",
        "                        v = self.wrap_bytes(v)\n",
        "                    self._metadata_entries.append(DatapointMetadataUpdateEntry(\n",
        "                        url=dp,\n",
        "                        key=k,\n",
        "                        value=str(v),\n",
        "                        valueType=value_type,\n",
        "                        # todo: preliminary type check\n",
        "                        allowMultiple=k in self._multivalue_fields\n",
        "                    ))\n",
        "\n",
        "    @staticmethod\n",
        "    def wrap_bytes(val: bytes) -> str:\n",
        "        \"\"\"\n",
        "        Handles bytes values for uploading metadata\n",
        "        The process is gzip -> base64\n",
        "        \"\"\"\n",
        "        compressed = gzip.compress(val)\n",
        "        return base64.b64encode(compressed).decode(\"utf-8\")\n",
        "\n",
        "    def get_metadata_entries(self):\n",
        "        return self._metadata_entries\n"
      ],
      "metadata": {
        "id": "F03-svKzgUoE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Data Engine Capabilities\n",
        "\n",
        "Data Engine capabilities also includes: [data enrichments](https://dagshub.com/docs/use_cases/data_engine/enrich_datasource/), [visualizations](https://dagshub.com/docs/use_cases/data_engine/visualizing_datasets/), [annotations](https://dagshub.com/docs/use_cases/data_engine/annotate_data/), [creating datasets](https://dagshub.com/docs/use_cases/data_engine/query_and_create_subsets/) and [model training](https://dagshub.com/docs/use_cases/data_engine/train_model/)."
      ],
      "metadata": {
        "id": "j-dlmYBZHKUB"
      }
    }
  ]
}