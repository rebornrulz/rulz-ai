{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb11af6efcea45e8bf9ada7918703b77": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f5963fc1fcae4b519636f726fef115dc",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m⠼\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠼</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "f5963fc1fcae4b519636f726fef115dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rebornrulz/rulz-ai/blob/master/colabs/dagshub/use_a_dagshub_datasource.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use the my-first-ds datasource"
      ],
      "metadata": {
        "id": "aXbzbUOWPL5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "e7h_JOpq3LwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JvR9_vR4VyK",
        "outputId": "5cb963fa-a237-459c-a227-e4a163cc1012",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fusepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install the DagsHub client\n",
        "!pip install -q dagshub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dagshub.data_engine import datasources\n",
        "ds = datasources.get('rebornrulz/rulz-ai', 'my-first-ds')"
      ],
      "metadata": {
        "id": "vYv-arA5M154",
        "outputId": "1abbd894-a6fa-4393-aeec-cfbfaf69811e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160,
          "referenced_widgets": [
            "eb11af6efcea45e8bf9ada7918703b77",
            "f5963fc1fcae4b519636f726fef115dc"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb11af6efcea45e8bf9ada7918703b77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=664c7e8b-b83e-47bf-82e6-bafa562d27c7&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=28873d02d479bdf39b9fd523efb0dc6c658964f391c6aee45cdff2f3d0452dfb\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query your data\n",
        "Datasources can be filtered using a pandas-like syntax.\n",
        "\n",
        "[For the full list of possible filters, see the docs](https://dagshub.com/docs/use_cases/data_engine/query_and_create_subsets)."
      ],
      "metadata": {
        "id": "OhdD9CXKRY2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import gzip\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os.path\n",
        "import time\n",
        "import webbrowser\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, TYPE_CHECKING, Union, Set, ContextManager\n",
        "\n",
        "import rich.progress\n",
        "from dataclasses_json import dataclass_json, config\n",
        "from pathvalidate import sanitize_filepath\n",
        "\n",
        "import dagshub.common.config\n",
        "from dagshub.common import rich_console\n",
        "from dagshub.common.analytics import send_analytics_event\n",
        "from dagshub.common.helpers import prompt_user, http_request, log_message\n",
        "from dagshub.common.rich_util import get_rich_progress\n",
        "from dagshub.common.util import lazy_load, multi_urljoin\n",
        "from dagshub.data_engine.client.models import PreprocessingStatus, MetadataFieldType, MetadataFieldSchema, \\\n",
        "    ScanOption, autogenerated_columns\n",
        "from dagshub.data_engine.model.datapoint import Datapoint\n",
        "from dagshub.data_engine.model.errors import WrongOperatorError, WrongOrderError, DatasetFieldComparisonError, \\\n",
        "    FieldNotFoundError\n",
        "from dagshub.data_engine.model.query import DatasourceQuery, _metadataTypeLookup, _metadataTypeLookupReverse\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    from dagshub.data_engine.model.query_result import QueryResult\n",
        "    from dagshub.data_engine.model.datasource_state import DatasourceState\n",
        "    import fiftyone as fo\n",
        "    import pandas\n",
        "else:\n",
        "    plugin_server_module = lazy_load(\"dagshub.data_engine.voxel_plugin_server.server\")\n",
        "    fo = lazy_load(\"fiftyone\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class DatapointMetadataUpdateEntry(json.JSONEncoder):\n",
        "    url: str\n",
        "    key: str\n",
        "    value: str\n",
        "    valueType: MetadataFieldType = field(\n",
        "        metadata=config(\n",
        "            encoder=lambda val: val.value\n",
        "        )\n",
        "    )\n",
        "    allowMultiple: bool = False\n",
        "\n",
        "\n",
        "class Datasource:\n",
        "\n",
        "    def __init__(self, datasource: \"DatasourceState\", query: Optional[DatasourceQuery] = None):\n",
        "        self._source = datasource\n",
        "        if query is None:\n",
        "            query = DatasourceQuery()\n",
        "        self._query = query\n",
        "\n",
        "        self.serialize_gql_query_input()\n",
        "\n",
        "    @property\n",
        "    def source(self) -> \"DatasourceState\":\n",
        "        return self._source\n",
        "\n",
        "    def clear_query(self):\n",
        "        \"\"\"\n",
        "        This function clears the query assigned to this datasource.\n",
        "        Once you clear the query, next time you try to get datapoints, you'll get all the datapoints in the datasource\n",
        "        \"\"\"\n",
        "        self._query = DatasourceQuery()\n",
        "\n",
        "    def __deepcopy__(self, memodict={}) -> \"Datasource\":\n",
        "        res = Datasource(self._source, self._query.__deepcopy__())\n",
        "        return res\n",
        "\n",
        "    def get_query(self):\n",
        "        return self._query\n",
        "\n",
        "    @property\n",
        "    def annotation_fields(self) -> List[str]:\n",
        "        # TODO: once the annotation type is implemented, expose those columns here\n",
        "        return [\"annotation\"]\n",
        "\n",
        "    def serialize_gql_query_input(self):\n",
        "        return {\n",
        "            \"query\": self._query.serialize_graphql(),\n",
        "        }\n",
        "\n",
        "    def sample(self, start: Optional[int] = None, end: Optional[int] = None):\n",
        "        if start is not None:\n",
        "            logger.warning(\"Starting slices is not implemented for now\")\n",
        "        return self._source.client.sample(self, end, include_metadata=True)\n",
        "\n",
        "    def head(self, size=100) -> \"QueryResult\":\n",
        "        \"\"\"\n",
        "        Executes the query and returns a QueryResult object containing first <size> datapoints\n",
        "\n",
        "        Args:\n",
        "            size: how many datapoints to get. Default is 100\n",
        "        \"\"\"\n",
        "        self._check_preprocess()\n",
        "        send_analytics_event(\"Client_DataEngine_DisplayTopResults\", repo=self.source.repoApi)\n",
        "        return self._source.client.head(self, size)\n",
        "\n",
        "    def all(self) -> \"QueryResult\":\n",
        "        \"\"\"\n",
        "        Executes the query and returns a QueryResult object containing all datapoints\n",
        "        \"\"\"\n",
        "        self._check_preprocess()\n",
        "        return self._source.client.get_datapoints(self)\n",
        "\n",
        "    def _check_preprocess(self):\n",
        "        self.source.get_from_dagshub()\n",
        "        if (self.source.preprocessing_status == PreprocessingStatus.IN_PROGRESS or\n",
        "            self.source.preprocessing_status == PreprocessingStatus.AUTO_SCAN_IN_PROGRESS):\n",
        "            logger.warning(\n",
        "                f\"Datasource {self.source.name} is currently in the progress of rescanning. \"\n",
        "                f\"Values might change if you requery later\")\n",
        "\n",
        "    def metadata_context(self) -> ContextManager[\"MetadataContextManager\"]:\n",
        "        \"\"\"\n",
        "        Returns a metadata context, that you can upload metadata through via update_metadata\n",
        "        Once the context is exited, all metadata is uploaded in one batch\n",
        "\n",
        "        with df.metadata_context() as ctx:\n",
        "            ctx.update_metadata([\"file1\", \"file2\"], {\"key1\": True, \"key2\": \"value\"})\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Need to have the context manager inside a wrapper to satisfy MyPy + PyCharm type hinter\n",
        "        @contextmanager\n",
        "        def func():\n",
        "            self.source.get_from_dagshub()\n",
        "            send_analytics_event(\"Client_DataEngine_addEnrichments\", repo=self.source.repoApi)\n",
        "            ctx = MetadataContextManager(self)\n",
        "            yield ctx\n",
        "            self._upload_metadata(ctx.get_metadata_entries())\n",
        "\n",
        "        return func()\n",
        "\n",
        "    def upload_metadata_from_dataframe(self, df: \"pandas.DataFrame\", path_column: Optional[Union[str, int]] = None):\n",
        "        \"\"\"\n",
        "        Uploads metadata from a pandas dataframe\n",
        "        path_column can either be a name of the column with the data or its index.\n",
        "        This will be the column from which the datapoints are extracted.\n",
        "        All the other columns are treated as metadata to upload\n",
        "        If path_column is not specified, the first column is used as the datapoints\n",
        "        \"\"\"\n",
        "        self.source.get_from_dagshub()\n",
        "        send_analytics_event(\"Client_DataEngine_addEnrichmentsWithDataFrame\", repo=self.source.repoApi)\n",
        "        self._upload_metadata(self._df_to_metadata(df, path_column, multivalue_fields=self._get_multivalue_fields()))\n",
        "\n",
        "    def _get_multivalue_fields(self) -> Set[str]:\n",
        "        res = set()\n",
        "        for col in self.source.metadata_fields:\n",
        "            if col.multiple:\n",
        "                res.add(col.name)\n",
        "        return res\n",
        "\n",
        "    def _df_to_metadata(self, df: \"pandas.DataFrame\", path_column: Optional[Union[str, int]] = None,\n",
        "                        multivalue_fields=set()) -> List[\n",
        "        DatapointMetadataUpdateEntry]:\n",
        "        res: List[DatapointMetadataUpdateEntry] = []\n",
        "        if path_column is None:\n",
        "            path_column = df.columns[0]\n",
        "        elif type(path_column) is str:\n",
        "            if path_column not in df.columns:\n",
        "                raise RuntimeError(f\"Column {path_column} does not exist in the dataframe\")\n",
        "        elif type(path_column) is int:\n",
        "            path_column = df.columns[path_column]\n",
        "\n",
        "        # objects are actually mixed and not guaranteed to be string, but this should cover most use cases\n",
        "        if df.dtypes[path_column] != \"object\":\n",
        "            raise RuntimeError(f\"Column {path_column} doesn't have strings\")\n",
        "\n",
        "        field_value_types = {f.name: f.valueType for f in self.fields}\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            datapoint = row[path_column]\n",
        "            for key, val in row.items():\n",
        "                if key == path_column:\n",
        "                    continue\n",
        "                key = str(key)\n",
        "                if key in autogenerated_columns:\n",
        "                    continue\n",
        "                if val is None:\n",
        "                    continue\n",
        "                # ONLY FOR PANDAS: since pandas doesn't distinguish between None and NaN, don't upload it\n",
        "                if type(val) is float and math.isnan(val):\n",
        "                    continue\n",
        "                if type(val) is list:\n",
        "                    if key not in multivalue_fields:\n",
        "                        multivalue_fields.add(key)\n",
        "                        # Promote all the existing uploading metadata to multivalue\n",
        "                        for update_entry in res:\n",
        "                            if update_entry.key == key:\n",
        "                                update_entry.allowMultiple = True\n",
        "                    for sub_val in val:\n",
        "                        value_type = field_value_types.get(key)\n",
        "                        if value_type is None:\n",
        "                            value_type = _metadataTypeLookup[type(sub_val)]\n",
        "                            field_value_types[key] = value_type\n",
        "                        # Don't override bytes if they're not bytes - probably just undownloaded values\n",
        "                        if value_type == MetadataFieldType.BLOB and type(sub_val) is not bytes:\n",
        "                            continue\n",
        "                        # Pandas quirk - integers are floats on the backend\n",
        "                        if value_type == MetadataFieldType.INTEGER:\n",
        "                            sub_val = int(sub_val)\n",
        "                        if type(sub_val) is bytes:\n",
        "                            sub_val = MetadataContextManager.wrap_bytes(sub_val)\n",
        "                        res.append(DatapointMetadataUpdateEntry(\n",
        "                            url=datapoint,\n",
        "                            key=key,\n",
        "                            value=str(sub_val),\n",
        "                            valueType=value_type,\n",
        "                            allowMultiple=True\n",
        "                        ))\n",
        "                else:\n",
        "                    value_type = field_value_types.get(key)\n",
        "                    if value_type is None:\n",
        "                        value_type = _metadataTypeLookup[type(val)]\n",
        "                        field_value_types[key] = value_type\n",
        "                    # Don't override bytes if they're not bytes - probably just undownloaded values\n",
        "                    if value_type == MetadataFieldType.BLOB and type(val) is not bytes:\n",
        "                        continue\n",
        "                    # Pandas quirk - integers are floats on the backend\n",
        "                    if value_type == MetadataFieldType.INTEGER:\n",
        "                        val = int(val)\n",
        "                    if type(val) is bytes:\n",
        "                        val = MetadataContextManager.wrap_bytes(val)\n",
        "                    res.append(DatapointMetadataUpdateEntry(\n",
        "                        url=datapoint,\n",
        "                        key=key,\n",
        "                        value=str(val),\n",
        "                        valueType=value_type,\n",
        "                        allowMultiple=key in multivalue_fields\n",
        "                    ))\n",
        "        return res\n",
        "\n",
        "    def delete_source(self, force: bool = False):\n",
        "        \"\"\"\n",
        "        Delete the record of this datasource\n",
        "        This will remove ALL the datapoints + metadata associated with the datasource\n",
        "        \"\"\"\n",
        "        prompt = f\"You are about to delete datasource \\\"{self.source.name}\\\" for repo \\\"{self.source.repo}\\\"\\n\" \\\n",
        "                 f\"This will remove the datasource and ALL datapoints \" \\\n",
        "                 f\"and metadata records associated with the source.\"\n",
        "        if not force:\n",
        "            user_response = prompt_user(prompt)\n",
        "            if not user_response:\n",
        "                print(\"Deletion cancelled\")\n",
        "                return\n",
        "        self.source.client.delete_datasource(self)\n",
        "\n",
        "    def scan_source(self, options: Optional[List[ScanOption]] = None):\n",
        "        \"\"\"\n",
        "        This function fires a call to the backend to rescan the datapoints.\n",
        "        Call this function whenever you uploaded new files and want them to appear when querying the datasource,\n",
        "        Or if you changed existing file contents and want their metadata to be updated automatically.\n",
        "\n",
        "        Notes about automatically scanned metadata:\n",
        "        1. Only new datapoints (files) will be added.\n",
        "           If files were removed from the source, their metadata will still remain,\n",
        "           and they will still be returned from queries on the datasource.\n",
        "           An API to actively remove metadata will be available soon.\n",
        "        2. Some metadata fields will be automatically scanned and updated by DagsHub based on this scan -\n",
        "           the list of automatic metadata fields is growing frequently!\n",
        "\n",
        "        :param options: List of scanning options. If not sure, leave empty.\n",
        "        \"\"\"\n",
        "        logger.debug(\"Rescanning datasource\")\n",
        "        self.source.client.scan_datasource(self, options=options)\n",
        "\n",
        "    def _upload_metadata(self, metadata_entries: List[DatapointMetadataUpdateEntry]):\n",
        "\n",
        "        progress = get_rich_progress(rich.progress.MofNCompleteColumn())\n",
        "\n",
        "        upload_batch_size = dagshub.common.config.dataengine_metadata_upload_batch_size\n",
        "        total_entries = len(metadata_entries)\n",
        "        total_task = progress.add_task(f\"Uploading metadata (batch size {upload_batch_size})...\",\n",
        "                                       total=total_entries)\n",
        "\n",
        "        with progress:\n",
        "            for start in range(0, total_entries, upload_batch_size):\n",
        "                entries = metadata_entries[start:start + upload_batch_size]\n",
        "                logger.debug(f\"Uploading {len(entries)} metadata entries...\")\n",
        "                self.source.client.update_metadata(self, entries)\n",
        "                progress.update(total_task, advance=upload_batch_size)\n",
        "            progress.update(total_task, completed=total_entries, refresh=True)\n",
        "\n",
        "        # Update the status from dagshub, so we get back the new metadata columns\n",
        "        self.source.get_from_dagshub()\n",
        "\n",
        "    def save_dataset(self, name: str):\n",
        "        \"\"\"\n",
        "        Save the dataset, which is a combination of datasource + query, on the backend.\n",
        "        That way you can persist and share your queries on the backend\n",
        "        You can get the dataset back by calling `datasources.get_dataset(repo, name)`\n",
        "        \"\"\"\n",
        "        send_analytics_event(\"Client_DataEngine_QuerySaved\", repo=self.source.repoApi)\n",
        "\n",
        "        self.source.client.save_dataset(self, name)\n",
        "        log_message(f\"Dataset {name} saved\")\n",
        "\n",
        "    def to_voxel51_dataset(self, **kwargs) -> \"fo.Dataset\":\n",
        "        \"\"\"\n",
        "        Creates a voxel51 dataset that can be used with `fo.launch_app()` to visualize it\n",
        "\n",
        "        Kwargs:\n",
        "            name (str): name of the dataset (by default uses the same name as the datasource)\n",
        "            force_download (bool): download the dataset even if the size of the files is bigger than 100MB\n",
        "            files_location (str|PathLike): path to the location where to download the local files\n",
        "                default: ~/dagshub_datasets/user/repo/ds_name/\n",
        "            redownload (bool): Redownload files, replacing the ones that might exist on the filesystem\n",
        "            voxel_annotations (List[str]) : List of columns from which to load voxel annotations serialized with\n",
        "                                        `to_json()`. This will override the labelstudio annotations\n",
        "        \"\"\"\n",
        "        return self.all().to_voxel51_dataset(**kwargs)\n",
        "\n",
        "    @property\n",
        "    def default_dataset_location(self) -> Path:\n",
        "        return Path(\n",
        "            sanitize_filepath(os.path.join(Path.home(), \"dagshub\", \"datasets\", self.source.repo, str(self.source.id))))\n",
        "\n",
        "    def visualize(self, **kwargs) -> \"fo.Session\":\n",
        "        return self.all().visualize(**kwargs)\n",
        "\n",
        "    @property\n",
        "    def fields(self) -> List[MetadataFieldSchema]:\n",
        "        return self.source.metadata_fields\n",
        "\n",
        "    def annotate(self) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Sends all datapoints in the datasource for annotation in Label Studio.\n",
        "        It's recommended to not send a huge amount of datapoints to be annotated at once, to avoid overloading\n",
        "        The Label Studio workspace.\n",
        "\n",
        "        :return: Link to open Label Studio in the browser\n",
        "        \"\"\"\n",
        "        return self.all().annotate()\n",
        "\n",
        "    def send_to_annotation(self):\n",
        "        \"\"\"\n",
        "        deprecated, see annotate()\n",
        "        \"\"\"\n",
        "        return self.annotate()\n",
        "\n",
        "    def send_datapoints_to_annotation(self, datapoints: Union[List[Datapoint], \"QueryResult\", List[Dict]],\n",
        "                                      open_project=True, ignore_warning=False) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Sends datapoints to annotations in Label Studio\n",
        "\n",
        "        :param datapoints: Either a list of Datapoints or dicts that have \"id\" and \"downloadurl\" fields.\n",
        "                     A QueryResult can also function as a list of Datapoint.\n",
        "        :param open_project: Specifies whether the link to the returned LS project should be opened from Python\n",
        "        :param ignore_warning: Suppress any non-lethal warnings that require user input\n",
        "        :return: Link to open Label Studio in the browser\n",
        "        \"\"\"\n",
        "        if len(datapoints) == 0:\n",
        "            logger.warning(\"No datapoints provided to be sent to annotation\")\n",
        "            return None\n",
        "        elif len(datapoints) > dagshub.common.config.recommended_annotate_limit and not ignore_warning:\n",
        "            force = prompt_user(f\"You are attempting to annotate {len(datapoints)} datapoints at once - it's \"\n",
        "                                f\"recommended to only annotate up to \"\n",
        "                                f\"{dagshub.common.config.recommended_annotate_limit} \"\n",
        "                                f\"datapoints at a time.\")\n",
        "            if not force:\n",
        "                return \"\"\n",
        "\n",
        "        req_data = {\n",
        "            \"datasource_id\": self.source.id,\n",
        "            \"datapoints\": []\n",
        "        }\n",
        "\n",
        "        for dp in datapoints:\n",
        "            req_dict = {}\n",
        "            if type(dp) is dict:\n",
        "                req_dict[\"id\"] = dp[\"datapoint_id\"]\n",
        "                req_dict[\"download_url\"] = dp[\"download_url\"]\n",
        "            else:\n",
        "                req_dict[\"id\"] = dp.datapoint_id\n",
        "                req_dict[\"download_url\"] = dp.download_url\n",
        "            req_data[\"datapoints\"].append(req_dict)\n",
        "\n",
        "        init_url = multi_urljoin(self.source.repoApi.data_engine_url, \"annotations/init\")\n",
        "        resp = http_request(\"POST\", init_url, json=req_data, auth=self.source.repoApi.auth)\n",
        "\n",
        "        if resp.status_code != 200:\n",
        "            logger.error(f\"Error while sending request for annotation: {resp.content}\")\n",
        "            return None\n",
        "        link = resp.json()[\"link\"]\n",
        "\n",
        "        # Do a raw print so it works in colab/jupyter\n",
        "        print(\"Open the following link to start working on your annotation project:\")\n",
        "        print(link)\n",
        "\n",
        "        if open_project:\n",
        "            webbrowser.open_new_tab(link)\n",
        "        return link\n",
        "\n",
        "    def _launch_annotation_workspace(self):\n",
        "        try:\n",
        "            start_workspace_url = multi_urljoin(self.source.repoApi.annotations_url, \"start\")\n",
        "            http_request(\"POST\", start_workspace_url, auth=self.source.repoApi.auth)\n",
        "        except:  # noqa\n",
        "            pass\n",
        "\n",
        "    def wait_until_ready(self, max_wait_time=300, fail_on_timeout=True):\n",
        "        \"\"\"\n",
        "       Blocks until the datasource preprocessing is complete\n",
        "\n",
        "       Args:\n",
        "           max_wait_time (int): Maximum time to wait in seconds\n",
        "           fail_on_timeout: Whether to raise a RuntimeError or continue if the scan does not complete on time\n",
        "       \"\"\"\n",
        "\n",
        "        # Start LS workspace to save time later in the flow\n",
        "        self._launch_annotation_workspace()\n",
        "\n",
        "        start = time.time()\n",
        "        if max_wait_time:\n",
        "            rich_console.log(f\"Maximum waiting time set to {int(max_wait_time / 60)} minutes\")\n",
        "        spinner = rich_console.status(\"Waiting for datasource preprocessing to complete...\")\n",
        "        with spinner:\n",
        "            while True:\n",
        "                self.source.get_from_dagshub()\n",
        "                if self.source.preprocessing_status == PreprocessingStatus.READY:\n",
        "                    return\n",
        "\n",
        "                if self.source.preprocessing_status == PreprocessingStatus.FAILED:\n",
        "                    raise RuntimeError(\"Datasource preprocessing failed\")\n",
        "\n",
        "                if max_wait_time is not None and (time.time() - start) > max_wait_time:\n",
        "                    if fail_on_timeout:\n",
        "                        raise RuntimeError(\n",
        "                            f\"Time limit of {max_wait_time} seconds reached before processing was completed.\")\n",
        "                    else:\n",
        "                        logger.warning(\n",
        "                            f\"Time limit of {max_wait_time} seconds reached before processing was completed.\")\n",
        "                        return\n",
        "\n",
        "                time.sleep(1)\n",
        "\n",
        "    def has_field(self, field_name: str):\n",
        "        reserved_searchable_fields = [\"path\"]\n",
        "        fields = (f.name for f in self.fields)\n",
        "        return field_name in reserved_searchable_fields or field_name in fields\n",
        "\n",
        "    def __repr__(self):\n",
        "        res = f\"Datasource {self.source.name}\"\n",
        "        res += f\"\\n\\tRepo: {self.source.repo}, path: {self.source.path}\"\n",
        "        res += f\"\\n\\t{self._query}\"\n",
        "        res += \"\\n\\tFields:\"\n",
        "        for f in self.fields:\n",
        "            res += f\"\\n\\t\\t{f}\"\n",
        "        return res + \"\\n\"\n",
        "\n",
        "    \"\"\" FUNCTIONS RELATED TO QUERYING\n",
        "    These are functions that overload operators on the DataSet, so you can do pandas-like filtering\n",
        "        ds = Dataset(...)\n",
        "        queried_ds = ds[ds[\"value\"] == 5]\n",
        "    \"\"\"\n",
        "\n",
        "    def __getitem__(self, other: Union[slice, str, \"Datasource\"]):\n",
        "        # Slicing - get items from the slice\n",
        "        if type(other) is slice:\n",
        "            return self.sample(other.start, other.stop)\n",
        "\n",
        "        # Otherwise we're doing querying\n",
        "        new_ds = self.__deepcopy__()\n",
        "        if type(other) is str:\n",
        "            if not self.has_field(other):\n",
        "                raise FieldNotFoundError(other)\n",
        "            new_ds._query = DatasourceQuery(other)\n",
        "            return new_ds\n",
        "        else:\n",
        "            # \"index\" is a datasource with a query - compose with \"and\"\n",
        "            # Example:\n",
        "            #   ds = Dataset()\n",
        "            #   filtered_ds = ds[ds[\"aaa\"] > 5]\n",
        "            #   filtered_ds2 = filtered_ds[filtered_ds[\"bbb\"] < 4]\n",
        "            if self._query.is_empty:\n",
        "                new_ds._query = other._query\n",
        "                return new_ds\n",
        "            else:\n",
        "                return other.__and__(self)\n",
        "\n",
        "    def __gt__(self, other: object):\n",
        "        self._test_not_comparing_other_ds(other)\n",
        "        if not isinstance(other, (int, float, str)):\n",
        "            raise NotImplementedError\n",
        "        return self.add_query_op(\"gt\", other)\n",
        "\n",
        "    def __ge__(self, other: object):\n",
        "        self._test_not_comparing_other_ds(other)\n",
        "        if not isinstance(other, (int, float, str)):\n",
        "            raise NotImplementedError\n",
        "        return self.add_query_op(\"ge\", other)\n",
        "\n",
        "    def __le__(self, other: object):\n",
        "        self._test_not_comparing_other_ds(other)\n",
        "        if not isinstance(other, (int, float, str)):\n",
        "            raise NotImplementedError\n",
        "        return self.add_query_op(\"le\", other)\n",
        "\n",
        "    def __lt__(self, other: object):\n",
        "        self._test_not_comparing_other_ds(other)\n",
        "        if not isinstance(other, (int, float, str)):\n",
        "            raise NotImplementedError\n",
        "        return self.add_query_op(\"lt\", other)\n",
        "\n",
        "    def __eq__(self, other: object):\n",
        "        self._test_not_comparing_other_ds(other)\n",
        "        if other is None:\n",
        "            return self.is_null()\n",
        "        if not isinstance(other, (int, float, str)):\n",
        "            raise NotImplementedError\n",
        "        return self.add_query_op(\"eq\", other)\n",
        "\n",
        "    def __ne__(self, other: object):\n",
        "        self._test_not_comparing_other_ds(other)\n",
        "        if other is None:\n",
        "            return self.is_not_null()\n",
        "        if not isinstance(other, (int, float, str)):\n",
        "            raise NotImplementedError\n",
        "        return self.add_query_op(\"eq\", other).add_query_op(\"not\")\n",
        "\n",
        "    def __invert__(self):\n",
        "        return self.add_query_op(\"not\")\n",
        "\n",
        "    def __contains__(self, item):\n",
        "        raise WrongOperatorError(\"Use `ds.contains(a)` for querying instead of `a in ds`\")\n",
        "\n",
        "    def contains(self, item: str):\n",
        "        if type(item) is not str:\n",
        "            return WrongOperatorError(f\"Cannot use contains with non-string value {item}\")\n",
        "        self._test_not_comparing_other_ds(item)\n",
        "        return self.add_query_op(\"contains\", item)\n",
        "\n",
        "    def is_null(self):\n",
        "        field = self._get_filtering_field()\n",
        "        value_type = _metadataTypeLookupReverse[field.valueType.value]\n",
        "        return self.add_query_op(\"isnull\", value_type())\n",
        "\n",
        "    def is_not_null(self):\n",
        "        return self.is_null().add_query_op(\"not\")\n",
        "\n",
        "    def _get_filtering_field(self) -> MetadataFieldSchema:\n",
        "        field_name = self.get_query().column_filter\n",
        "        if field_name is None:\n",
        "            raise RuntimeError(\"The current query filter is not a field\")\n",
        "        for col in self.source.metadata_fields:\n",
        "            if col.name == field_name:\n",
        "                return col\n",
        "        raise RuntimeError(f\"Field {field_name} doesn't exist in the current uploaded metadata\")\n",
        "\n",
        "    def __and__(self, other: \"Datasource\"):\n",
        "        return self.add_query_op(\"and\", other)\n",
        "\n",
        "    def __or__(self, other: \"Datasource\"):\n",
        "        return self.add_query_op(\"or\", other)\n",
        "\n",
        "    # Prevent users from messing up their queries due to operator order\n",
        "    # They always need to put the dataset query filters in parentheses, otherwise the binary and/or get executed before\n",
        "    def __rand__(self, other):\n",
        "        if type(other) is not Datasource:\n",
        "            raise WrongOrderError(type(other))\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __ror__(self, other):\n",
        "        if type(other) is not Datasource:\n",
        "            raise WrongOrderError(type(other))\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def add_query_op(self, op: str,\n",
        "                     other: Optional[Union[str, int, float, \"Datasource\", \"DatasourceQuery\"]] = None) -> \"Datasource\":\n",
        "        \"\"\"\n",
        "        Returns a new dataset with an added query param\n",
        "        \"\"\"\n",
        "        new_ds = self.__deepcopy__()\n",
        "        if type(other) is Datasource:\n",
        "            other = other.get_query()\n",
        "        new_ds._query.compose(op, other)\n",
        "        return new_ds\n",
        "\n",
        "    @staticmethod\n",
        "    def _test_not_comparing_other_ds(other):\n",
        "        if type(other) is Datasource:\n",
        "            raise DatasetFieldComparisonError()\n",
        "\n",
        "\n",
        "class MetadataContextManager:\n",
        "    def __init__(self, datasource: Datasource):\n",
        "        self._datasource = datasource\n",
        "        self._metadata_entries: List[DatapointMetadataUpdateEntry] = []\n",
        "        self._multivalue_fields = datasource._get_multivalue_fields()\n",
        "\n",
        "    def update_metadata(self, datapoints: Union[List[str], str], metadata: Dict[str, Any]):\n",
        "        if isinstance(datapoints, str):\n",
        "            datapoints = [datapoints]\n",
        "\n",
        "        field_value_types = {f.name: f.valueType for f in self._datasource.fields}\n",
        "\n",
        "        for dp in datapoints:\n",
        "            for k, v in metadata.items():\n",
        "                if v is None:\n",
        "                    continue\n",
        "                if k in autogenerated_columns:\n",
        "                    continue\n",
        "\n",
        "                if type(v) is list:\n",
        "                    if k not in self._multivalue_fields:\n",
        "                        self._multivalue_fields.add(k)\n",
        "                        # Promote all existing ones to multivalue\n",
        "                        for e in self._metadata_entries:\n",
        "                            if e.key == k:\n",
        "                                e.allowMultiple = True\n",
        "                    for sub_val in v:\n",
        "\n",
        "                        value_type = field_value_types.get(k)\n",
        "                        if value_type is None:\n",
        "                            value_type = _metadataTypeLookup[type(sub_val)]\n",
        "                            field_value_types[k] = value_type\n",
        "                        # Don't override bytes if they're not bytes - probably just undownloaded values\n",
        "                        if value_type == MetadataFieldType.BLOB and type(sub_val) is not bytes:\n",
        "                            continue\n",
        "\n",
        "                        if type(v) is bytes:\n",
        "                            sub_val = self.wrap_bytes(sub_val)\n",
        "                        self._metadata_entries.append(DatapointMetadataUpdateEntry(\n",
        "                            url=dp,\n",
        "                            key=k,\n",
        "                            value=str(sub_val),\n",
        "                            # todo: preliminary type check\n",
        "                            valueType=value_type,\n",
        "                            allowMultiple=k in self._multivalue_fields\n",
        "                        ))\n",
        "\n",
        "                else:\n",
        "\n",
        "                    value_type = field_value_types.get(k)\n",
        "                    if value_type is None:\n",
        "                        value_type = _metadataTypeLookup[type(v)]\n",
        "                        field_value_types[k] = value_type\n",
        "                    # Don't override bytes if they're not bytes - probably just undownloaded values\n",
        "                    if value_type == MetadataFieldType.BLOB and type(v) is not bytes:\n",
        "                        continue\n",
        "\n",
        "                    if type(v) is bytes:\n",
        "                        v = self.wrap_bytes(v)\n",
        "                    self._metadata_entries.append(DatapointMetadataUpdateEntry(\n",
        "                        url=dp,\n",
        "                        key=k,\n",
        "                        value=str(v),\n",
        "                        valueType=value_type,\n",
        "                        # todo: preliminary type check\n",
        "                        allowMultiple=k in self._multivalue_fields\n",
        "                    ))\n",
        "\n",
        "    @staticmethod\n",
        "    def wrap_bytes(val: bytes) -> str:\n",
        "        \"\"\"\n",
        "        Handles bytes values for uploading metadata\n",
        "        The process is gzip -> base64\n",
        "        \"\"\"\n",
        "        compressed = gzip.compress(val)\n",
        "        return base64.b64encode(compressed).decode(\"utf-8\")\n",
        "\n",
        "    def get_metadata_entries(self):\n",
        "        return self._metadata_entries\n"
      ],
      "metadata": {
        "id": "F03-svKzgUoE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Data Engine Capabilities\n",
        "\n",
        "Data Engine capabilities also includes: [data enrichments](https://dagshub.com/docs/use_cases/data_engine/enrich_datasource/), [visualizations](https://dagshub.com/docs/use_cases/data_engine/visualizing_datasets/), [annotations](https://dagshub.com/docs/use_cases/data_engine/annotate_data/), [creating datasets](https://dagshub.com/docs/use_cases/data_engine/query_and_create_subsets/) and [model training](https://dagshub.com/docs/use_cases/data_engine/train_model/)."
      ],
      "metadata": {
        "id": "j-dlmYBZHKUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install mlflow dagshub"
      ],
      "metadata": {
        "id": "xwFr38ij3ZIl",
        "outputId": "6f29ed0c-581f-4d93-ae9b-d39d31bda509",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-2.7.1-py3-none-any.whl (18.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dagshub\n",
            "  Downloading dagshub-0.3.8.post1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.1)\n",
            "Collecting databricks-cli<1,>=0.8.7 (from mlflow)\n",
            "  Downloading databricks-cli-0.17.8.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4)\n",
            "Collecting gitpython<4,>=2.1.0 (from mlflow)\n",
            "  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.0.1)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.20.3)\n",
            "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2023.3.post1)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.31.0)\n",
            "Requirement already satisfied: packaging<24 in /usr/local/lib/python3.10/dist-packages (from mlflow) (23.1)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.8.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4.4)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker<7,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.5)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.23.5)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.11.3)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.5.3)\n",
            "Collecting querystring-parser<2 (from mlflow)\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.21)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied: pyarrow<14,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (9.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.4.4)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
            "Collecting gunicorn<22 (from mlflow)\n",
            "  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.2)\n",
            "Collecting fusepy>=3 (from dagshub)\n",
            "  Downloading fusepy-3.0.1.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from dagshub) (1.4.4)\n",
            "Collecting httpx~=0.23.0 (from dagshub)\n",
            "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich[jupyter]~=13.1.0 (from dagshub)\n",
            "  Downloading rich-13.1.0-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dacite~=1.6.0 (from dagshub)\n",
            "  Downloading dacite-1.6.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tenacity~=8.2.2 in /usr/local/lib/python3.10/dist-packages (from dagshub) (8.2.3)\n",
            "Collecting gql[requests] (from dagshub)\n",
            "  Downloading gql-3.4.1-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from dagshub)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting treelib~=1.6.4 (from dagshub)\n",
            "  Downloading treelib-1.6.4-py3-none-any.whl (18 kB)\n",
            "Collecting pathvalidate~=3.0.0 (from dagshub)\n",
            "  Downloading pathvalidate-3.0.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dagshub) (2.8.2)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.5.0)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
            "Collecting urllib3<2.0.0,>=1.26.7 (from databricks-cli<1,>=0.8.7->mlflow)\n",
            "  Downloading urllib3-1.26.17-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<7,>=4.0.0->mlflow) (1.6.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow) (2.3.7)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow) (2.1.2)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=2.1.0->mlflow)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx~=0.23.0->dagshub) (2023.7.22)\n",
            "Collecting httpcore<0.17.0,>=0.15.0 (from httpx~=0.23.0->dagshub)\n",
            "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3 (from httpx~=0.23.0->dagshub)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx~=0.23.0->dagshub) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.4)\n",
            "Collecting commonmark<0.10.0,>=0.9.0 (from rich[jupyter]~=13.1.0->dagshub)\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich[jupyter]~=13.1.0->dagshub) (2.16.1)\n",
            "Requirement already satisfied: ipywidgets<8.0.0,>=7.5.1 in /usr/local/lib/python3.10/dist-packages (from rich[jupyter]~=13.1.0->dagshub) (7.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.2.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (2.0.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->dagshub)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json->dagshub)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting graphql-core<3.3,>=3.2 (from gql[requests]->dagshub)\n",
            "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.10/dist-packages (from gql[requests]->dagshub) (1.9.2)\n",
            "Collecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting requests-toolbelt<1,>=0.9.1 (from gql[requests]->dagshub)\n",
            "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore<0.17.0,>=0.15.0->httpx~=0.23.0->dagshub)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx~=0.23.0->dagshub) (3.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (3.6.6)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (3.0.9)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.0.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.17.0,>=0.15.0->httpx~=0.23.0->dagshub) (1.1.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (6.3.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (3.0.39)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (4.8.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.8.3)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (5.3.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.17.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (1.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.2.7)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (3.10.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (2.18.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (4.19.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.10.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]~=13.1.0->dagshub) (2.21)\n",
            "Building wheels for collected packages: databricks-cli, fusepy\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.17.8-py3-none-any.whl size=145466 sha256=e1bc29254bef662216988fa33b37701b4a8155e2de58f5bbd7ee5169333525db\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/c3/96/9efd9f76a128935c533c654a62f6d0f950d638b295553797f5\n",
            "  Building wheel for fusepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fusepy: filename=fusepy-3.0.1-py3-none-any.whl size=10486 sha256=d37fe09c2420d67dfda6e36f10faf8f68fc1c3df32f0074ed396127d9397b9e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/18/f6/f0d6be9d0435e2677ce5cc758e91da50053dce456a346f08c5\n",
            "Successfully built databricks-cli fusepy\n",
            "Installing collected packages: rfc3986, fusepy, commonmark, urllib3, treelib, smmap, rich, querystring-parser, pathvalidate, mypy-extensions, marshmallow, Mako, jedi, h11, gunicorn, graphql-core, dacite, backoff, typing-inspect, httpcore, gql, gitdb, alembic, requests-toolbelt, httpx, gitpython, docker, dataclasses-json, databricks-cli, mlflow, dagshub\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.5\n",
            "    Uninstalling urllib3-2.0.5:\n",
            "      Successfully uninstalled urllib3-2.0.5\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.5.3\n",
            "    Uninstalling rich-13.5.3:\n",
            "      Successfully uninstalled rich-13.5.3\n",
            "Successfully installed Mako-1.2.4 alembic-1.12.0 backoff-2.2.1 commonmark-0.9.1 dacite-1.6.0 dagshub-0.3.8.post1 databricks-cli-0.17.8 dataclasses-json-0.6.1 docker-6.1.3 fusepy-3.0.1 gitdb-4.0.10 gitpython-3.1.37 gql-3.4.1 graphql-core-3.2.3 gunicorn-21.2.0 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 jedi-0.19.1 marshmallow-3.20.1 mlflow-2.7.1 mypy-extensions-1.0.0 pathvalidate-3.0.0 querystring-parser-1.2.4 requests-toolbelt-0.10.1 rfc3986-1.5.0 rich-13.1.0 smmap-5.0.1 treelib-1.6.4 typing-inspect-0.9.0 urllib3-1.26.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Internal module implementing the fluent API, allowing management of an active\n",
        "MLflow run. This module is exposed to users at the top-level :py:mod:`mlflow` module.\n",
        "\"\"\"\n",
        "import atexit\n",
        "import contextlib\n",
        "import inspect\n",
        "import logging\n",
        "import os\n",
        "from copy import deepcopy\n",
        "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n",
        "\n",
        "from mlflow.data.dataset import Dataset\n",
        "from mlflow.entities import (\n",
        "    DatasetInput,\n",
        "    Experiment,\n",
        "    InputTag,\n",
        "    Metric,\n",
        "    Param,\n",
        "    Run,\n",
        "    RunStatus,\n",
        "    RunTag,\n",
        "    ViewType,\n",
        ")\n",
        "from mlflow.entities.lifecycle_stage import LifecycleStage\n",
        "from mlflow.environment_variables import (\n",
        "    MLFLOW_EXPERIMENT_ID,\n",
        "    MLFLOW_EXPERIMENT_NAME,\n",
        "    MLFLOW_RUN_ID,\n",
        ")\n",
        "from mlflow.exceptions import MlflowException\n",
        "from mlflow.protos.databricks_pb2 import (\n",
        "    INVALID_PARAMETER_VALUE,\n",
        "    RESOURCE_DOES_NOT_EXIST,\n",
        ")\n",
        "from mlflow.store.tracking import SEARCH_MAX_RESULTS_DEFAULT\n",
        "from mlflow.tracking import _get_store, artifact_utils\n",
        "from mlflow.tracking.client import MlflowClient\n",
        "from mlflow.tracking.context import registry as context_registry\n",
        "from mlflow.tracking.default_experiment import registry as default_experiment_registry\n",
        "from mlflow.utils import get_results_from_paginated_fn\n",
        "from mlflow.utils.annotations import experimental\n",
        "from mlflow.utils.autologging_utils import (\n",
        "    AUTOLOGGING_CONF_KEY_IS_GLOBALLY_CONFIGURED,\n",
        "    AUTOLOGGING_INTEGRATIONS,\n",
        "    autologging_integration,\n",
        "    autologging_is_disabled,\n",
        "    is_testing,\n",
        ")\n",
        "from mlflow.utils.databricks_utils import is_in_databricks_runtime\n",
        "from mlflow.utils.import_hooks import register_post_import_hook\n",
        "from mlflow.utils.mlflow_tags import (\n",
        "    MLFLOW_DATASET_CONTEXT,\n",
        "    MLFLOW_EXPERIMENT_PRIMARY_METRIC_GREATER_IS_BETTER,\n",
        "    MLFLOW_EXPERIMENT_PRIMARY_METRIC_NAME,\n",
        "    MLFLOW_PARENT_RUN_ID,\n",
        "    MLFLOW_RUN_NAME,\n",
        "    MLFLOW_RUN_NOTE,\n",
        ")\n",
        "from mlflow.utils.time import get_current_time_millis\n",
        "from mlflow.utils.validation import _validate_experiment_id_type, _validate_run_id\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    import matplotlib\n",
        "    import matplotlib.figure\n",
        "    import numpy\n",
        "    import pandas\n",
        "    import PIL\n",
        "    import plotly\n",
        "\n",
        "_active_run_stack = []\n",
        "_active_experiment_id = None\n",
        "_last_active_run_id = None\n",
        "\n",
        "SEARCH_MAX_RESULTS_PANDAS = 100000\n",
        "NUM_RUNS_PER_PAGE_PANDAS = 10000\n",
        "\n",
        "_logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def set_experiment(experiment_name: str = None, experiment_id: str = None) -> Experiment:\n",
        "    \"\"\"\n",
        "    Set the given experiment as the active experiment. The experiment must either be specified by\n",
        "    name via `experiment_name` or by ID via `experiment_id`. The experiment name and ID cannot\n",
        "    both be specified.\n",
        "\n",
        "    :param experiment_name: Case sensitive name of the experiment to be activated. If an experiment\n",
        "                            with this name does not exist, a new experiment wth this name is\n",
        "                            created. On certain platforms such as Databricks, the experiment name\n",
        "                            must an absolute path, e.g. ``\"/Users/<username>/my-experiment\"``.\n",
        "    :param experiment_id: ID of the experiment to be activated. If an experiment with this ID\n",
        "                          does not exist, an exception is thrown.\n",
        "    :return: An instance of :py:class:`mlflow.entities.Experiment` representing the new active\n",
        "             experiment.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        # Set an experiment name, which must be unique and case-sensitive.\n",
        "        experiment = mlflow.set_experiment(\"Social NLP Experiments\")\n",
        "\n",
        "        # Get Experiment Details\n",
        "        print(f\"Experiment_id: {experiment.experiment_id}\")\n",
        "        print(f\"Artifact Location: {experiment.artifact_location}\")\n",
        "        print(f\"Tags: {experiment.tags}\")\n",
        "        print(f\"Lifecycle_stage: {experiment.lifecycle_stage}\")\n",
        "\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "        Experiment_id: 1\n",
        "        Artifact Location: file:///.../mlruns/1\n",
        "        Tags: {}\n",
        "        Lifecycle_stage: active\n",
        "    \"\"\"\n",
        "    if (experiment_name is not None and experiment_id is not None) or (\n",
        "        experiment_name is None and experiment_id is None\n",
        "    ):\n",
        "        raise MlflowException(\n",
        "            message=\"Must specify exactly one of: `experiment_id` or `experiment_name`.\",\n",
        "            error_code=INVALID_PARAMETER_VALUE,\n",
        "        )\n",
        "\n",
        "    client = MlflowClient()\n",
        "    if experiment_id is None:\n",
        "        experiment = client.get_experiment_by_name(experiment_name)\n",
        "        if not experiment:\n",
        "            _logger.info(\n",
        "                \"Experiment with name '%s' does not exist. Creating a new experiment.\",\n",
        "                experiment_name,\n",
        "            )\n",
        "            # NB: If two simultaneous threads or processes attempt to set the same experiment\n",
        "            # simultaneously, a race condition may be encountered here wherein experiment creation\n",
        "            # fails\n",
        "            experiment_id = client.create_experiment(experiment_name)\n",
        "            experiment = client.get_experiment(experiment_id)\n",
        "    else:\n",
        "        experiment = client.get_experiment(experiment_id)\n",
        "        if experiment is None:\n",
        "            raise MlflowException(\n",
        "                message=f\"Experiment with ID '{experiment_id}' does not exist.\",\n",
        "                error_code=RESOURCE_DOES_NOT_EXIST,\n",
        "            )\n",
        "\n",
        "    if experiment.lifecycle_stage != LifecycleStage.ACTIVE:\n",
        "        raise MlflowException(\n",
        "            message=(\n",
        "                \"Cannot set a deleted experiment '%s' as the active experiment. \"\n",
        "                \"You can restore the experiment, or permanently delete the \"\n",
        "                \"experiment to create a new one.\" % experiment.name\n",
        "            ),\n",
        "            error_code=INVALID_PARAMETER_VALUE,\n",
        "        )\n",
        "\n",
        "    global _active_experiment_id\n",
        "    _active_experiment_id = experiment.experiment_id\n",
        "    return experiment\n",
        "\n",
        "\n",
        "def _set_experiment_primary_metric(\n",
        "    experiment_id: str, primary_metric: str, greater_is_better: bool\n",
        "):\n",
        "    client = MlflowClient()\n",
        "    client.set_experiment_tag(experiment_id, MLFLOW_EXPERIMENT_PRIMARY_METRIC_NAME, primary_metric)\n",
        "    client.set_experiment_tag(\n",
        "        experiment_id, MLFLOW_EXPERIMENT_PRIMARY_METRIC_GREATER_IS_BETTER, str(greater_is_better)\n",
        "    )\n",
        "\n",
        "\n",
        "class ActiveRun(Run):  # pylint: disable=abstract-method\n",
        "    \"\"\"Wrapper around :py:class:`mlflow.entities.Run` to enable using Python ``with`` syntax.\"\"\"\n",
        "\n",
        "    def __init__(self, run):\n",
        "        Run.__init__(self, run.info, run.data)\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        status = RunStatus.FINISHED if exc_type is None else RunStatus.FAILED\n",
        "        end_run(RunStatus.to_string(status))\n",
        "        return exc_type is None\n",
        "\n",
        "\n",
        "def start_run(\n",
        "    run_id: str = None,\n",
        "    experiment_id: Optional[str] = None,\n",
        "    run_name: Optional[str] = None,\n",
        "    nested: bool = False,\n",
        "    tags: Optional[Dict[str, Any]] = None,\n",
        "    description: Optional[str] = None,\n",
        ") -> ActiveRun:\n",
        "    \"\"\"\n",
        "    Start a new MLflow run, setting it as the active run under which metrics and parameters\n",
        "    will be logged. The return value can be used as a context manager within a ``with`` block;\n",
        "    otherwise, you must call ``end_run()`` to terminate the current run.\n",
        "\n",
        "    If you pass a ``run_id`` or the ``MLFLOW_RUN_ID`` environment variable is set,\n",
        "    ``start_run`` attempts to resume a run with the specified run ID and\n",
        "    other parameters are ignored. ``run_id`` takes precedence over ``MLFLOW_RUN_ID``.\n",
        "\n",
        "    If resuming an existing run, the run status is set to ``RunStatus.RUNNING``.\n",
        "\n",
        "    MLflow sets a variety of default tags on the run, as defined in\n",
        "    :ref:`MLflow system tags <system_tags>`.\n",
        "\n",
        "    :param run_id: If specified, get the run with the specified UUID and log parameters\n",
        "                     and metrics under that run. The run's end time is unset and its status\n",
        "                     is set to running, but the run's other attributes (``source_version``,\n",
        "                     ``source_type``, etc.) are not changed.\n",
        "    :param experiment_id: ID of the experiment under which to create the current run (applicable\n",
        "                          only when ``run_id`` is not specified). If ``experiment_id`` argument\n",
        "                          is unspecified, will look for valid experiment in the following order:\n",
        "                          activated using ``set_experiment``, ``MLFLOW_EXPERIMENT_NAME``\n",
        "                          environment variable, ``MLFLOW_EXPERIMENT_ID`` environment variable,\n",
        "                          or the default experiment as defined by the tracking server.\n",
        "    :param run_name: Name of new run.\n",
        "                     Used only when ``run_id`` is unspecified. If a new run is created and\n",
        "                     ``run_name`` is not specified, a unique name will be generated for the run.\n",
        "    :param nested: Controls whether run is nested in parent run. ``True`` creates a nested run.\n",
        "    :param tags: An optional dictionary of string keys and values to set as tags on the run.\n",
        "                 If a run is being resumed, these tags are set on the resumed run. If a new run is\n",
        "                 being created, these tags are set on the new run.\n",
        "    :param description: An optional string that populates the description box of the run.\n",
        "                        If a run is being resumed, the description is set on the resumed run.\n",
        "                        If a new run is being created, the description is set on the new run.\n",
        "    :return: :py:class:`mlflow.ActiveRun` object that acts as a context manager wrapping\n",
        "             the run's state.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        # Create nested runs\n",
        "        experiment_id = mlflow.create_experiment(\"experiment1\")\n",
        "        with mlflow.start_run(\n",
        "            run_name=\"PARENT_RUN\",\n",
        "            experiment_id=experiment_id,\n",
        "            tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
        "            description=\"parent\",\n",
        "        ) as parent_run:\n",
        "            mlflow.log_param(\"parent\", \"yes\")\n",
        "            with mlflow.start_run(\n",
        "                run_name=\"CHILD_RUN\",\n",
        "                experiment_id=experiment_id,\n",
        "                description=\"child\",\n",
        "                nested=True,\n",
        "            ) as child_run:\n",
        "                mlflow.log_param(\"child\", \"yes\")\n",
        "\n",
        "        print(\"parent run:\")\n",
        "\n",
        "        print(f\"run_id: {parent_run.info.run_id}\")\n",
        "        print(\"description: {}\".format(parent_run.data.tags.get(\"mlflow.note.content\")))\n",
        "        print(\"version tag value: {}\".format(parent_run.data.tags.get(\"version\")))\n",
        "        print(\"priority tag value: {}\".format(parent_run.data.tags.get(\"priority\")))\n",
        "        print(\"--\")\n",
        "\n",
        "        # Search all child runs with a parent id\n",
        "        query = f\"tags.mlflow.parentRunId = '{parent_run.info.run_id}'\"\n",
        "        results = mlflow.search_runs(experiment_ids=[experiment_id], filter_string=query)\n",
        "        print(\"child runs:\")\n",
        "        print(results[[\"run_id\", \"params.child\", \"tags.mlflow.runName\"]])\n",
        "\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "        parent run:\n",
        "        run_id: 8979459433a24a52ab3be87a229a9cdf\n",
        "        description: starting a parent for experiment 7\n",
        "        version tag value: v1\n",
        "        priority tag value: P1\n",
        "        --\n",
        "        child runs:\n",
        "                                     run_id params.child tags.mlflow.runName\n",
        "        0  7d175204675e40328e46d9a6a5a7ee6a          yes           CHILD_RUN\n",
        "    \"\"\"\n",
        "    global _active_run_stack\n",
        "    _validate_experiment_id_type(experiment_id)\n",
        "    # back compat for int experiment_id\n",
        "    experiment_id = str(experiment_id) if isinstance(experiment_id, int) else experiment_id\n",
        "    if len(_active_run_stack) > 0 and not nested:\n",
        "        raise Exception(\n",
        "            (\n",
        "                \"Run with UUID {} is already active. To start a new run, first end the \"\n",
        "                + \"current run with mlflow.end_run(). To start a nested \"\n",
        "                + \"run, call start_run with nested=True\"\n",
        "            ).format(_active_run_stack[0].info.run_id)\n",
        "        )\n",
        "    client = MlflowClient()\n",
        "    if run_id:\n",
        "        existing_run_id = run_id\n",
        "    elif run_id := MLFLOW_RUN_ID.get():\n",
        "        existing_run_id = run_id\n",
        "        del os.environ[MLFLOW_RUN_ID.name]\n",
        "    else:\n",
        "        existing_run_id = None\n",
        "    if existing_run_id:\n",
        "        _validate_run_id(existing_run_id)\n",
        "        active_run_obj = client.get_run(existing_run_id)\n",
        "        # Check to see if experiment_id from environment matches experiment_id from set_experiment()\n",
        "        if (\n",
        "            _active_experiment_id is not None\n",
        "            and _active_experiment_id != active_run_obj.info.experiment_id\n",
        "        ):\n",
        "            raise MlflowException(\n",
        "                f\"Cannot start run with ID {existing_run_id} because active run ID \"\n",
        "                \"does not match environment run ID. Make sure --experiment-name \"\n",
        "                \"or --experiment-id matches experiment set with \"\n",
        "                \"set_experiment(), or just use command-line arguments\"\n",
        "            )\n",
        "        # Check to see if current run isn't deleted\n",
        "        if active_run_obj.info.lifecycle_stage == LifecycleStage.DELETED:\n",
        "            raise MlflowException(\n",
        "                f\"Cannot start run with ID {existing_run_id} because it is in the deleted state.\"\n",
        "            )\n",
        "        # Use previous end_time because a value is required for update_run_info\n",
        "        end_time = active_run_obj.info.end_time\n",
        "        _get_store().update_run_info(\n",
        "            existing_run_id, run_status=RunStatus.RUNNING, end_time=end_time, run_name=None\n",
        "        )\n",
        "        tags = tags or {}\n",
        "        if description:\n",
        "            if MLFLOW_RUN_NOTE in tags:\n",
        "                raise MlflowException(\n",
        "                    f\"Description is already set via the tag {MLFLOW_RUN_NOTE} in tags.\"\n",
        "                    f\"Remove the key {MLFLOW_RUN_NOTE} from the tags or omit the description.\",\n",
        "                    error_code=INVALID_PARAMETER_VALUE,\n",
        "                )\n",
        "            tags[MLFLOW_RUN_NOTE] = description\n",
        "\n",
        "        if tags:\n",
        "            client.log_batch(\n",
        "                run_id=existing_run_id,\n",
        "                tags=[RunTag(key, str(value)) for key, value in tags.items()],\n",
        "            )\n",
        "        active_run_obj = client.get_run(existing_run_id)\n",
        "    else:\n",
        "        if len(_active_run_stack) > 0:\n",
        "            parent_run_id = _active_run_stack[-1].info.run_id\n",
        "        else:\n",
        "            parent_run_id = None\n",
        "\n",
        "        exp_id_for_run = experiment_id if experiment_id is not None else _get_experiment_id()\n",
        "\n",
        "        user_specified_tags = deepcopy(tags) or {}\n",
        "        if description:\n",
        "            if MLFLOW_RUN_NOTE in user_specified_tags:\n",
        "                raise MlflowException(\n",
        "                    f\"Description is already set via the tag {MLFLOW_RUN_NOTE} in tags.\"\n",
        "                    f\"Remove the key {MLFLOW_RUN_NOTE} from the tags or omit the description.\",\n",
        "                    error_code=INVALID_PARAMETER_VALUE,\n",
        "                )\n",
        "            user_specified_tags[MLFLOW_RUN_NOTE] = description\n",
        "        if parent_run_id is not None:\n",
        "            user_specified_tags[MLFLOW_PARENT_RUN_ID] = parent_run_id\n",
        "        if run_name:\n",
        "            user_specified_tags[MLFLOW_RUN_NAME] = run_name\n",
        "\n",
        "        resolved_tags = context_registry.resolve_tags(user_specified_tags)\n",
        "\n",
        "        active_run_obj = client.create_run(\n",
        "            experiment_id=exp_id_for_run, tags=resolved_tags, run_name=run_name\n",
        "        )\n",
        "\n",
        "    _active_run_stack.append(ActiveRun(active_run_obj))\n",
        "    return _active_run_stack[-1]\n",
        "\n",
        "\n",
        "def end_run(status: str = RunStatus.to_string(RunStatus.FINISHED)) -> None:\n",
        "    \"\"\"End an active MLflow run (if there is one).\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        # Start run and get status\n",
        "        mlflow.start_run()\n",
        "        run = mlflow.active_run()\n",
        "        print(f\"run_id: {run.info.run_id}; status: {run.info.status}\")\n",
        "\n",
        "        # End run and get status\n",
        "        mlflow.end_run()\n",
        "        run = mlflow.get_run(run.info.run_id)\n",
        "        print(f\"run_id: {run.info.run_id}; status: {run.info.status}\")\n",
        "        print(\"--\")\n",
        "\n",
        "        # Check for any active runs\n",
        "        print(f\"Active run: {mlflow.active_run()}\")\n",
        "\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "        run_id: b47ee4563368419880b44ad8535f6371; status: RUNNING\n",
        "        run_id: b47ee4563368419880b44ad8535f6371; status: FINISHED\n",
        "        --\n",
        "        Active run: None\n",
        "    \"\"\"\n",
        "    global _active_run_stack, _last_active_run_id\n",
        "    if len(_active_run_stack) > 0:\n",
        "        # Clear out the global existing run environment variable as well.\n",
        "        MLFLOW_RUN_ID.unset()\n",
        "        run = _active_run_stack.pop()\n",
        "        MlflowClient().set_terminated(run.info.run_id, status)\n",
        "        _last_active_run_id = run.info.run_id\n",
        "\n",
        "\n",
        "def _safe_end_run():\n",
        "    with contextlib.suppress(Exception):\n",
        "        end_run()\n",
        "\n",
        "\n",
        "atexit.register(_safe_end_run)\n",
        "\n",
        "\n",
        "def active_run() -> Optional[ActiveRun]:\n",
        "    \"\"\"Get the currently active ``Run``, or None if no such run exists.\n",
        "\n",
        "    **Note**: You cannot access currently-active run attributes\n",
        "    (parameters, metrics, etc.) through the run returned by ``mlflow.active_run``. In order\n",
        "    to access such attributes, use the :py:class:`mlflow.client.MlflowClient` as follows:\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        mlflow.start_run()\n",
        "        run = mlflow.active_run()\n",
        "        print(f\"Active run_id: {run.info.run_id}\")\n",
        "        mlflow.end_run()\n",
        "\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "        Active run_id: 6f252757005748708cd3aad75d1ff462\n",
        "    \"\"\"\n",
        "    return _active_run_stack[-1] if len(_active_run_stack) > 0 else None\n",
        "\n",
        "\n",
        "def last_active_run() -> Optional[Run]:\n",
        "    \"\"\"\n",
        "    Gets the most recent active run.\n",
        "\n",
        "    Examples:\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: To retrieve the most recent autologged run:\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        from sklearn.datasets import load_diabetes\n",
        "        from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "        mlflow.autolog()\n",
        "\n",
        "        db = load_diabetes()\n",
        "        X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n",
        "\n",
        "        # Create and train models.\n",
        "        rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n",
        "        rf.fit(X_train, y_train)\n",
        "\n",
        "        # Use the model to make predictions on the test dataset.\n",
        "        predictions = rf.predict(X_test)\n",
        "        autolog_run = mlflow.last_active_run()\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: To get the most recently active run that ended:\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        mlflow.start_run()\n",
        "        mlflow.end_run()\n",
        "        run = mlflow.last_active_run()\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: To retrieve the currently active run:\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        mlflow.start_run()\n",
        "        run = mlflow.last_active_run()\n",
        "        mlflow.end_run()\n",
        "\n",
        "    :return: The active run (this is equivalent to ``mlflow.active_run()``) if one exists.\n",
        "             Otherwise, the last run started from the current Python process that reached\n",
        "             a terminal status (i.e. FINISHED, FAILED, or KILLED).\n",
        "    \"\"\"\n",
        "    _active_run = active_run()\n",
        "    if _active_run is not None:\n",
        "        return _active_run\n",
        "    if _last_active_run_id is None:\n",
        "        return None\n",
        "    return get_run(_last_active_run_id)\n",
        "\n",
        "\n",
        "def get_run(run_id: str) -> Run:\n",
        "    \"\"\"\n",
        "    Fetch the run from backend store. The resulting :py:class:`Run <mlflow.entities.Run>`\n",
        "    contains a collection of run metadata -- :py:class:`RunInfo <mlflow.entities.RunInfo>`,\n",
        "    as well as a collection of run parameters, tags, and metrics --\n",
        "    :py:class:`RunData <mlflow.entities.RunData>`. It also contains a collection of run\n",
        "    inputs (experimental), including information about datasets used by the run --\n",
        "    :py:class:`RunInputs <mlflow.entities.RunInputs>`. In the case where multiple metrics with the\n",
        "    same key are logged for the run, the :py:class:`RunData <mlflow.entities.RunData>` contains the\n",
        "    most recently logged value at the largest step for each metric.\n",
        "\n",
        "    :param run_id: Unique identifier for the run.\n",
        "\n",
        "    :return: A single :py:class:`mlflow.entities.Run` object, if the run exists. Otherwise,\n",
        "                raises an exception.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        with mlflow.start_run() as run:\n",
        "            mlflow.log_param(\"p\", 0)\n",
        "\n",
        "        run_id = run.info.run_id\n",
        "        print(\n",
        "            f\"run_id: {run_id}; lifecycle_stage: {mlflow.get_run(run_id).info.lifecycle_stage}\"\n",
        "        )\n",
        "\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "        run_id: 7472befefc754e388e8e922824a0cca5; lifecycle_stage: active\n",
        "    \"\"\"\n",
        "    return MlflowClient().get_run(run_id)\n",
        "\n",
        "\n",
        "def get_parent_run(run_id: str) -> Optional[Run]:\n",
        "    \"\"\"\n",
        "    Gets the parent run for the given run id if one exists.\n",
        "\n",
        "    :param run_id: Unique identifier for the child run.\n",
        "\n",
        "    :return: A single :py:class:`mlflow.entities.Run` object, if the parent run exists. Otherwise,\n",
        "                returns None.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        # Create nested runs\n",
        "        with mlflow.start_run():\n",
        "            with mlflow.start_run(nested=True) as child_run:\n",
        "                child_run_id = child_run.info.run_id\n",
        "\n",
        "        parent_run = mlflow.get_parent_run(child_run_id)\n",
        "\n",
        "        print(f\"child_run_id: {child_run_id}\")\n",
        "        print(f\"parent_run_id: {parent_run.info.run_id}\")\n",
        "\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "        child_run_id: 7d175204675e40328e46d9a6a5a7ee6a\n",
        "        parent_run_id: 8979459433a24a52ab3be87a229a9cdf\n",
        "    \"\"\"\n",
        "    return MlflowClient().get_parent_run(run_id)\n",
        "\n",
        "\n",
        "def log_param(key: str, value: Any) -> Any:\n",
        "    \"\"\"\n",
        "    Log a parameter (e.g. model hyperparameter) under the current run. If no run is active,\n",
        "    this method will create a new active run.\n",
        "\n",
        "    :param key: Parameter name (string). This string may only contain alphanumerics,\n",
        "                underscores (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n",
        "                All backend stores support keys up to length 250, but some may\n",
        "                support larger keys.\n",
        "    :param value: Parameter value (string, but will be string-ified if not).\n",
        "                  All backend stores support values up to length 500, but some\n",
        "                  may support larger values.\n",
        "\n",
        "    :return: the parameter value that is logged.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        with mlflow.start_run():\n",
        "            value = mlflow.log_param(\"learning_rate\", 0.01)\n",
        "            assert value == 0.01\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    return MlflowClient().log_param(run_id, key, value)\n",
        "\n",
        "\n",
        "def set_experiment_tag(key: str, value: Any) -> None:\n",
        "    \"\"\"\n",
        "    Set a tag on the current experiment. Value is converted to a string.\n",
        "\n",
        "    :param key: Tag name (string). This string may only contain alphanumerics, underscores\n",
        "                (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n",
        "                All backend stores will support keys up to length 250, but some may\n",
        "                support larger keys.\n",
        "    :param value: Tag value (string, but will be string-ified if not).\n",
        "                  All backend stores will support values up to length 5000, but some\n",
        "                  may support larger values.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        with mlflow.start_run():\n",
        "            mlflow.set_experiment_tag(\"release.version\", \"2.2.0\")\n",
        "    \"\"\"\n",
        "    experiment_id = _get_experiment_id()\n",
        "    MlflowClient().set_experiment_tag(experiment_id, key, value)\n",
        "\n",
        "\n",
        "def set_tag(key: str, value: Any) -> None:\n",
        "    \"\"\"\n",
        "    Set a tag under the current run. If no run is active, this method will create a\n",
        "    new active run.\n",
        "\n",
        "    :param key: Tag name (string). This string may only contain alphanumerics, underscores\n",
        "                (_), dashes (-), periods (.), spaces ( ), and slashes (/).\n",
        "                All backend stores will support keys up to length 250, but some may\n",
        "                support larger keys.\n",
        "    :param value: Tag value (string, but will be string-ified if not).\n",
        "                  All backend stores will support values up to length 5000, but some\n",
        "                  may support larger values.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        with mlflow.start_run():\n",
        "            mlflow.set_tag(\"release.version\", \"2.2.0\")\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    MlflowClient().set_tag(run_id, key, value)\n",
        "\n",
        "\n",
        "def delete_tag(key: str) -> None:\n",
        "    \"\"\"\n",
        "    Delete a tag from a run. This is irreversible. If no run is active, this method\n",
        "    will create a new active run.\n",
        "\n",
        "    :param key: Name of the tag\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        tags = {\"engineering\": \"ML Platform\", \"engineering_remote\": \"ML Platform\"}\n",
        "\n",
        "        with mlflow.start_run() as run:\n",
        "            mlflow.set_tags(tags)\n",
        "\n",
        "        with mlflow.start_run(run_id=run.info.run_id):\n",
        "            mlflow.delete_tag(\"engineering_remote\")\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    MlflowClient().delete_tag(run_id, key)\n",
        "\n",
        "\n",
        "def log_metric(key: str, value: float, step: Optional[int] = None) -> None:\n",
        "    \"\"\"\n",
        "    Log a metric under the current run. If no run is active, this method will create\n",
        "    a new active run.\n",
        "\n",
        "    :param key: Metric name (string). This string may only contain alphanumerics, underscores (_),\n",
        "                dashes (-), periods (.), spaces ( ), and slashes (/).\n",
        "                All backend stores will support keys up to length 250, but some may\n",
        "                support larger keys.\n",
        "    :param value: Metric value (float). Note that some special values such as +/- Infinity may be\n",
        "                  replaced by other values depending on the store. For example, the\n",
        "                  SQLAlchemy store replaces +/- Infinity with max / min float values.\n",
        "                  All backend stores will support values up to length 5000, but some\n",
        "                  may support larger values.\n",
        "    :param step: Metric step (int). Defaults to zero if unspecified.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        with mlflow.start_run():\n",
        "            mlflow.log_metric(\"mse\", 2500.00)\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    MlflowClient().log_metric(run_id, key, value, get_current_time_millis(), step or 0)\n",
        "\n",
        "\n",
        "def log_metrics(metrics: Dict[str, float], step: Optional[int] = None) -> None:\n",
        "    \"\"\"\n",
        "    Log multiple metrics for the current run. If no run is active, this method will create a new\n",
        "    active run.\n",
        "\n",
        "    :param metrics: Dictionary of metric_name: String -> value: Float. Note that some special\n",
        "                    values such as +/- Infinity may be replaced by other values depending on\n",
        "                    the store. For example, sql based store may replace +/- Infinity with\n",
        "                    max / min float values.\n",
        "    :param step: A single integer step at which to log the specified\n",
        "                 Metrics. If unspecified, each metric is logged at step zero.\n",
        "\n",
        "    :returns: None\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        metrics = {\"mse\": 2500.00, \"rmse\": 50.00}\n",
        "\n",
        "        # Log a batch of metrics\n",
        "        with mlflow.start_run():\n",
        "            mlflow.log_metrics(metrics)\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    timestamp = get_current_time_millis()\n",
        "    metrics_arr = [Metric(key, value, timestamp, step or 0) for key, value in metrics.items()]\n",
        "    MlflowClient().log_batch(run_id=run_id, metrics=metrics_arr, params=[], tags=[])\n",
        "\n",
        "\n",
        "def log_params(params: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Log a batch of params for the current run. If no run is active, this method will create a\n",
        "    new active run.\n",
        "\n",
        "    :param params: Dictionary of param_name: String -> value: (String, but will be string-ified if\n",
        "                   not)\n",
        "    :returns: None\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        params = {\"learning_rate\": 0.01, \"n_estimators\": 10}\n",
        "\n",
        "        # Log a batch of parameters\n",
        "        with mlflow.start_run():\n",
        "            mlflow.log_params(params)\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    params_arr = [Param(key, str(value)) for key, value in params.items()]\n",
        "    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\n",
        "\n",
        "\n",
        "@experimental\n",
        "def log_input(\n",
        "    dataset: Dataset, context: Optional[str] = None, tags: Optional[Dict[str, str]] = None\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Log a dataset used in the current run.\n",
        "\n",
        "    :param dataset: :py:class:`mlflow.data.dataset.Dataset` object to be logged.\n",
        "    :param context: Context in which the dataset is used. For example: \"training\", \"testing\".\n",
        "                    This will be set as an input tag with key `mlflow.data.context`.\n",
        "    :param tags: Tags to be associated with the dataset. Dictionary of tag_key -> tag_value.\n",
        "    :returns: None\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import numpy as np\n",
        "        import mlflow\n",
        "\n",
        "        array = np.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "        dataset = mlflow.data.from_numpy(array, source=\"data.csv\")\n",
        "\n",
        "        # Log an input dataset used for training\n",
        "        with mlflow.start_run():\n",
        "            mlflow.log_input(dataset, context=\"training\")\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    tags_to_log = []\n",
        "    if tags:\n",
        "        tags_to_log.extend([InputTag(key=key, value=value) for key, value in tags.items()])\n",
        "    if context:\n",
        "        tags_to_log.append(InputTag(key=MLFLOW_DATASET_CONTEXT, value=context))\n",
        "\n",
        "    dataset_input = DatasetInput(dataset=dataset._to_mlflow_entity(), tags=tags_to_log)\n",
        "\n",
        "    MlflowClient().log_inputs(run_id=run_id, datasets=[dataset_input])\n",
        "\n",
        "\n",
        "def set_experiment_tags(tags: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Set tags for the current active experiment.\n",
        "\n",
        "    :param tags: Dictionary containing tag names and corresponding values.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        tags = {\n",
        "            \"engineering\": \"ML Platform\",\n",
        "            \"release.candidate\": \"RC1\",\n",
        "            \"release.version\": \"2.2.0\",\n",
        "        }\n",
        "\n",
        "        # Set a batch of tags\n",
        "        with mlflow.start_run():\n",
        "            mlflow.set_experiment_tags(tags)\n",
        "    \"\"\"\n",
        "    for key, value in tags.items():\n",
        "        set_experiment_tag(key, value)\n",
        "\n",
        "\n",
        "def set_tags(tags: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Log a batch of tags for the current run. If no run is active, this method will create a\n",
        "    new active run.\n",
        "\n",
        "    :param tags: Dictionary of tag_name: String -> value: (String, but will be string-ified if\n",
        "                 not)\n",
        "    :returns: None\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        tags = {\n",
        "            \"engineering\": \"ML Platform\",\n",
        "            \"release.candidate\": \"RC1\",\n",
        "            \"release.version\": \"2.2.0\",\n",
        "        }\n",
        "\n",
        "        # Set a batch of tags\n",
        "        with mlflow.start_run():\n",
        "            mlflow.set_tags(tags)\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    tags_arr = [RunTag(key, str(value)) for key, value in tags.items()]\n",
        "    MlflowClient().log_batch(run_id=run_id, metrics=[], params=[], tags=tags_arr)\n",
        "\n",
        "\n",
        "def log_artifact(local_path: str, artifact_path: Optional[str] = None) -> None:\n",
        "    \"\"\"\n",
        "    Log a local file or directory as an artifact of the currently active run. If no run is\n",
        "    active, this method will create a new active run.\n",
        "\n",
        "    :param local_path: Path to the file to write.\n",
        "    :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        # Create a features.txt artifact file\n",
        "        features = \"rooms, zipcode, median_price, school_rating, transport\"\n",
        "        with open(\"features.txt\", \"w\") as f:\n",
        "            f.write(features)\n",
        "\n",
        "        # With artifact_path=None write features.txt under\n",
        "        # root artifact_uri/artifacts directory\n",
        "        with mlflow.start_run():\n",
        "            mlflow.log_artifact(\"features.txt\")\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    MlflowClient().log_artifact(run_id, local_path, artifact_path)\n",
        "\n",
        "\n",
        "def log_artifacts(local_dir: str, artifact_path: Optional[str] = None) -> None:\n",
        "    \"\"\"\n",
        "    Log all the contents of a local directory as artifacts of the run. If no run is active,\n",
        "    this method will create a new active run.\n",
        "\n",
        "    :param local_dir: Path to the directory of files to write.\n",
        "    :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import json\n",
        "        import os\n",
        "        import mlflow\n",
        "\n",
        "        # Create some files to preserve as artifacts\n",
        "        features = \"rooms, zipcode, median_price, school_rating, transport\"\n",
        "        data = {\"state\": \"TX\", \"Available\": 25, \"Type\": \"Detached\"}\n",
        "\n",
        "        # Create couple of artifact files under the directory \"data\"\n",
        "        os.makedirs(\"data\", exist_ok=True)\n",
        "        with open(\"data/data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "        with open(\"data/features.txt\", \"w\") as f:\n",
        "            f.write(features)\n",
        "\n",
        "        # Write all files in \"data\" to root artifact_uri/states\n",
        "        with mlflow.start_run():\n",
        "            mlflow.log_artifacts(\"data\", artifact_path=\"states\")\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n",
        "\n",
        "\n",
        "def log_text(text: str, artifact_file: str) -> None:\n",
        "    \"\"\"\n",
        "    Log text as an artifact.\n",
        "\n",
        "    :param text: String containing text to log.\n",
        "    :param artifact_file: The run-relative artifact file path in posixpath format to which\n",
        "                          the text is saved (e.g. \"dir/file.txt\").\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        with mlflow.start_run():\n",
        "            # Log text to a file under the run's root artifact directory\n",
        "            mlflow.log_text(\"text1\", \"file1.txt\")\n",
        "\n",
        "            # Log text in a subdirectory of the run's root artifact directory\n",
        "            mlflow.log_text(\"text2\", \"dir/file2.txt\")\n",
        "\n",
        "            # Log HTML text\n",
        "            mlflow.log_text(\"<h1>header</h1>\", \"index.html\")\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    MlflowClient().log_text(run_id, text, artifact_file)\n",
        "\n",
        "\n",
        "def log_dict(dictionary: Dict[str, Any], artifact_file: str) -> None:\n",
        "    \"\"\"\n",
        "    Log a JSON/YAML-serializable object (e.g. `dict`) as an artifact. The serialization\n",
        "    format (JSON or YAML) is automatically inferred from the extension of `artifact_file`.\n",
        "    If the file extension doesn't exist or match any of [\".json\", \".yml\", \".yaml\"],\n",
        "    JSON format is used.\n",
        "\n",
        "    :param dictionary: Dictionary to log.\n",
        "    :param artifact_file: The run-relative artifact file path in posixpath format to which\n",
        "                          the dictionary is saved (e.g. \"dir/data.json\").\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        dictionary = {\"k\": \"v\"}\n",
        "\n",
        "        with mlflow.start_run():\n",
        "            # Log a dictionary as a JSON file under the run's root artifact directory\n",
        "            mlflow.log_dict(dictionary, \"data.json\")\n",
        "\n",
        "            # Log a dictionary as a YAML file in a subdirectory of the run's root artifact directory\n",
        "            mlflow.log_dict(dictionary, \"dir/data.yml\")\n",
        "\n",
        "            # If the file extension doesn't exist or match any of [\".json\", \".yaml\", \".yml\"],\n",
        "            # JSON format is used.\n",
        "            mlflow.log_dict(dictionary, \"data\")\n",
        "            mlflow.log_dict(dictionary, \"data.txt\")\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    MlflowClient().log_dict(run_id, dictionary, artifact_file)\n",
        "\n",
        "\n",
        "def log_figure(\n",
        "    figure: Union[\"matplotlib.figure.Figure\", \"plotly.graph_objects.Figure\"],\n",
        "    artifact_file: str,\n",
        "    *,\n",
        "    save_kwargs: Optional[Dict[str, Any]] = None,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Log a figure as an artifact. The following figure objects are supported:\n",
        "\n",
        "    - `matplotlib.figure.Figure`_\n",
        "    - `plotly.graph_objects.Figure`_\n",
        "\n",
        "    .. _matplotlib.figure.Figure:\n",
        "        https://matplotlib.org/api/_as_gen/matplotlib.figure.Figure.html\n",
        "\n",
        "    .. _plotly.graph_objects.Figure:\n",
        "        https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html\n",
        "\n",
        "    :param figure: Figure to log.\n",
        "    :param artifact_file: The run-relative artifact file path in posixpath format to which\n",
        "                          the figure is saved (e.g. \"dir/file.png\").\n",
        "    :param save_kwargs: Additional keyword arguments passed to the method that saves the figure.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Matplotlib Example\n",
        "\n",
        "        import mlflow\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.plot([0, 1], [2, 3])\n",
        "\n",
        "        with mlflow.start_run():\n",
        "            mlflow.log_figure(fig, \"figure.png\")\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Plotly Example\n",
        "\n",
        "        import mlflow\n",
        "        from plotly import graph_objects as go\n",
        "\n",
        "        fig = go.Figure(go.Scatter(x=[0, 1], y=[2, 3]))\n",
        "\n",
        "        with mlflow.start_run():\n",
        "            mlflow.log_figure(fig, \"figure.html\")\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    MlflowClient().log_figure(run_id, figure, artifact_file, save_kwargs=save_kwargs)\n",
        "\n",
        "\n",
        "def log_image(image: Union[\"numpy.ndarray\", \"PIL.Image.Image\"], artifact_file: str) -> None:\n",
        "    \"\"\"\n",
        "    Log an image as an artifact. The following image objects are supported:\n",
        "\n",
        "    - `numpy.ndarray`_\n",
        "    - `PIL.Image.Image`_\n",
        "\n",
        "    .. _numpy.ndarray:\n",
        "        https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html\n",
        "\n",
        "    .. _PIL.Image.Image:\n",
        "        https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image\n",
        "\n",
        "    Numpy array support\n",
        "        - data type (( ) represents a valid value range):\n",
        "\n",
        "            - bool\n",
        "            - integer (0 ~ 255)\n",
        "            - unsigned integer (0 ~ 255)\n",
        "            - float (0.0 ~ 1.0)\n",
        "\n",
        "            .. warning::\n",
        "\n",
        "                - Out-of-range integer values will be **clipped** to [0, 255].\n",
        "                - Out-of-range float values will be **clipped** to [0, 1].\n",
        "\n",
        "        - shape (H: height, W: width):\n",
        "\n",
        "            - H x W (Grayscale)\n",
        "            - H x W x 1 (Grayscale)\n",
        "            - H x W x 3 (an RGB channel order is assumed)\n",
        "            - H x W x 4 (an RGBA channel order is assumed)\n",
        "\n",
        "    :param image: Image to log.\n",
        "    :param artifact_file: The run-relative artifact file path in posixpath format to which\n",
        "                          the image is saved (e.g. \"dir/image.png\").\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Numpy Example\n",
        "\n",
        "        import mlflow\n",
        "        import numpy as np\n",
        "\n",
        "        image = np.random.randint(0, 256, size=(100, 100, 3), dtype=np.uint8)\n",
        "\n",
        "        with mlflow.start_run():\n",
        "            mlflow.log_image(image, \"image.png\")\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Pillow Example\n",
        "\n",
        "        import mlflow\n",
        "        from PIL import Image\n",
        "\n",
        "        image = Image.new(\"RGB\", (100, 100))\n",
        "\n",
        "        with mlflow.start_run():\n",
        "            mlflow.log_image(image, \"image.png\")\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    MlflowClient().log_image(run_id, image, artifact_file)\n",
        "\n",
        "\n",
        "@experimental\n",
        "def log_table(\n",
        "    data: Union[Dict[str, Any], \"pandas.DataFrame\"],\n",
        "    artifact_file: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Log a table to MLflow Tracking as a JSON artifact. If the artifact_file already exists\n",
        "    in the run, the data would be appended to the existing artifact_file.\n",
        "\n",
        "    :param data: Dictionary or pandas.DataFrame to log.\n",
        "    :param artifact_file: The run-relative artifact file path in posixpath format to which\n",
        "                              the table is saved (e.g. \"dir/file.json\").\n",
        "    :return: None\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Dictionary Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        table_dict = {\n",
        "            \"inputs\": [\"What is MLflow?\", \"What is Databricks?\"],\n",
        "            \"outputs\": [\"MLflow is ...\", \"Databricks is ...\"],\n",
        "            \"toxicity\": [0.0, 0.0],\n",
        "        }\n",
        "\n",
        "        with mlflow.start_run():\n",
        "            # Log the dictionary as a table\n",
        "            mlflow.log_table(data=table_dict, artifact_file=\"qabot_eval_results.json\")\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Pandas DF Example\n",
        "\n",
        "        import mlflow\n",
        "        import pandas as pd\n",
        "\n",
        "        table_dict = {\n",
        "            \"inputs\": [\"What is MLflow?\", \"What is Databricks?\"],\n",
        "            \"outputs\": [\"MLflow is ...\", \"Databricks is ...\"],\n",
        "            \"toxicity\": [0.0, 0.0],\n",
        "        }\n",
        "        df = pd.DataFrame.from_dict(table_dict)\n",
        "\n",
        "        with mlflow.start_run():\n",
        "            # Log the df as a table\n",
        "            mlflow.log_table(data=df, artifact_file=\"qabot_eval_results.json\")\n",
        "    \"\"\"\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    MlflowClient().log_table(run_id, data, artifact_file)\n",
        "\n",
        "\n",
        "@experimental\n",
        "def load_table(\n",
        "    artifact_file: str,\n",
        "    run_ids: Optional[List[str]] = None,\n",
        "    extra_columns: Optional[List[str]] = None,\n",
        ") -> \"pandas.DataFrame\":\n",
        "    \"\"\"\n",
        "    Load a table from MLflow Tracking as a pandas.DataFrame. The table is loaded from the\n",
        "    specified artifact_file in the specified run_ids. The extra_columns are columns that\n",
        "    are not in the table but are augmented with run information and added to the DataFrame.\n",
        "\n",
        "    :param artifact_file: The run-relative artifact file path in posixpath format to which\n",
        "                          table to load (e.g. \"dir/file.json\").\n",
        "    :param run_ids: Optional list of run_ids to load the table from. If no run_ids are specified,\n",
        "                    the table is loaded from all runs in the current experiment.\n",
        "    :param extra_columns: Optional list of extra columns to add to the returned DataFrame\n",
        "                          For example, if extra_columns=[\"run_id\"], then the returned DataFrame\n",
        "                          will have a column named run_id.\n",
        "\n",
        "    :return: pandas.DataFrame containing the loaded table if the artifact exists\n",
        "             or else throw a MlflowException.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example with passing run_ids\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        table_dict = {\n",
        "            \"inputs\": [\"What is MLflow?\", \"What is Databricks?\"],\n",
        "            \"outputs\": [\"MLflow is ...\", \"Databricks is ...\"],\n",
        "            \"toxicity\": [0.0, 0.0],\n",
        "        }\n",
        "\n",
        "        with mlflow.start_run() as run:\n",
        "            # Log the dictionary as a table\n",
        "            mlflow.log_table(data=table_dict, artifact_file=\"qabot_eval_results.json\")\n",
        "            run_id = run.info.run_id\n",
        "\n",
        "        loaded_table = mlflow.load_table(\n",
        "            artifact_file=\"qabot_eval_results.json\",\n",
        "            run_ids=[run_id],\n",
        "            # Append a column containing the associated run ID for each row\n",
        "            extra_columns=[\"run_id\"],\n",
        "        )\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example with passing no run_ids\n",
        "\n",
        "        # Loads the table with the specified name for all runs in the given\n",
        "        # experiment and joins them together\n",
        "        import mlflow\n",
        "\n",
        "        table_dict = {\n",
        "            \"inputs\": [\"What is MLflow?\", \"What is Databricks?\"],\n",
        "            \"outputs\": [\"MLflow is ...\", \"Databricks is ...\"],\n",
        "            \"toxicity\": [0.0, 0.0],\n",
        "        }\n",
        "\n",
        "        with mlflow.start_run():\n",
        "            # Log the dictionary as a table\n",
        "            mlflow.log_table(data=table_dict, artifact_file=\"qabot_eval_results.json\")\n",
        "\n",
        "        loaded_table = mlflow.load_table(\n",
        "            \"qabot_eval_results.json\",\n",
        "            # Append the run ID and the parent run ID to the table\n",
        "            extra_columns=[\"run_id\"],\n",
        "        )\n",
        "    \"\"\"\n",
        "    experiment_id = _get_experiment_id()\n",
        "    return MlflowClient().load_table(experiment_id, artifact_file, run_ids, extra_columns)\n",
        "\n",
        "\n",
        "def _record_logged_model(mlflow_model):\n",
        "    run_id = _get_or_start_run().info.run_id\n",
        "    MlflowClient()._record_logged_model(run_id, mlflow_model)\n",
        "\n",
        "\n",
        "def get_experiment(experiment_id: str) -> Experiment:\n",
        "    \"\"\"\n",
        "    Retrieve an experiment by experiment_id from the backend store\n",
        "\n",
        "    :param experiment_id: The string-ified experiment ID returned from ``create_experiment``.\n",
        "    :return: :py:class:`mlflow.entities.Experiment`\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        experiment = mlflow.get_experiment(\"0\")\n",
        "        print(f\"Name: {experiment.name}\")\n",
        "        print(f\"Artifact Location: {experiment.artifact_location}\")\n",
        "        print(f\"Tags: {experiment.tags}\")\n",
        "        print(f\"Lifecycle_stage: {experiment.lifecycle_stage}\")\n",
        "        print(f\"Creation timestamp: {experiment.creation_time}\")\n",
        "\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "        Name: Default\n",
        "        Artifact Location: file:///.../mlruns/0\n",
        "        Tags: {}\n",
        "        Lifecycle_stage: active\n",
        "        Creation timestamp: 1662004217511\n",
        "    \"\"\"\n",
        "    return MlflowClient().get_experiment(experiment_id)\n",
        "\n",
        "\n",
        "def get_experiment_by_name(name: str) -> Optional[Experiment]:\n",
        "    \"\"\"\n",
        "    Retrieve an experiment by experiment name from the backend store\n",
        "\n",
        "    :param name: The case sensitive experiment name.\n",
        "    :return: An instance of :py:class:`mlflow.entities.Experiment`\n",
        "             if an experiment with the specified name exists, otherwise None.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        # Case sensitive name\n",
        "        experiment = mlflow.get_experiment_by_name(\"Default\")\n",
        "        print(f\"Experiment_id: {experiment.experiment_id}\")\n",
        "        print(f\"Artifact Location: {experiment.artifact_location}\")\n",
        "        print(f\"Tags: {experiment.tags}\")\n",
        "        print(f\"Lifecycle_stage: {experiment.lifecycle_stage}\")\n",
        "        print(f\"Creation timestamp: {experiment.creation_time}\")\n",
        "\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "        Experiment_id: 0\n",
        "        Artifact Location: file:///.../mlruns/0\n",
        "        Tags: {}\n",
        "        Lifecycle_stage: active\n",
        "        Creation timestamp: 1662004217511\n",
        "    \"\"\"\n",
        "    return MlflowClient().get_experiment_by_name(name)\n",
        "\n",
        "\n",
        "def search_experiments(\n",
        "    view_type: int = ViewType.ACTIVE_ONLY,\n",
        "    max_results: Optional[int] = None,\n",
        "    filter_string: Optional[str] = None,\n",
        "    order_by: Optional[List[str]] = None,\n",
        ") -> List[Experiment]:\n",
        "    \"\"\"\n",
        "    Search for experiments that match the specified search query.\n",
        "\n",
        "    :param view_type: One of enum values ``ACTIVE_ONLY``, ``DELETED_ONLY``, or ``ALL``\n",
        "                      defined in :py:class:`mlflow.entities.ViewType`.\n",
        "    :param max_results: If passed, specifies the maximum number of experiments desired. If not\n",
        "                        passed, all experiments will be returned.\n",
        "    :param filter_string:\n",
        "        Filter query string (e.g., ``\"name = 'my_experiment'\"``), defaults to searching for all\n",
        "        experiments. The following identifiers, comparators, and logical operators are supported.\n",
        "\n",
        "        Identifiers\n",
        "          - ``name``: Experiment name\n",
        "          - ``creation_time``: Experiment creation time\n",
        "          - ``last_update_time``: Experiment last update time\n",
        "          - ``tags.<tag_key>``: Experiment tag. If ``tag_key`` contains\n",
        "            spaces, it must be wrapped with backticks (e.g., ``\"tags.`extra key`\"``).\n",
        "\n",
        "        Comparators for string attributes and tags\n",
        "            - ``=``: Equal to\n",
        "            - ``!=``: Not equal to\n",
        "            - ``LIKE``: Case-sensitive pattern match\n",
        "            - ``ILIKE``: Case-insensitive pattern match\n",
        "\n",
        "        Comparators for numeric attributes\n",
        "            - ``=``: Equal to\n",
        "            - ``!=``: Not equal to\n",
        "            - ``<``: Less than\n",
        "            - ``<=``: Less than or equal to\n",
        "            - ``>``: Greater than\n",
        "            - ``>=``: Greater than or equal to\n",
        "\n",
        "        Logical operators\n",
        "          - ``AND``: Combines two sub-queries and returns True if both of them are True.\n",
        "\n",
        "    :param order_by:\n",
        "        List of columns to order by. The ``order_by`` column can contain an optional ``DESC`` or\n",
        "        ``ASC`` value (e.g., ``\"name DESC\"``). The default ordering is ``ASC``, so ``\"name\"`` is\n",
        "        equivalent to ``\"name ASC\"``. If unspecified, defaults to ``[\"last_update_time DESC\"]``,\n",
        "        which lists experiments updated most recently first. The following fields are supported:\n",
        "\n",
        "            - ``experiment_id``: Experiment ID\n",
        "            - ``name``: Experiment name\n",
        "            - ``creation_time``: Experiment creation time\n",
        "            - ``last_update_time``: Experiment last update time\n",
        "\n",
        "    :return: A list of :py:class:`Experiment <mlflow.entities.Experiment>` objects.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "\n",
        "        def assert_experiment_names_equal(experiments, expected_names):\n",
        "            actual_names = [e.name for e in experiments if e.name != \"Default\"]\n",
        "            assert actual_names == expected_names, (actual_names, expected_names)\n",
        "\n",
        "\n",
        "        mlflow.set_tracking_uri(\"sqlite:///:memory:\")\n",
        "\n",
        "        # Create experiments\n",
        "        for name, tags in [\n",
        "            (\"a\", None),\n",
        "            (\"b\", None),\n",
        "            (\"ab\", {\"k\": \"v\"}),\n",
        "            (\"bb\", {\"k\": \"V\"}),\n",
        "        ]:\n",
        "            mlflow.create_experiment(name, tags=tags)\n",
        "\n",
        "        # Search for experiments with name \"a\"\n",
        "        experiments = mlflow.search_experiments(filter_string=\"name = 'a'\")\n",
        "        assert_experiment_names_equal(experiments, [\"a\"])\n",
        "\n",
        "        # Search for experiments with name starting with \"a\"\n",
        "        experiments = mlflow.search_experiments(filter_string=\"name LIKE 'a%'\")\n",
        "        assert_experiment_names_equal(experiments, [\"ab\", \"a\"])\n",
        "\n",
        "        # Search for experiments with tag key \"k\" and value ending with \"v\" or \"V\"\n",
        "        experiments = mlflow.search_experiments(filter_string=\"tags.k ILIKE '%v'\")\n",
        "        assert_experiment_names_equal(experiments, [\"bb\", \"ab\"])\n",
        "\n",
        "        # Search for experiments with name ending with \"b\" and tag {\"k\": \"v\"}\n",
        "        experiments = mlflow.search_experiments(filter_string=\"name LIKE '%b' AND tags.k = 'v'\")\n",
        "        assert_experiment_names_equal(experiments, [\"ab\"])\n",
        "\n",
        "        # Sort experiments by name in ascending order\n",
        "        experiments = mlflow.search_experiments(order_by=[\"name\"])\n",
        "        assert_experiment_names_equal(experiments, [\"a\", \"ab\", \"b\", \"bb\"])\n",
        "\n",
        "        # Sort experiments by ID in descending order\n",
        "        experiments = mlflow.search_experiments(order_by=[\"experiment_id DESC\"])\n",
        "        assert_experiment_names_equal(experiments, [\"bb\", \"ab\", \"b\", \"a\"])\n",
        "    \"\"\"\n",
        "\n",
        "    def pagination_wrapper_func(number_to_get, next_page_token):\n",
        "        return MlflowClient().search_experiments(\n",
        "            view_type=view_type,\n",
        "            max_results=number_to_get,\n",
        "            filter_string=filter_string,\n",
        "            order_by=order_by,\n",
        "            page_token=next_page_token,\n",
        "        )\n",
        "\n",
        "    return get_results_from_paginated_fn(\n",
        "        pagination_wrapper_func,\n",
        "        SEARCH_MAX_RESULTS_DEFAULT,\n",
        "        max_results,\n",
        "    )\n",
        "\n",
        "\n",
        "def create_experiment(\n",
        "    name: str,\n",
        "    artifact_location: Optional[str] = None,\n",
        "    tags: Optional[Dict[str, Any]] = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Create an experiment.\n",
        "\n",
        "    :param name: The experiment name, which must be unique and is case sensitive\n",
        "    :param artifact_location: The location to store run artifacts.\n",
        "                              If not provided, the server picks an appropriate default.\n",
        "    :param tags: An optional dictionary of string keys and values to set as\n",
        "                            tags on the experiment.\n",
        "    :return: String ID of the created experiment.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "        from pathlib import Path\n",
        "\n",
        "        # Create an experiment name, which must be unique and case sensitive\n",
        "        experiment_id = mlflow.create_experiment(\n",
        "            \"Social NLP Experiments\",\n",
        "            artifact_location=Path.cwd().joinpath(\"mlruns\").as_uri(),\n",
        "            tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
        "        )\n",
        "        experiment = mlflow.get_experiment(experiment_id)\n",
        "        print(f\"Name: {experiment.name}\")\n",
        "        print(f\"Experiment_id: {experiment.experiment_id}\")\n",
        "        print(f\"Artifact Location: {experiment.artifact_location}\")\n",
        "        print(f\"Tags: {experiment.tags}\")\n",
        "        print(f\"Lifecycle_stage: {experiment.lifecycle_stage}\")\n",
        "        print(f\"Creation timestamp: {experiment.creation_time}\")\n",
        "\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "        Name: Social NLP Experiments\n",
        "        Experiment_id: 1\n",
        "        Artifact Location: file:///.../mlruns\n",
        "        Tags: {'version': 'v1', 'priority': 'P1'}\n",
        "        Lifecycle_stage: active\n",
        "        Creation timestamp: 1662004217511\n",
        "    \"\"\"\n",
        "    return MlflowClient().create_experiment(name, artifact_location, tags)\n",
        "\n",
        "\n",
        "def delete_experiment(experiment_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Delete an experiment from the backend store.\n",
        "\n",
        "    :param experiment_id: The The string-ified experiment ID returned from ``create_experiment``.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        experiment_id = mlflow.create_experiment(\"New Experiment\")\n",
        "        mlflow.delete_experiment(experiment_id)\n",
        "\n",
        "        # Examine the deleted experiment details.\n",
        "        experiment = mlflow.get_experiment(experiment_id)\n",
        "        print(f\"Name: {experiment.name}\")\n",
        "        print(f\"Artifact Location: {experiment.artifact_location}\")\n",
        "        print(f\"Lifecycle_stage: {experiment.lifecycle_stage}\")\n",
        "        print(f\"Last Updated timestamp: {experiment.last_update_time}\")\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "        Name: New Experiment\n",
        "        Artifact Location: file:///.../mlruns/2\n",
        "        Lifecycle_stage: deleted\n",
        "        Last Updated timestamp: 1662004217511\n",
        "    \"\"\"\n",
        "    MlflowClient().delete_experiment(experiment_id)\n",
        "\n",
        "\n",
        "def delete_run(run_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Deletes a run with the given ID.\n",
        "\n",
        "    :param run_id: Unique identifier for the run to delete.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        with mlflow.start_run() as run:\n",
        "            mlflow.log_param(\"p\", 0)\n",
        "\n",
        "        run_id = run.info.run_id\n",
        "        mlflow.delete_run(run_id)\n",
        "\n",
        "        print(\n",
        "            f\"run_id: {run_id}; lifecycle_stage: {mlflow.get_run(run_id).info.lifecycle_stage}\"\n",
        "        )\n",
        "\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "        run_id: 45f4af3e6fd349e58579b27fcb0b8277; lifecycle_stage: deleted\n",
        "    \"\"\"\n",
        "    MlflowClient().delete_run(run_id)\n",
        "\n",
        "\n",
        "def get_artifact_uri(artifact_path: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Get the absolute URI of the specified artifact in the currently active run.\n",
        "    If `path` is not specified, the artifact root URI of the currently active\n",
        "    run will be returned; calls to ``log_artifact`` and ``log_artifacts`` write\n",
        "    artifact(s) to subdirectories of the artifact root URI.\n",
        "\n",
        "    If no run is active, this method will create a new active run.\n",
        "\n",
        "    :param artifact_path: The run-relative artifact path for which to obtain an absolute URI.\n",
        "                          For example, \"path/to/artifact\". If unspecified, the artifact root URI\n",
        "                          for the currently active run will be returned.\n",
        "    :return: An *absolute* URI referring to the specified artifact or the currently active run's\n",
        "             artifact root. For example, if an artifact path is provided and the currently active\n",
        "             run uses an S3-backed store, this may be a uri of the form\n",
        "             ``s3://<bucket_name>/path/to/artifact/root/path/to/artifact``. If an artifact path\n",
        "             is not provided and the currently active run uses an S3-backed store, this may be a\n",
        "             URI of the form ``s3://<bucket_name>/path/to/artifact/root``.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        features = \"rooms, zipcode, median_price, school_rating, transport\"\n",
        "        with open(\"features.txt\", \"w\") as f:\n",
        "            f.write(features)\n",
        "\n",
        "        # Log the artifact in a directory \"features\" under the root artifact_uri/features\n",
        "        with mlflow.start_run():\n",
        "            mlflow.log_artifact(\"features.txt\", artifact_path=\"features\")\n",
        "\n",
        "            # Fetch the artifact uri root directory\n",
        "            artifact_uri = mlflow.get_artifact_uri()\n",
        "            print(f\"Artifact uri: {artifact_uri}\")\n",
        "\n",
        "            # Fetch a specific artifact uri\n",
        "            artifact_uri = mlflow.get_artifact_uri(artifact_path=\"features/features.txt\")\n",
        "            print(f\"Artifact uri: {artifact_uri}\")\n",
        "\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "        Artifact uri: file:///.../0/a46a80f1c9644bd8f4e5dd5553fffce/artifacts\n",
        "        Artifact uri: file:///.../0/a46a80f1c9644bd8f4e5dd5553fffce/artifacts/features/features.txt\n",
        "    \"\"\"\n",
        "    return artifact_utils.get_artifact_uri(\n",
        "        run_id=_get_or_start_run().info.run_id, artifact_path=artifact_path\n",
        "    )\n",
        "\n",
        "\n",
        "def search_runs(\n",
        "    experiment_ids: Optional[List[str]] = None,\n",
        "    filter_string: str = \"\",\n",
        "    run_view_type: int = ViewType.ACTIVE_ONLY,\n",
        "    max_results: int = SEARCH_MAX_RESULTS_PANDAS,\n",
        "    order_by: Optional[List[str]] = None,\n",
        "    output_format: str = \"pandas\",\n",
        "    search_all_experiments: bool = False,\n",
        "    experiment_names: Optional[List[str]] = None,\n",
        ") -> Union[List[Run], \"pandas.DataFrame\"]:\n",
        "    \"\"\"\n",
        "    Search for Runs that fit the specified criteria.\n",
        "\n",
        "    :param experiment_ids: List of experiment IDs. Search can work with experiment IDs or\n",
        "                           experiment names, but not both in the same call. Values other than\n",
        "                           ``None`` or ``[]`` will result in error if ``experiment_names`` is\n",
        "                           also not ``None`` or ``[]``. ``None`` will default to the active\n",
        "                           experiment if ``experiment_names`` is ``None`` or ``[]``.\n",
        "    :param filter_string: Filter query string, defaults to searching all runs.\n",
        "    :param run_view_type: one of enum values ``ACTIVE_ONLY``, ``DELETED_ONLY``, or ``ALL`` runs\n",
        "                            defined in :py:class:`mlflow.entities.ViewType`.\n",
        "    :param max_results: The maximum number of runs to put in the dataframe. Default is 100,000\n",
        "                        to avoid causing out-of-memory issues on the user's machine.\n",
        "    :param order_by: List of columns to order by (e.g., \"metrics.rmse\"). The ``order_by`` column\n",
        "                     can contain an optional ``DESC`` or ``ASC`` value. The default is ``ASC``.\n",
        "                     The default ordering is to sort by ``start_time DESC``, then ``run_id``.\n",
        "    :param output_format: The output format to be returned. If ``pandas``, a ``pandas.DataFrame``\n",
        "                          is returned and, if ``list``, a list of :py:class:`mlflow.entities.Run`\n",
        "                          is returned.\n",
        "    :param search_all_experiments: Boolean specifying whether all experiments should be searched.\n",
        "        Only honored if ``experiment_ids`` is ``[]`` or ``None``.\n",
        "    :param experiment_names: List of experiment names. Search can work with experiment IDs or\n",
        "                             experiment names, but not both in the same call. Values other\n",
        "                             than ``None`` or ``[]`` will result in error if ``experiment_ids``\n",
        "                             is also not ``None`` or ``[]``. ``None`` will default to the active\n",
        "                             experiment if ``experiment_ids`` is ``None`` or ``[]``.\n",
        "    :return: If output_format is ``list``: a list of :py:class:`mlflow.entities.Run`. If\n",
        "             output_format is ``pandas``: ``pandas.DataFrame`` of runs, where each metric,\n",
        "             parameter, and tag is expanded into its own column named metrics.*, params.*, or\n",
        "             tags.* respectively. For runs that don't have a particular metric, parameter, or tag,\n",
        "             the value for the corresponding column is (NumPy) ``Nan``, ``None``, or ``None``\n",
        "             respectively.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        # Create an experiment and log two runs under it\n",
        "        experiment_name = \"Social NLP Experiments\"\n",
        "        experiment_id = mlflow.create_experiment(experiment_name)\n",
        "        with mlflow.start_run(experiment_id=experiment_id):\n",
        "            mlflow.log_metric(\"m\", 1.55)\n",
        "            mlflow.set_tag(\"s.release\", \"1.1.0-RC\")\n",
        "        with mlflow.start_run(experiment_id=experiment_id):\n",
        "            mlflow.log_metric(\"m\", 2.50)\n",
        "            mlflow.set_tag(\"s.release\", \"1.2.0-GA\")\n",
        "\n",
        "        # Search for all the runs in the experiment with the given experiment ID\n",
        "        df = mlflow.search_runs([experiment_id], order_by=[\"metrics.m DESC\"])\n",
        "        print(df[[\"metrics.m\", \"tags.s.release\", \"run_id\"]])\n",
        "        print(\"--\")\n",
        "\n",
        "        # Search the experiment_id using a filter_string with tag\n",
        "        # that has a case insensitive pattern\n",
        "        filter_string = \"tags.s.release ILIKE '%rc%'\"\n",
        "        df = mlflow.search_runs([experiment_id], filter_string=filter_string)\n",
        "        print(df[[\"metrics.m\", \"tags.s.release\", \"run_id\"]])\n",
        "        print(\"--\")\n",
        "\n",
        "        # Search for all the runs in the experiment with the given experiment name\n",
        "        df = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"metrics.m DESC\"])\n",
        "        print(df[[\"metrics.m\", \"tags.s.release\", \"run_id\"]])\n",
        "\n",
        "\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "           metrics.m tags.s.release                            run_id\n",
        "        0       2.50       1.2.0-GA  147eed886ab44633902cc8e19b2267e2\n",
        "        1       1.55       1.1.0-RC  5cc7feaf532f496f885ad7750809c4d4\n",
        "        --\n",
        "           metrics.m tags.s.release                            run_id\n",
        "        0       1.55       1.1.0-RC  5cc7feaf532f496f885ad7750809c4d4\n",
        "        --\n",
        "           metrics.m tags.s.release                            run_id\n",
        "        0       2.50       1.2.0-GA  147eed886ab44633902cc8e19b2267e2\n",
        "        1       1.55       1.1.0-RC  5cc7feaf532f496f885ad7750809c4d4\n",
        "    \"\"\"\n",
        "    no_ids = experiment_ids is None or len(experiment_ids) == 0\n",
        "    no_names = experiment_names is None or len(experiment_names) == 0\n",
        "    no_ids_or_names = no_ids and no_names\n",
        "    if not no_ids and not no_names:\n",
        "        raise MlflowException(\n",
        "            message=\"Only experiment_ids or experiment_names can be used, but not both\",\n",
        "            error_code=INVALID_PARAMETER_VALUE,\n",
        "        )\n",
        "\n",
        "    if search_all_experiments and no_ids_or_names:\n",
        "        experiment_ids = [\n",
        "            exp.experiment_id for exp in search_experiments(view_type=ViewType.ACTIVE_ONLY)\n",
        "        ]\n",
        "    elif no_ids_or_names:\n",
        "        experiment_ids = [_get_experiment_id()]\n",
        "    elif not no_names:\n",
        "        experiments = []\n",
        "        for n in experiment_names:\n",
        "            if n is not None:\n",
        "                experiment_by_name = get_experiment_by_name(n)\n",
        "                if experiment_by_name:\n",
        "                    experiments.append(experiment_by_name)\n",
        "                else:\n",
        "                    _logger.warning(\"Cannot retrieve experiment by name %s\", n)\n",
        "        experiment_ids = [e.experiment_id for e in experiments if e is not None]\n",
        "\n",
        "    if len(experiment_ids) == 0:\n",
        "        runs = []\n",
        "    else:\n",
        "        # Using an internal function as the linter doesn't like assigning a lambda, and inlining the\n",
        "        # full thing is a mess\n",
        "        def pagination_wrapper_func(number_to_get, next_page_token):\n",
        "            return MlflowClient().search_runs(\n",
        "                experiment_ids,\n",
        "                filter_string,\n",
        "                run_view_type,\n",
        "                number_to_get,\n",
        "                order_by,\n",
        "                next_page_token,\n",
        "            )\n",
        "\n",
        "        runs = get_results_from_paginated_fn(\n",
        "            pagination_wrapper_func,\n",
        "            NUM_RUNS_PER_PAGE_PANDAS,\n",
        "            max_results,\n",
        "        )\n",
        "\n",
        "    if output_format == \"list\":\n",
        "        return runs  # List[mlflow.entities.run.Run]\n",
        "    elif output_format == \"pandas\":\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "\n",
        "        info = {\n",
        "            \"run_id\": [],\n",
        "            \"experiment_id\": [],\n",
        "            \"status\": [],\n",
        "            \"artifact_uri\": [],\n",
        "            \"start_time\": [],\n",
        "            \"end_time\": [],\n",
        "        }\n",
        "        params, metrics, tags = ({}, {}, {})\n",
        "        PARAM_NULL, METRIC_NULL, TAG_NULL = (None, np.nan, None)\n",
        "        for i, run in enumerate(runs):\n",
        "            info[\"run_id\"].append(run.info.run_id)\n",
        "            info[\"experiment_id\"].append(run.info.experiment_id)\n",
        "            info[\"status\"].append(run.info.status)\n",
        "            info[\"artifact_uri\"].append(run.info.artifact_uri)\n",
        "            info[\"start_time\"].append(pd.to_datetime(run.info.start_time, unit=\"ms\", utc=True))\n",
        "            info[\"end_time\"].append(pd.to_datetime(run.info.end_time, unit=\"ms\", utc=True))\n",
        "\n",
        "            # Params\n",
        "            param_keys = set(params.keys())\n",
        "            for key in param_keys:\n",
        "                if key in run.data.params:\n",
        "                    params[key].append(run.data.params[key])\n",
        "                else:\n",
        "                    params[key].append(PARAM_NULL)\n",
        "            new_params = set(run.data.params.keys()) - param_keys\n",
        "            for p in new_params:\n",
        "                params[p] = [PARAM_NULL] * i  # Fill in null values for all previous runs\n",
        "                params[p].append(run.data.params[p])\n",
        "\n",
        "            # Metrics\n",
        "            metric_keys = set(metrics.keys())\n",
        "            for key in metric_keys:\n",
        "                if key in run.data.metrics:\n",
        "                    metrics[key].append(run.data.metrics[key])\n",
        "                else:\n",
        "                    metrics[key].append(METRIC_NULL)\n",
        "            new_metrics = set(run.data.metrics.keys()) - metric_keys\n",
        "            for m in new_metrics:\n",
        "                metrics[m] = [METRIC_NULL] * i\n",
        "                metrics[m].append(run.data.metrics[m])\n",
        "\n",
        "            # Tags\n",
        "            tag_keys = set(tags.keys())\n",
        "            for key in tag_keys:\n",
        "                if key in run.data.tags:\n",
        "                    tags[key].append(run.data.tags[key])\n",
        "                else:\n",
        "                    tags[key].append(TAG_NULL)\n",
        "            new_tags = set(run.data.tags.keys()) - tag_keys\n",
        "            for t in new_tags:\n",
        "                tags[t] = [TAG_NULL] * i\n",
        "                tags[t].append(run.data.tags[t])\n",
        "\n",
        "        data = {}\n",
        "        data.update(info)\n",
        "        for key, value in metrics.items():\n",
        "            data[\"metrics.\" + key] = value\n",
        "        for key, value in params.items():\n",
        "            data[\"params.\" + key] = value\n",
        "        for key, value in tags.items():\n",
        "            data[\"tags.\" + key] = value\n",
        "        return pd.DataFrame(data)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Unsupported output format: %s. Supported string values are 'pandas' or 'list'\"\n",
        "            % output_format\n",
        "        )\n",
        "\n",
        "\n",
        "def _get_or_start_run():\n",
        "    if len(_active_run_stack) > 0:\n",
        "        return _active_run_stack[-1]\n",
        "    return start_run()\n",
        "\n",
        "\n",
        "def _get_experiment_id_from_env():\n",
        "    experiment_name = MLFLOW_EXPERIMENT_NAME.get()\n",
        "    experiment_id = MLFLOW_EXPERIMENT_ID.get()\n",
        "    if experiment_name is not None:\n",
        "        exp = MlflowClient().get_experiment_by_name(experiment_name)\n",
        "        if exp:\n",
        "            if experiment_id and experiment_id != exp.experiment_id:\n",
        "                raise MlflowException(\n",
        "                    message=f\"The provided {MLFLOW_EXPERIMENT_ID} environment variable \"\n",
        "                    f\"value `{experiment_id}` does not match the experiment id \"\n",
        "                    f\"`{exp.experiment_id}` for experiment name `{experiment_name}`\",\n",
        "                    error_code=INVALID_PARAMETER_VALUE,\n",
        "                )\n",
        "            else:\n",
        "                return exp.experiment_id\n",
        "        else:\n",
        "            return MlflowClient().create_experiment(name=experiment_name)\n",
        "    if experiment_id is not None:\n",
        "        try:\n",
        "            exp = MlflowClient().get_experiment(experiment_id)\n",
        "            return exp.experiment_id\n",
        "        except MlflowException as exc:\n",
        "            raise MlflowException(\n",
        "                message=f\"The provided {MLFLOW_EXPERIMENT_ID} environment variable \"\n",
        "                f\"value `{experiment_id}` does not exist in the tracking server. Provide a valid \"\n",
        "                f\"experiment_id.\",\n",
        "                error_code=INVALID_PARAMETER_VALUE,\n",
        "            ) from exc\n",
        "\n",
        "\n",
        "def _get_experiment_id():\n",
        "    if _active_experiment_id:\n",
        "        return _active_experiment_id\n",
        "    else:\n",
        "        return _get_experiment_id_from_env() or default_experiment_registry.get_experiment_id()\n",
        "\n",
        "\n",
        "@autologging_integration(\"mlflow\")\n",
        "def autolog(\n",
        "    log_input_examples: bool = False,\n",
        "    log_model_signatures: bool = True,\n",
        "    log_models: bool = True,\n",
        "    log_datasets: bool = True,\n",
        "    disable: bool = False,\n",
        "    exclusive: bool = False,\n",
        "    disable_for_unsupported_versions: bool = False,\n",
        "    silent: bool = False,\n",
        "    extra_tags: Optional[Dict[str, str]] = None,\n",
        "    # pylint: disable=unused-argument\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Enables (or disables) and configures autologging for all supported integrations.\n",
        "\n",
        "    The parameters are passed to any autologging integrations that support them.\n",
        "\n",
        "    See the :ref:`tracking docs <automatic-logging>` for a list of supported autologging\n",
        "    integrations.\n",
        "\n",
        "    Note that framework-specific configurations set at any point will take precedence over\n",
        "    any configurations set by this function. For example:\n",
        "\n",
        "    .. testcode:: python\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        mlflow.autolog(log_models=False, exclusive=True)\n",
        "        import sklearn\n",
        "\n",
        "    would enable autologging for `sklearn` with `log_models=False` and `exclusive=True`,\n",
        "    but\n",
        "\n",
        "    .. testcode:: python\n",
        "\n",
        "        import mlflow\n",
        "\n",
        "        mlflow.autolog(log_models=False, exclusive=True)\n",
        "\n",
        "        import sklearn\n",
        "\n",
        "        mlflow.sklearn.autolog(log_models=True)\n",
        "\n",
        "    would enable autologging for `sklearn` with `log_models=True` and `exclusive=False`,\n",
        "    the latter resulting from the default value for `exclusive` in `mlflow.sklearn.autolog`;\n",
        "    other framework autolog functions (e.g. `mlflow.tensorflow.autolog`) would use the\n",
        "    configurations set by `mlflow.autolog` (in this instance, `log_models=False`, `exclusive=True`),\n",
        "    until they are explicitly called by the user.\n",
        "\n",
        "    :param log_input_examples: If ``True``, input examples from training datasets are collected and\n",
        "                               logged along with model artifacts during training. If ``False``,\n",
        "                               input examples are not logged.\n",
        "                               Note: Input examples are MLflow model attributes\n",
        "                               and are only collected if ``log_models`` is also ``True``.\n",
        "    :param log_model_signatures: If ``True``,\n",
        "                                 :py:class:`ModelSignatures <mlflow.models.ModelSignature>`\n",
        "                                 describing model inputs and outputs are collected and logged along\n",
        "                                 with model artifacts during training. If ``False``, signatures are\n",
        "                                 not logged. Note: Model signatures are MLflow model attributes\n",
        "                                 and are only collected if ``log_models`` is also ``True``.\n",
        "    :param log_models: If ``True``, trained models are logged as MLflow model artifacts.\n",
        "                       If ``False``, trained models are not logged.\n",
        "                       Input examples and model signatures, which are attributes of MLflow models,\n",
        "                       are also omitted when ``log_models`` is ``False``.\n",
        "    :param log_datasets: If ``True``, dataset information is logged to MLflow Tracking.\n",
        "                         If ``False``, dataset information is not logged.\n",
        "    :param disable: If ``True``, disables all supported autologging integrations. If ``False``,\n",
        "                    enables all supported autologging integrations.\n",
        "    :param exclusive: If ``True``, autologged content is not logged to user-created fluent runs.\n",
        "                      If ``False``, autologged content is logged to the active fluent run,\n",
        "                      which may be user-created.\n",
        "    :param disable_for_unsupported_versions: If ``True``, disable autologging for versions of\n",
        "                      all integration libraries that have not been tested against this version\n",
        "                      of the MLflow client or are incompatible.\n",
        "    :param silent: If ``True``, suppress all event logs and warnings from MLflow during autologging\n",
        "                   setup and training execution. If ``False``, show all events and warnings during\n",
        "                   autologging setup and training execution.\n",
        "    :param extra_tags: A dictionary of extra tags to set on each managed run created by autologging.\n",
        "\n",
        "    .. testcode:: python\n",
        "        :caption: Example\n",
        "\n",
        "        import numpy as np\n",
        "        import mlflow.sklearn\n",
        "        from mlflow import MlflowClient\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "        def print_auto_logged_info(r):\n",
        "            tags = {k: v for k, v in r.data.tags.items() if not k.startswith(\"mlflow.\")}\n",
        "            artifacts = [f.path for f in MlflowClient().list_artifacts(r.info.run_id, \"model\")]\n",
        "            print(f\"run_id: {r.info.run_id}\")\n",
        "            print(f\"artifacts: {artifacts}\")\n",
        "            print(f\"params: {r.data.params}\")\n",
        "            print(f\"metrics: {r.data.metrics}\")\n",
        "            print(f\"tags: {tags}\")\n",
        "\n",
        "\n",
        "        # prepare training data\n",
        "        X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
        "        y = np.dot(X, np.array([1, 2])) + 3\n",
        "\n",
        "        # Auto log all the parameters, metrics, and artifacts\n",
        "        mlflow.autolog()\n",
        "        model = LinearRegression()\n",
        "        with mlflow.start_run() as run:\n",
        "            model.fit(X, y)\n",
        "\n",
        "        # fetch the auto logged parameters and metrics for ended run\n",
        "        print_auto_logged_info(mlflow.get_run(run_id=run.info.run_id))\n",
        "\n",
        "    .. code-block:: text\n",
        "        :caption: Output\n",
        "\n",
        "        run_id: fd10a17d028c47399a55ab8741721ef7\n",
        "        artifacts: ['model/MLmodel', 'model/conda.yaml', 'model/model.pkl']\n",
        "        params: {'copy_X': 'True',\n",
        "                 'normalize': 'False',\n",
        "                 'fit_intercept': 'True',\n",
        "                 'n_jobs': 'None'}\n",
        "        metrics: {'training_score': 1.0,\n",
        "                  'training_root_mean_squared_error': 4.440892098500626e-16,\n",
        "                  'training_r2_score': 1.0,\n",
        "                  'training_mean_absolute_error': 2.220446049250313e-16,\n",
        "                  'training_mean_squared_error': 1.9721522630525295e-31}\n",
        "        tags: {'estimator_class': 'sklearn.linear_model._base.LinearRegression',\n",
        "               'estimator_name': 'LinearRegression'}\n",
        "    \"\"\"\n",
        "    from mlflow import (\n",
        "        fastai,\n",
        "        gluon,\n",
        "        lightgbm,\n",
        "        pyspark,\n",
        "        pytorch,\n",
        "        sklearn,\n",
        "        spark,\n",
        "        statsmodels,\n",
        "        tensorflow,\n",
        "        transformers,\n",
        "        xgboost,\n",
        "    )\n",
        "\n",
        "    locals_copy = locals().items()\n",
        "\n",
        "    # Mapping of library module name to specific autolog function\n",
        "    # eg: mxnet.gluon is the actual library, mlflow.gluon.autolog is our autolog function for it\n",
        "    LIBRARY_TO_AUTOLOG_FN = {\n",
        "        \"tensorflow\": tensorflow.autolog,\n",
        "        \"mxnet.gluon\": gluon.autolog,\n",
        "        \"xgboost\": xgboost.autolog,\n",
        "        \"lightgbm\": lightgbm.autolog,\n",
        "        \"statsmodels\": statsmodels.autolog,\n",
        "        \"sklearn\": sklearn.autolog,\n",
        "        \"fastai\": fastai.autolog,\n",
        "        \"pyspark\": spark.autolog,\n",
        "        \"pyspark.ml\": pyspark.ml.autolog,\n",
        "        # TODO: Broaden this beyond pytorch_lightning as we add autologging support for more\n",
        "        # Pytorch frameworks under mlflow.pytorch.autolog\n",
        "        \"pytorch_lightning\": pytorch.autolog,\n",
        "        \"setfit\": transformers.autolog,\n",
        "        \"transformers\": transformers.autolog,\n",
        "    }\n",
        "\n",
        "    def get_autologging_params(autolog_fn):\n",
        "        try:\n",
        "            needed_params = list(inspect.signature(autolog_fn).parameters.keys())\n",
        "            return {k: v for k, v in locals_copy if k in needed_params}\n",
        "        except Exception:\n",
        "            return {}\n",
        "\n",
        "    def setup_autologging(module):\n",
        "        try:\n",
        "            autolog_fn = LIBRARY_TO_AUTOLOG_FN[module.__name__]\n",
        "\n",
        "            # Only call integration's autolog function with `mlflow.autolog` configs\n",
        "            # if the integration's autolog function has not already been called by the user.\n",
        "            # Logic is as follows:\n",
        "            # - if a previous_config exists, that means either `mlflow.autolog` or\n",
        "            #   `mlflow.integration.autolog` was called.\n",
        "            # - if the config contains `AUTOLOGGING_CONF_KEY_IS_GLOBALLY_CONFIGURED`, the\n",
        "            #   configuration was set by `mlflow.autolog`, and so we can safely call `autolog_fn`\n",
        "            #   with `autologging_params`.\n",
        "            # - if the config doesn't contain this key, the configuration was set by an\n",
        "            #   `mlflow.integration.autolog` call, so we should not call `autolog_fn` with\n",
        "            #   new configs.\n",
        "            prev_config = AUTOLOGGING_INTEGRATIONS.get(autolog_fn.integration_name)\n",
        "            if prev_config and not prev_config.get(\n",
        "                AUTOLOGGING_CONF_KEY_IS_GLOBALLY_CONFIGURED, False\n",
        "            ):\n",
        "                return\n",
        "\n",
        "            autologging_params = get_autologging_params(autolog_fn)\n",
        "            autolog_fn(**autologging_params)\n",
        "            AUTOLOGGING_INTEGRATIONS[autolog_fn.integration_name][\n",
        "                AUTOLOGGING_CONF_KEY_IS_GLOBALLY_CONFIGURED\n",
        "            ] = True\n",
        "            if not autologging_is_disabled(\n",
        "                autolog_fn.integration_name\n",
        "            ) and not autologging_params.get(\"silent\", False):\n",
        "                _logger.info(\"Autologging successfully enabled for %s.\", module.__name__)\n",
        "        except Exception as e:\n",
        "            if is_testing():\n",
        "                # Raise unexpected exceptions in test mode in order to detect\n",
        "                # errors within dependent autologging integrations\n",
        "                raise\n",
        "            elif not autologging_params.get(\"silent\", False):\n",
        "                _logger.warning(\n",
        "                    \"Exception raised while enabling autologging for %s: %s\",\n",
        "                    module.__name__,\n",
        "                    str(e),\n",
        "                )\n",
        "\n",
        "    # for each autolog library (except pyspark), register a post-import hook.\n",
        "    # this way, we do not send any errors to the user until we know they are using the library.\n",
        "    # the post-import hook also retroactively activates for previously-imported libraries.\n",
        "    for module in list(set(LIBRARY_TO_AUTOLOG_FN.keys()) - {\"pyspark\", \"pyspark.ml\"}):\n",
        "        register_post_import_hook(setup_autologging, module, overwrite=True)\n",
        "\n",
        "    if is_in_databricks_runtime():\n",
        "        # for pyspark, we activate autologging immediately, without waiting for a module import.\n",
        "        # this is because on Databricks a SparkSession already exists and the user can directly\n",
        "        #   interact with it, and this activity should be logged.\n",
        "        import pyspark as pyspark_module\n",
        "        import pyspark.ml as pyspark_ml_module\n",
        "\n",
        "        setup_autologging(pyspark_module)\n",
        "        setup_autologging(pyspark_ml_module)\n",
        "    else:\n",
        "        register_post_import_hook(setup_autologging, \"pyspark\", overwrite=True)\n",
        "        register_post_import_hook(setup_autologging, \"pyspark.ml\", overwrite=True)\n"
      ],
      "metadata": {
        "id": "Jp6O5t7d3yq0"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}