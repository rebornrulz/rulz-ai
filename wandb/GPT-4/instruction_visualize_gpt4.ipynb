{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Tuning with GPT-4\n",
    "\n",
    "This notebook is developed to produce the pie chart html/figure in the GPT-4-LLM paper. It analyzes the GPT4 output by following the instructions.\n",
    "\n",
    "```\n",
    "``Instruction Tuning with GPT-4'' (https://arxiv.org/abs/2304.03277)\n",
    "Baolin Peng*, Chunyuan Li*, Pengcheng He*, Michel Galley, Jianfeng Gao (*Equal Contribution)\n",
    "```\n",
    "\n",
    "- Project: https://instruction-tuning-with-gpt-4.github.io/\n",
    "- Github Repo: https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM\n",
    "\n",
    "Please submit an issue in the github repo, if you have any questions.\n",
    "\n",
    "\n",
    "\n",
    "*Note: The original script from [self-instruct repo](https://github.com/yizhongw/self-instruct/blob/main/self_instruct/instruction_visualize.ipynb). The script uses Berkeley Neural Parser to parse the generated instructions, and visualize the results using Plotly. Please make sure to install benepar following their documentation [here](https://github.com/nikitakit/self-attentive-parser#installation).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'benepar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbenepar\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpython -m spacy download en_core_web_md\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39men_core_web_md\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'benepar'"
     ]
    }
   ],
   "source": [
    "import benepar, spacy\n",
    "\n",
    "!python -m spacy download en_core_web_md\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"The time for action is now. It's never too late to do something.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Verb-Noun Pairs for GPT4 Output\n",
    "\n",
    ":warning: Warning: It takes 20 minutes to run the entire pre-processing, and save it into a csv file. You consider to skip processing, and load our pre-process Verb-Noun CSV file in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     first_sent \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(doc\u001b[39m.\u001b[39msents)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m find_root_verb_and_its_dobj(first_sent\u001b[39m.\u001b[39mroot)\n\u001b[1;32m---> 19\u001b[0m find_root_verb_and_its_dobj_in_string(\u001b[39m\"\u001b[39;49m\u001b[39mWrite me a story about education.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m, in \u001b[0;36mfind_root_verb_and_its_dobj_in_string\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_root_verb_and_its_dobj_in_string\u001b[39m(s):\n\u001b[1;32m---> 15\u001b[0m     doc \u001b[39m=\u001b[39m nlp(s)\n\u001b[0;32m     16\u001b[0m     first_sent \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(doc\u001b[39m.\u001b[39msents)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m find_root_verb_and_its_dobj(first_sent\u001b[39m.\u001b[39mroot)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "def find_root_verb_and_its_dobj(tree_root):\n",
    "    # first check if the current node and its children satisfy the condition\n",
    "    if tree_root.pos_ == \"VERB\":\n",
    "        for child in tree_root.children:\n",
    "            if child.dep_ == \"dobj\" and child.pos_ == \"NOUN\":\n",
    "                return tree_root.lemma_, child.lemma_\n",
    "        return tree_root.lemma_, None\n",
    "    # if not, check its children\n",
    "    for child in tree_root.children:\n",
    "        return find_root_verb_and_its_dobj(child)\n",
    "    # if no children satisfy the condition, return None\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def find_root_verb_and_its_dobj_in_string(s):\n",
    "    doc = nlp(s)\n",
    "    first_sent = list(doc.sents)[0]\n",
    "    return find_root_verb_and_its_dobj(first_sent.root)\n",
    "\n",
    "\n",
    "find_root_verb_and_its_dobj_in_string(\"Write me a story about education.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "\n",
    "generated_data_path = \"data/gpt4_alpaca_data_0329.json\"\n",
    "\n",
    "with open(generated_data_path, \"r\") as fin:\n",
    "    gpt4_machine_generated_tasks = json.load(fin)\n",
    "\n",
    "# print(gpt4_machine_generated_tasks[0])\n",
    "\n",
    "instruction_outputs = set(\n",
    "    [task[\"output\"] for task in gpt4_machine_generated_tasks]\n",
    ")  # if you are interested in studying the instructions, please change the task key\n",
    "print(len(instruction_outputs))\n",
    "\n",
    "raw_phrases = []\n",
    "for out in tqdm.tqdm(instruction_outputs):\n",
    "    try:\n",
    "        verb, noun = find_root_verb_and_its_dobj_in_string(out)\n",
    "        raw_phrases.append({\"verb\": verb, \"noun\": noun, \"instruction_output\": out})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_phrases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mlen\u001b[39m(raw_phrases)\n\u001b[0;32m      2\u001b[0m raw_phrases \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(raw_phrases)\n\u001b[0;32m      3\u001b[0m raw_phrases\u001b[39m.\u001b[39mto_csv(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/gpt4_alpaca_verb_noun_output.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'raw_phrases' is not defined"
     ]
    }
   ],
   "source": [
    "len(raw_phrases)\n",
    "raw_phrases = pd.DataFrame(raw_phrases)\n",
    "raw_phrases.to_csv(r\"data/gpt4_alpaca_verb_noun_output.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pie Chart Creation on Verb-Noun\n",
    "\n",
    "Load our pre-process Verb-Noun CSV file, and create the html file with plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m raw_phrases \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/gpt4_alpaca_verb_noun_output.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m raw_phrases \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(raw_phrases)\n\u001b[0;32m      3\u001b[0m phrases \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(raw_phrases)\u001b[39m.\u001b[39mdropna()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "raw_phrases = pd.read_csv(r\"data/gpt4_alpaca_verb_noun_output.csv\")\n",
    "raw_phrases = pd.DataFrame(raw_phrases)\n",
    "phrases = pd.DataFrame(raw_phrases).dropna()\n",
    "count_list = (\n",
    "    phrases[[\"verb\", \"noun\"]]\n",
    "    .groupby([\"verb\", \"noun\"])\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mlen\u001b[39m(count_list)\n\u001b[0;32m      2\u001b[0m \u001b[39m# count_list[:25]\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[39m# count_list[:25].plot.barh()\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# plt.ylabel('verb, noun')\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# plt.xlabel('frequency')\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# plt.show()\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'count_list' is not defined"
     ]
    }
   ],
   "source": [
    "len(count_list)\n",
    "# count_list[:25]\n",
    "\n",
    "# count_list[:25].plot.barh()\n",
    "# plt.ylabel('verb, noun')\n",
    "# plt.xlabel('frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phrases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m top_verbs \u001b[39m=\u001b[39m phrases[[\u001b[39m\"\u001b[39m\u001b[39mverb\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mgroupby([\u001b[39m\"\u001b[39m\u001b[39mverb\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39msize()\u001b[39m.\u001b[39mnlargest(\u001b[39m20\u001b[39m)\u001b[39m.\u001b[39mreset_index()\n\u001b[0;32m      3\u001b[0m df \u001b[39m=\u001b[39m phrases[phrases[\u001b[39m\"\u001b[39m\u001b[39mverb\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39misin(top_verbs[\u001b[39m\"\u001b[39m\u001b[39mverb\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtolist())]\n\u001b[0;32m      4\u001b[0m \u001b[39m# df = df[~df[\"noun\"].isin([\"I\", \"what\"])]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# df = phrases\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# df[~df[\"verb\"].isin(top_verbs[\"verb\"].tolist())][\"verb\"] = \"other\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# df[~df[\"verb\"].isin(top_verbs[\"verb\"].tolist())][\"noun\"] = \"other\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'phrases' is not defined"
     ]
    }
   ],
   "source": [
    "top_verbs = phrases[[\"verb\"]].groupby([\"verb\"]).size().nlargest(20).reset_index()\n",
    "\n",
    "df = phrases[phrases[\"verb\"].isin(top_verbs[\"verb\"].tolist())]\n",
    "# df = df[~df[\"noun\"].isin([\"I\", \"what\"])]\n",
    "# df = phrases\n",
    "# df[~df[\"verb\"].isin(top_verbs[\"verb\"].tolist())][\"verb\"] = \"other\"\n",
    "# df[~df[\"verb\"].isin(top_verbs[\"verb\"].tolist())][\"noun\"] = \"other\"\n",
    "df = (\n",
    "    df.groupby([\"verb\", \"noun\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"count\"})\n",
    "    .sort_values(by=[\"count\"], ascending=False)\n",
    ")\n",
    "# df = df[df[\"count\"] > 10]\n",
    "df = (\n",
    "    df.groupby(\"verb\")\n",
    "    .apply(lambda x: x.sort_values(\"count\", ascending=False).head(4))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# df[\"blank\"] = \"ROOT\"\n",
    "# df = phrases.groupby([\"verb\", \"noun\"]).size().sort_values(ascending=False).head(5).reset_index().rename(columns={0: \"count\"})\n",
    "\n",
    "df = df[df[\"count\"] > 10]\n",
    "fig = px.sunburst(df, path=[\"verb\", \"noun\"], values=\"count\")\n",
    "# fig.update_layout(uniformtext=dict(minsize=10, mode='hide'))\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    "    font_family=\"Times New Roman\",\n",
    ")\n",
    "# fig.show()\n",
    "fig.write_html(\"output/gpt4_alpaca_verb_noun_output.html\")\n",
    "# fig.savefig(\"output/verb_noun.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-instruct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "364c1b690e879c5ffe4e7bee613e5b5479420b51b03f29835176bef56dfadb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
