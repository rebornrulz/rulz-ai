{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tghWegsjhpkt"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:36.845160Z",
     "iopub.status.busy": "2023-07-28T11:06:36.844915Z",
     "iopub.status.idle": "2023-07-28T11:06:36.849808Z",
     "shell.execute_reply": "2023-07-28T11:06:36.849007Z"
    },
    "id": "rSGJWC5biBiG"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuSYVbwEYNHw"
   },
   "source": [
    "# TensorFlow Data Validation\n",
    "***An Example of a Key Component of TensorFlow Extended***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLsMb4vqY244"
   },
   "source": [
    "Note: You can run this example right now in a Jupyter-style notebook, no setup required!  Just click \"Run in Google Colab\"\n",
    "\n",
    "<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic\">\n",
    "<img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a></td>\n",
    "<td><a target=\"_blank\" href=\"https://colab.sandbox.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/data_validation/tfdv_basic.ipynb\">\n",
    "<img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Run in Google Colab</a></td>\n",
    "<td><a target=\"_blank\" href=\"https://github.com/tensorflow/tfx/blob/master/docs/tutorials/data_validation/tfdv_basic.ipynb\">\n",
    "<img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">View source on GitHub</a></td>\n",
    "<td><a href=\"https://storage.googleapis.com/tensorflow_docs/tfx/docs/tutorials/data_validation/tfdv_basic.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a></td>\n",
    "</table></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPt5BHTwy_0F"
   },
   "source": [
    "This example colab notebook illustrates how TensorFlow Data Validation (TFDV) can be used to investigate and visualize your dataset.  That includes looking at descriptive statistics, inferring a schema, checking for and fixing anomalies, and checking for drift and skew in our dataset.  It's important to understand your dataset's characteristics, including how it might change over time in your production pipeline.  It's also important to look for anomalies in your data, and to compare your training, evaluation, and serving datasets to make sure that they're consistent.\n",
    "\n",
    "We'll use data from the [Taxi Trips dataset](https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew) released by the City of Chicago.\n",
    "\n",
    "Note: This site provides applications using data that has been modified for use from its original source, www.cityofchicago.org, the official website of the City of Chicago. The City of Chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. The data provided at this site is subject to change at any time. It is understood that the data provided at this site is being used at one’s own risk.\n",
    "\n",
    "[Read more](https://cloud.google.com/bigquery/public-data/chicago-taxi) about the dataset in [Google BigQuery](https://cloud.google.com/bigquery/). Explore the full dataset in the [BigQuery UI](https://bigquery.cloud.google.com/dataset/bigquery-public-data:chicago_taxi_trips).\n",
    "\n",
    "Key Point: As a modeler and developer, think about how this data is used and the potential benefits and harm a model's predictions can cause. A model like this could reinforce societal biases and disparities. Is a feature relevant to the problem you want to solve or will it introduce bias? For more information, read about [ML fairness](https://developers.google.com/machine-learning/fairness-overview/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fnm6Mj3vTGLm"
   },
   "source": [
    "The columns in the dataset are:\n",
    "<table>\n",
    "<tr><td>pickup_community_area</td><td>fare</td><td>trip_start_month</td></tr>\n",
    "\n",
    "<tr><td>trip_start_hour</td><td>trip_start_day</td><td>trip_start_timestamp</td></tr>\n",
    "<tr><td>pickup_latitude</td><td>pickup_longitude</td><td>dropoff_latitude</td></tr>\n",
    "<tr><td>dropoff_longitude</td><td>trip_miles</td><td>pickup_census_tract</td></tr>\n",
    "<tr><td>dropoff_census_tract</td><td>payment_type</td><td>company</td></tr>\n",
    "<tr><td>trip_seconds</td><td>dropoff_community_area</td><td>tips</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsYC3O-DnYro"
   },
   "source": [
    "## Install and import packages\n",
    "\n",
    "Install the packages for TensorFlow Data Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATF_g5c2g2Ha"
   },
   "source": [
    "### Upgrade Pip\n",
    "\n",
    "To avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab.  Local systems can of course be upgraded separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:36.854423Z",
     "iopub.status.busy": "2023-07-28T11:06:36.854137Z",
     "iopub.status.idle": "2023-07-28T11:06:36.861881Z",
     "shell.execute_reply": "2023-07-28T11:06:36.861018Z"
    },
    "id": "b0ISmRq3nY3-"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import colab\n",
    "  !pip install --upgrade pip\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qBFH1ARcSNk"
   },
   "source": [
    "### Install Data Validation packages\n",
    "\n",
    "Install the TensorFlow Data Validation packages and dependencies, which takes a few minutes. You may see warnings and errors regarding incompatible dependency versions, which you will resolve in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:36.865303Z",
     "iopub.status.busy": "2023-07-28T11:06:36.865048Z",
     "iopub.status.idle": "2023-07-28T11:06:42.014027Z",
     "shell.execute_reply": "2023-07-28T11:06:42.012921Z"
    },
    "id": "hPJsE5Gkdp8m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing TensorFlow Data Validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_data_validation[visualization]<2\n",
      "  Obtaining dependency information for tensorflow_data_validation[visualization]<2 from https://files.pythonhosted.org/packages/cd/f0/fec7d474d3355c2af22c4f1d0aa5add3d18049eb7db3b23cd8b92ac7a23e/tensorflow_data_validation-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow_data_validation-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting absl-py<2.0.0,>=0.9 (from tensorflow_data_validation[visualization]<2)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting apache-beam[gcp]<3,>=2.47 (from tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for apache-beam[gcp]<3,>=2.47 from https://files.pythonhosted.org/packages/34/5a/58dc76940bb924869781be8f7a107444d90778430bd9d7d8f5e1dc89ef7a/apache_beam-2.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading apache_beam-2.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow_data_validation[visualization]<2) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow_data_validation[visualization]<2) (1.24.2)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from tensorflow_data_validation[visualization]<2) (1.5.3)\n",
      "Collecting protobuf<5,>=3.20.3 (from tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for protobuf<5,>=3.20.3 from https://files.pythonhosted.org/packages/ed/33/f7a5717125d9f699b193fa23904725514b82643d522aa189aba03149ba3b/protobuf-4.24.0-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-4.24.0-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Collecting pyarrow<11,>=10 (from tensorflow_data_validation[visualization]<2)\n",
      "  Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyfarmhash<0.4,>=0.2.2 (from tensorflow_data_validation[visualization]<2)\n",
      "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six<2,>=1.12 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow_data_validation[visualization]<2) (1.16.0)\n",
      "Collecting tensorflow<3,>=2.13 (from tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for tensorflow<3,>=2.13 from https://files.pythonhosted.org/packages/5a/f2/5c2f878c62c8b79c629b11b33516bb55054d7677eba6f56f3a20296b56bd/tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting tensorflow-metadata<1.15,>=1.14.0 (from tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for tensorflow-metadata<1.15,>=1.14.0 from https://files.pythonhosted.org/packages/41/23/3705c7139886c079ef4c0e3be56a5a1fb90e9ee413a4b7caaee0ee0ea6fe/tensorflow_metadata-1.14.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tfx-bsl<1.15,>=1.14.0 (from tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for tfx-bsl<1.15,>=1.14.0 from https://files.pythonhosted.org/packages/da/16/ad1f6777db875a38dea5989c430ce2979484407b8514642cadbe1a720604/tfx_bsl-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tfx_bsl-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting ipython<8,>=7 (from tensorflow_data_validation[visualization]<2)\n",
      "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m793.8/793.8 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting crcmod<2.0,>=1.7 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting orjson<4.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for orjson<4.0 from https://files.pythonhosted.org/packages/c9/49/045ede311c0997692ee3467f09e238ef1b537d1b70a2292447d9df511c16/orjson-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading orjson-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cloudpickle~=2.2.1 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting fastavro<2,>=0.23.6 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for fastavro<2,>=0.23.6 from https://files.pythonhosted.org/packages/43/96/8cecc293becb041c0a6106d9b4cd8295f9eff30f883731204c3ca43813f2/fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting fasteners<1.0,>=0.3 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
      "Collecting grpcio!=1.48.0,<2,>=1.33.1 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for grpcio!=1.48.0,<2,>=1.33.1 from https://files.pythonhosted.org/packages/28/fa/c38a010d3fffcac07ef121abb34eb2c3db0876df74267ce5bde13c3a6ed7/grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading hdfs-2.7.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting httplib2<0.23.0,>=0.8 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting objsize<0.7.0,>=0.6.1 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for pymongo<5.0.0,>=3.8.0 from https://files.pythonhosted.org/packages/3b/92/457200cee53892db496431a08e44fdcf475ee4a6ef14534be492f2f1cb17/pymongo-4.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pymongo-4.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting proto-plus<2,>=1.7.1 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for proto-plus<2,>=1.7.1 from https://files.pythonhosted.org/packages/36/5b/e02636d221917d6fa2a61289b3f16002eb4c93d51c0191ac8e896d527182/proto_plus-1.22.3-py3-none-any.whl.metadata\n",
      "  Downloading proto_plus-1.22.3-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf<5,>=3.20.3 (from tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for protobuf<5,>=3.20.3 from https://files.pythonhosted.org/packages/01/cb/445b3e465abdb8042a41957dc8f60c54620dc7540dbcf9b458a921531ca2/protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Collecting pydot<2,>=1.2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /home/codespace/.local/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in /home/codespace/.local/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2) (2023.3)\n",
      "Collecting regex>=2020.6.8 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for regex>=2020.6.8 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.24.0 in /home/codespace/.local/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2) (4.7.1)\n",
      "Collecting zstandard<1,>=0.18.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6,>=3.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for cachetools<6,>=3.1.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting google-apitools<0.5.32,>=0.5.31 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-auth<3,>=1.18.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-auth<3,>=1.18.0 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-httplib2<0.2.0,>=0.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-cloud-datastore<3,>=2.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-datastore<3,>=2.0.0 from https://files.pythonhosted.org/packages/45/44/e2ee842931fe00d8e7506dcb3206afaa8d32a661e04f09d01931f6f4003f/google_cloud_datastore-2.17.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_datastore-2.17.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting google-cloud-pubsub<3,>=2.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-pubsub<3,>=2.1.0 from https://files.pythonhosted.org/packages/a2/50/3c2993f929985a4fca742ff776878e4f4d3e804b8efd345fdfd054a46e6b/google_cloud_pubsub-2.18.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_pubsub-2.18.2-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting google-cloud-pubsublite<2,>=1.2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-pubsublite<2,>=1.2.0 from https://files.pythonhosted.org/packages/2d/d6/91f605fc67e9d0b4e419c4f291adff15c2f241bf3d70abe8068688599d60/google_cloud_pubsublite-1.8.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_pubsublite-1.8.3-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting google-cloud-bigquery<4,>=2.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-bigquery<4,>=2.0.0 from https://files.pythonhosted.org/packages/cc/6a/d0ef792288f2fa2cfea80899a82de302b3332dfda41984fe114e2cfbf700/google_cloud_bigquery-3.11.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_bigquery-3.11.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting google-cloud-bigquery-storage<3,>=2.6.3 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-bigquery-storage<3,>=2.6.3 from https://files.pythonhosted.org/packages/83/eb/e5016412a9c6ac26b46480f83d58112b99b48c0e38ae040b46f8795dea6f/google_cloud_bigquery_storage-2.22.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_bigquery_storage-2.22.0-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting google-cloud-core<3,>=2.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-core<3,>=2.0.0 from https://files.pythonhosted.org/packages/a2/40/02045f776fdb6e44194f34b6375a26ce8a61bd9bd03cd8930ed91cf51a62/google_cloud_core-2.3.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_core-2.3.3-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting google-cloud-bigtable<3,>=2.19.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-bigtable<3,>=2.19.0 from https://files.pythonhosted.org/packages/f1/2e/fcdd3aed46825c04bc93c0223fff674059e6a789cfda5374948bea1a3f6e/google_cloud_bigtable-2.21.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_bigtable-2.21.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting google-cloud-spanner<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-spanner<4,>=3.0.0 from https://files.pythonhosted.org/packages/e9/a8/31e93da06b9da3e6d5729f8fa5f7c21a47cfd1c19ec7a6f3a9587a19901e/google_cloud_spanner-3.40.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_spanner-3.40.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting google-cloud-dlp<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-dlp<4,>=3.0.0 from https://files.pythonhosted.org/packages/83/21/1d12ac0200d599ac7d2d9ec735851fa51eb365b962638de124247645232f/google_cloud_dlp-3.12.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_dlp-3.12.2-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting google-cloud-language<3,>=2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-language<3,>=2.0 from https://files.pythonhosted.org/packages/0b/28/a137e4b56fd79595b42a5e14576148337265d8218d0787068610a004158b/google_cloud_language-2.11.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_language-2.11.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting google-cloud-videointelligence<3,>=2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-videointelligence<3,>=2.0 from https://files.pythonhosted.org/packages/54/49/463abf7c16f703d40951c21e9d9439702923127ae5acad2c4924dbc52288/google_cloud_videointelligence-2.11.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_videointelligence-2.11.3-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting google-cloud-vision<4,>=2 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-vision<4,>=2 from https://files.pythonhosted.org/packages/14/16/e3ea36b6989e0cff58c2cc38261b9b5133a58d1fb08d64fb85fc552f3466/google_cloud_vision-3.4.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_vision-3.4.4-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-cloud-recommendations-ai<0.11.0,>=0.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-recommendations-ai<0.11.0,>=0.1.0 from https://files.pythonhosted.org/packages/84/c9/14998f5e22c536e06cf452549c6cdfca1c521fe931751d90890c1eb15653/google_cloud_recommendations_ai-0.10.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_recommendations_ai-0.10.4-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting google-cloud-aiplatform<2.0,>=1.26.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-aiplatform<2.0,>=1.26.0 from https://files.pythonhosted.org/packages/a6/bf/e5ce3e5ddd150f92ec4bda3387706ccc4a6f10d71a26e542174ea7090f77/google_cloud_aiplatform-1.30.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_aiplatform-1.30.1-py2.py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/codespace/.local/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]<2) (68.0.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/codespace/.local/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]<2) (0.19.0)\n",
      "Requirement already satisfied: decorator in /home/codespace/.local/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]<2) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /home/codespace/.local/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]<2) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/codespace/.local/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]<2) (5.9.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]<2) (3.0.39)\n",
      "Requirement already satisfied: pygments in /home/codespace/.local/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]<2) (2.15.1)\n",
      "Requirement already satisfied: backcall in /home/codespace/.local/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]<2) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /home/codespace/.local/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]<2) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/codespace/.local/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]<2) (4.8.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for flatbuffers>=23.1.21 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=2.9.0 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/0d/7a/e55589e4093cca1934db5e99644c1c2424a9b3aac104b7f6176605a5eeb7/h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/ea/df/55525e489c43f9dbb6c8ea27d8a567b3dcd18a22f3c45483055f5ca6611d/libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2) (23.1)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.14,>=2.13.0 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting typing-extensions>=3.7.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/6a/48/3cdab86db01701ea043de445f3af1c3e539835781e57d803d2a87f256475/tensorflow_io_gcs_filesystem-0.33.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.33.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata<1.15,>=1.14.0->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for googleapis-common-protos<2,>=1.52.0 from https://files.pythonhosted.org/packages/a7/bc/416a1ffeba4dcd072bc10523dac9ed97f2e7fc4b760580e2bdbdc1e2afdd/googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf<5,>=3.20.3 (from tensorflow_data_validation[visualization]<2)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-api-python-client<2,>=1.7.11 (from tfx-bsl<1.15,>=1.14.0->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-serving-api<3,>=2.13.0 (from tfx-bsl<1.15,>=1.14.0->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for tensorflow-serving-api<3,>=2.13.0 from https://files.pythonhosted.org/packages/71/fe/4004906ca558255673fab41a1bb2189ca6454417487a8608100677c8cc15/tensorflow_serving_api-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_serving_api-2.13.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/python/3.10.8/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2) (0.38.4)\n",
      "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.15,>=1.14.0->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting google-api-core<3dev,>=1.21.0 (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.15,>=1.14.0->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-api-core<3dev,>=1.21.0 from https://files.pythonhosted.org/packages/6e/c4/c3cd048b6cbeba8d9ae50dd7643ac065b85237338aa7501b0efae91eb4d9/google_api_core-2.11.1-py3-none-any.whl.metadata\n",
      "  Downloading google_api_core-2.11.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting oauth2client>=1.4.12 (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting urllib3<2.0 (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for urllib3<2.0 from https://files.pythonhosted.org/packages/c5/05/c214b32d21c0b465506f95c4f28ccbcba15022e000b043b72b3df7728471/urllib3-1.26.16-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-storage<3.0.0dev,>=1.32.0 from https://files.pythonhosted.org/packages/88/14/c9d4faae7ea4bff4405152cbc762b61100aa6949273b4eb3203d23308670/google_cloud_storage-2.10.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_storage-2.10.0-py2.py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for google-cloud-resource-manager<3.0.0dev,>=1.3.3 from https://files.pythonhosted.org/packages/d2/f3/e7d1848edf2f4714d8925e6c749464c190e696b3c85264441c2b3787ae72/google_cloud_resource_manager-1.10.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_resource_manager-1.10.3-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting shapely<2.0.0 (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-resumable-media<3.0dev,>=0.6.0 (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Collecting grpcio-status>=1.33.2 (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/d0/3f/347d93056572fdbd64d4f0fc58a18d420763a7118f8b177437d9dab0ae6f/grpcio_status-1.57.0-py3-none-any.whl.metadata\n",
      "  Downloading grpcio_status-1.57.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting overrides<7.0.0,>=6.0.1 (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading overrides-6.5.0-py3-none-any.whl (17 kB)\n",
      "Collecting sqlparse>=0.4.4 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/codespace/.local/lib/python3.10/site-packages (from httplib2<0.23.0,>=0.8->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2) (3.0.9)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/codespace/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython<8,>=7->tensorflow_data_validation[visualization]<2) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/codespace/.local/lib/python3.10/site-packages (from pexpect>4.3->ipython<8,>=7->tensorflow_data_validation[visualization]<2) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/codespace/.local/lib/python3.10/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow_data_validation[visualization]<2) (0.2.6)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for dnspython<3.0.0,>=1.16.0 from https://files.pythonhosted.org/packages/f6/b4/0a9bee52c50f226a3cbfb54263d02bb421c7f2adc136520729c2c689c1e5/dnspython-2.4.2-py3-none-any.whl.metadata\n",
      "  Downloading dnspython-2.4.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2) (2023.7.22)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/9b/59/a7c32e3d8d0e546a206e0552a2c04444544f15c1da4a01df8938d20c6ffc/werkzeug-2.3.7-py3-none-any.whl.metadata\n",
      "  Downloading werkzeug-2.3.7-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status>=1.33.2 (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/ef/16/3018689d96918e9c4c7407adf96b721df4d6748ba65db82c5eaa63564335/grpcio_status-1.56.2-py3-none-any.whl.metadata\n",
      "  Downloading grpcio_status-1.56.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/2b/21/aaff30111c5941fd9adb5abbf06e04a0e491a685f48ffb291f72ad595ec7/grpcio_status-1.56.0-py3-none-any.whl.metadata\n",
      "  Downloading grpcio_status-1.56.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/b6/c8/0efd0cf1ff62b3acc28619b9ba80a6ead4eb0ee42bb1c3b3841610af98a7/grpcio_status-1.55.3-py3-none-any.whl.metadata\n",
      "  Downloading grpcio_status-1.55.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/b9/ad/51cf59eb26ffa5360ba9dc318580b777e5dda1aa344ad34475103d97e5ac/grpcio_status-1.54.3-py3-none-any.whl.metadata\n",
      "  Downloading grpcio_status-1.54.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/5f/06/19520e0c6ab22fe09a0c8cfeabdf63b1ce6ea5d6ac88a65a2fa57b080976/grpcio_status-1.54.2-py3-none-any.whl.metadata\n",
      "  Downloading grpcio_status-1.54.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.54.0-py3-none-any.whl (5.1 kB)\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/fb/c0/8ee53675cb1aecaa7fcbc9bea6575cb8c19a21dd0ce80fe6e5edb04426f4/grpcio_status-1.53.2-py3-none-any.whl.metadata\n",
      "  Downloading grpcio_status-1.53.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Obtaining dependency information for grpcio-status>=1.33.2 from https://files.pythonhosted.org/packages/61/14/6a4476403abaf954c0b5715de9cfdb7528143c5ac372316fa95704ae8551/grpcio_status-1.53.1-py3-none-any.whl.metadata\n",
      "  Downloading grpcio_status-1.53.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.53.0-py3-none-any.whl (5.1 kB)\n",
      "  Downloading grpcio_status-1.51.3-py3-none-any.whl (5.1 kB)\n",
      "  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Collecting pyasn1>=0.1.7 (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2) (2.1.3)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<3,>=2.13->tensorflow_data_validation[visualization]<2)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "Downloading tfx_bsl-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_data_validation-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_aiplatform-1.30.1-py2.py3-none-any.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_bigquery-3.11.4-py2.py3-none-any.whl (219 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.6/219.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_bigquery_storage-2.22.0-py2.py3-none-any.whl (190 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_bigtable-2.21.0-py2.py3-none-any.whl (293 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.0/293.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_cloud_datastore-2.17.0-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_dlp-3.12.2-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_language-2.11.0-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_pubsub-2.18.2-py2.py3-none-any.whl (265 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.9/265.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_pubsublite-1.8.3-py2.py3-none-any.whl (288 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_recommendations_ai-0.10.4-py2.py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.3/173.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_spanner-3.40.0-py2.py3-none-any.whl (332 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.9/332.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_videointelligence-2.11.3-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.4/229.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_vision-3.4.4-py2.py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.3/140.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading proto_plus-1.22.3-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pymongo-4.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (603 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m603.6/603.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.33.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_serving_api-2.13.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading apache_beam-2.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.5/120.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_resource_manager-1.10.3-py2.py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.0/321.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyfarmhash, crcmod, dill, google-apitools, hdfs, docopt\n",
      "  Building wheel for pyfarmhash (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp310-cp310-linux_x86_64.whl size=102044 sha256=af74e08deeba79613ce4a3b09f435760ace6c15005055ccaa3d61605e3ce89da\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/e0/08/da/f66b1f3258fe3f1e767b2136c5444dbfa9fa3f7944cc5e1983\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=37119 sha256=584c729df9d9fa85357cc975dc92e0574c868e66cbe51e79124217175e9c0247\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=24dcfbd01cc1ec173b11b7f60a6bdcbdbe761950dbb8b1c35d219e0b7bbbe3f0\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
      "  Building wheel for google-apitools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131022 sha256=8440b8f03434a26626e7b7b785cf91b2384e9bfa466ae10e7d07542feb0f630b\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/04/b7/e0/9712f8c23a5da3d9d16fb88216b897bf60e85b12f5470f26ee\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.7.2-py3-none-any.whl size=34171 sha256=a8a344a34b74e982c5c391851fccc18ad9573e917a00d51a52c3db3fd981117a\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/ab/39/8e/e1905de9af8ae74911cd3e53e721995cd230816f63776e5825\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=4633ebec02f22518fe0eae2ba0a41bd62eb319627e2c3e8f054426ed30394f17\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built pyfarmhash crcmod dill google-apitools hdfs docopt\n",
      "Installing collected packages: pyfarmhash, libclang, flatbuffers, docopt, crcmod, zstandard, wrapt, werkzeug, urllib3, uritemplate, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, sqlparse, shapely, regex, pydot, pyasn1, pyarrow, protobuf, overrides, orjson, opt-einsum, objsize, oauthlib, markdown, keras, httplib2, h5py, grpcio, google-pasta, google-crc32c, gast, fasteners, fastavro, dnspython, dill, cloudpickle, cachetools, astunparse, absl-py, rsa, pymongo, pyasn1-modules, proto-plus, ipython, googleapis-common-protos, google-resumable-media, tensorflow-metadata, requests-oauthlib, oauth2client, hdfs, grpcio-status, google-auth, grpc-google-iam-v1, google-auth-oauthlib, google-auth-httplib2, google-apitools, google-api-core, apache-beam, tensorboard, google-cloud-core, google-api-python-client, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-storage, google-cloud-spanner, google-cloud-resource-manager, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-cloud-bigquery-storage, google-cloud-bigquery, tensorflow-serving-api, google-cloud-pubsublite, google-cloud-aiplatform, tfx-bsl, tensorflow_data_validation\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.4\n",
      "    Uninstalling urllib3-2.0.4:\n",
      "      Successfully uninstalled urllib3-2.0.4\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "\u001b[33m  WARNING: The script sqlformat is installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script plasma_store is installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: overrides\n",
      "    Found existing installation: overrides 7.3.1\n",
      "    Uninstalling overrides-7.3.1:\n",
      "      Successfully uninstalled overrides-7.3.1\n",
      "\u001b[33m  WARNING: The script markdown_py is installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script fastavro is installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.14.0\n",
      "    Uninstalling ipython-8.14.0:\n",
      "      Successfully uninstalled ipython-8.14.0\n",
      "\u001b[33m  WARNING: The scripts iptest, iptest3, ipython and ipython3 are installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts hdfscli and hdfscli-avro are installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script gen_client is installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tensorboard is installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 apache-beam-2.49.0 astunparse-1.6.3 cachetools-5.3.1 cloudpickle-2.2.1 crcmod-1.7 dill-0.3.1.1 dnspython-2.4.2 docopt-0.6.2 fastavro-1.8.2 fasteners-0.18 flatbuffers-23.5.26 gast-0.4.0 google-api-core-2.11.1 google-api-python-client-1.12.11 google-apitools-0.5.31 google-auth-2.22.0 google-auth-httplib2-0.1.0 google-auth-oauthlib-1.0.0 google-cloud-aiplatform-1.30.1 google-cloud-bigquery-3.11.4 google-cloud-bigquery-storage-2.22.0 google-cloud-bigtable-2.21.0 google-cloud-core-2.3.3 google-cloud-datastore-2.17.0 google-cloud-dlp-3.12.2 google-cloud-language-2.11.0 google-cloud-pubsub-2.18.2 google-cloud-pubsublite-1.8.3 google-cloud-recommendations-ai-0.10.4 google-cloud-resource-manager-1.10.3 google-cloud-spanner-3.40.0 google-cloud-storage-2.10.0 google-cloud-videointelligence-2.11.3 google-cloud-vision-3.4.4 google-crc32c-1.5.0 google-pasta-0.2.0 google-resumable-media-2.5.0 googleapis-common-protos-1.60.0 grpc-google-iam-v1-0.12.6 grpcio-1.57.0 grpcio-status-1.48.2 h5py-3.9.0 hdfs-2.7.2 httplib2-0.22.0 ipython-7.34.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.4 oauth2client-4.1.3 oauthlib-3.2.2 objsize-0.6.1 opt-einsum-3.3.0 orjson-3.9.4 overrides-6.5.0 proto-plus-1.22.3 protobuf-3.20.3 pyarrow-10.0.1 pyasn1-0.5.0 pyasn1-modules-0.3.0 pydot-1.4.2 pyfarmhash-0.3.2 pymongo-4.4.1 regex-2023.8.8 requests-oauthlib-1.3.1 rsa-4.9 shapely-1.8.5.post1 sqlparse-0.4.4 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.33.0 tensorflow-metadata-1.14.0 tensorflow-serving-api-2.13.0 tensorflow_data_validation-1.14.0 termcolor-2.3.0 tfx-bsl-1.14.0 typing-extensions-4.5.0 uritemplate-3.0.1 urllib3-1.26.16 werkzeug-2.3.7 wrapt-1.15.0 zstandard-0.21.0\n"
     ]
    }
   ],
   "source": [
    "print('Installing TensorFlow Data Validation')\n",
    "!pip install --upgrade 'tensorflow_data_validation[visualization]<2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_NXX5GaSiZx"
   },
   "source": [
    "### Import TensorFlow and reload updated packages\n",
    "\n",
    "The prior step updates the default packages in the Gooogle Colab environment, so you must reload the package resources to resolve the new dependencies.\n",
    "\n",
    "Note: This step resolves the dependency error from the installation. If you are still experiencing code execution problems after running this code, restart the runtime (Runtime > Restart runtime ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:42.019455Z",
     "iopub.status.busy": "2023-07-28T11:06:42.019122Z",
     "iopub.status.idle": "2023-07-28T11:06:42.103592Z",
     "shell.execute_reply": "2023-07-28T11:06:42.102730Z"
    },
    "id": "E2j9VD9HbGWw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pkg_resources' from '/home/codespace/.local/lib/python3.10/site-packages/pkg_resources/__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import importlib\n",
    "importlib.reload(pkg_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFXK2AdpSpv0"
   },
   "source": [
    "Check the versions of TensorFlow and the Data Validation before proceeding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:42.107297Z",
     "iopub.status.busy": "2023-07-28T11:06:42.107021Z",
     "iopub.status.idle": "2023-07-28T11:06:45.760295Z",
     "shell.execute_reply": "2023-07-28T11:06:45.759237Z"
    },
    "id": "F5rPatTDSCHB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 10:09:14.851073: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-16 10:09:14.880942: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-16 10:09:14.881618: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-16 10:09:16.817057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_lzma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/python/3.10.8/lib/python3.10/site-packages/fastavro/read.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _read\n\u001b[1;32m      3\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[0;32mfastavro/_read.pyx:11\u001b[0m, in \u001b[0;36minit fastavro._read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/python/3.10.8/lib/python3.10/lzma.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m_lzma\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m_lzma\u001b[39;00m \u001b[39mimport\u001b[39;00m _encode_filter_properties, _decode_filter_properties\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_lzma'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_data_validation\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfdv\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTF version:\u001b[39m\u001b[39m'\u001b[39m, tf\u001b[39m.\u001b[39m__version__)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTFDV version:\u001b[39m\u001b[39m'\u001b[39m, tfdv\u001b[39m.\u001b[39mversion\u001b[39m.\u001b[39m__version__)\n",
      "File \u001b[0;32m/usr/local/python/3.10.8/lib/python3.10/site-packages/tensorflow_data_validation/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Init module for TensorFlow Data Validation.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# Import stats API.\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_data_validation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats_api\u001b[39;00m \u001b[39mimport\u001b[39;00m default_sharded_output_suffix\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_data_validation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats_api\u001b[39;00m \u001b[39mimport\u001b[39;00m default_sharded_output_supported\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_data_validation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats_api\u001b[39;00m \u001b[39mimport\u001b[39;00m GenerateStatistics\n",
      "File \u001b[0;32m/usr/local/python/3.10.8/lib/python3.10/site-packages/tensorflow_data_validation/api/stats_api.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Generator, Text, Optional\n\u001b[0;32m---> 49\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mapache_beam\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbeam\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyarrow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpa\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_data_validation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m artifacts_io_impl\n",
      "File \u001b[0;32m/usr/local/python/3.10.8/lib/python3.10/site-packages/apache_beam/__init__.py:87\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39m# pylint: disable=wrong-import-position\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mapache_beam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpickler\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mapache_beam\u001b[39;00m \u001b[39mimport\u001b[39;00m coders\n\u001b[1;32m     88\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mapache_beam\u001b[39;00m \u001b[39mimport\u001b[39;00m io\n\u001b[1;32m     89\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mapache_beam\u001b[39;00m \u001b[39mimport\u001b[39;00m metrics\n",
      "File \u001b[0;32m/usr/local/python/3.10.8/lib/python3.10/site-packages/apache_beam/coders/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Licensed to the Apache Software Foundation (ASF) under one or more\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# contributor license agreements.  See the NOTICE file distributed with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mapache_beam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcoders\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcoders\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mapache_beam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcoders\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrow_coder\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mapache_beam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcoders\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypecoders\u001b[39;00m \u001b[39mimport\u001b[39;00m registry\n",
      "File \u001b[0;32m/usr/local/python/3.10.8/lib/python3.10/site-packages/apache_beam/coders/coders.py:59\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers_pb2\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mproto\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mapache_beam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcoders\u001b[39;00m \u001b[39mimport\u001b[39;00m coder_impl\n\u001b[1;32m     60\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mapache_beam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcoders\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mavro_record\u001b[39;00m \u001b[39mimport\u001b[39;00m AvroRecord\n\u001b[1;32m     61\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mapache_beam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mportability\u001b[39;00m \u001b[39mimport\u001b[39;00m common_urns\n",
      "File \u001b[0;32m/usr/local/python/3.10.8/lib/python3.10/site-packages/apache_beam/coders/coder_impl.py:57\u001b[0m, in \u001b[0;36minit apache_beam.coders.coder_impl\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/python/3.10.8/lib/python3.10/site-packages/fastavro/__init__.py:47\u001b[0m\n\u001b[1;32m     43\u001b[0m __version_info__ \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     44\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m __version_info__\n\u001b[0;32m---> 47\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfastavro\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mread\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfastavro\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrite\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfastavro\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.10.8/lib/python3.10/site-packages/fastavro/read.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _read\n\u001b[1;32m      3\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _read_py \u001b[39mas\u001b[39;00m _read  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m json_read\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m logical_readers\n",
      "File \u001b[0;32m/usr/local/python/3.10.8/lib/python3.10/site-packages/fastavro/_read_py.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbz2\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlzma\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mzlib\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime, timezone\n",
      "File \u001b[0;32m/usr/local/python/3.10.8/lib/python3.10/lzma.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mio\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m_lzma\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m_lzma\u001b[39;00m \u001b[39mimport\u001b[39;00m _encode_filter_properties, _decode_filter_properties\n\u001b[1;32m     29\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39m_compression\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_lzma'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "print('TF version:', tf.__version__)\n",
    "print('TFDV version:', tfdv.version.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MizoHg1DRlK"
   },
   "source": [
    "## Load the dataset\n",
    "We will download our dataset from Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:45.764303Z",
     "iopub.status.busy": "2023-07-28T11:06:45.763804Z",
     "iopub.status.idle": "2023-07-28T11:06:46.039565Z",
     "shell.execute_reply": "2023-07-28T11:06:46.038372Z"
    },
    "id": "x5gfFiTeDa6Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what we downloaded:\n",
      "/tmpfs/tmp/tmpsjp7pkfy/data:\r\n",
      "eval  serving  train\r\n",
      "\r\n",
      "/tmpfs/tmp/tmpsjp7pkfy/data/eval:\r\n",
      "data.csv\r\n",
      "\r\n",
      "/tmpfs/tmp/tmpsjp7pkfy/data/serving:\r\n",
      "data.csv\r\n",
      "\r\n",
      "/tmpfs/tmp/tmpsjp7pkfy/data/train:\r\n",
      "data.csv\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile, urllib, zipfile\n",
    "\n",
    "# Set up some globals for our file paths\n",
    "BASE_DIR = tempfile.mkdtemp()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'chicago_taxi_output')\n",
    "TRAIN_DATA = os.path.join(DATA_DIR, 'train', 'data.csv')\n",
    "EVAL_DATA = os.path.join(DATA_DIR, 'eval', 'data.csv')\n",
    "SERVING_DATA = os.path.join(DATA_DIR, 'serving', 'data.csv')\n",
    "\n",
    "# Download the zip file from GCP and unzip it\n",
    "zip, headers = urllib.request.urlretrieve('https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/chicago_data.zip')\n",
    "zipfile.ZipFile(zip).extractall(BASE_DIR)\n",
    "zipfile.ZipFile(zip).close()\n",
    "\n",
    "print(\"Here's what we downloaded:\")\n",
    "!ls -R {os.path.join(BASE_DIR, 'data')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0sFmiTbT8-x"
   },
   "source": [
    "## Compute and visualize statistics\n",
    "\n",
    "First we'll use [`tfdv.generate_statistics_from_csv`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_csv) to compute statistics for our training data. (ignore the snappy warnings)\n",
    "\n",
    "TFDV can compute descriptive [statistics](https://github.com/tensorflow/metadata/blob/v0.6.0/tensorflow_metadata/proto/v0/statistics.proto) that provide a quick overview of the data in terms of the features that are present and the shapes of their value distributions.\n",
    "\n",
    "Internally, TFDV uses [Apache Beam](https://beam.apache.org/)'s data-parallel processing framework to scale the computation of statistics over large datasets. For applications that wish to integrate deeper with TFDV (e.g., attach statistics generation at the end of a data-generation pipeline), the API also exposes a Beam PTransform for statistics generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:46.044027Z",
     "iopub.status.busy": "2023-07-28T11:06:46.043665Z",
     "iopub.status.idle": "2023-07-28T11:06:48.998993Z",
     "shell.execute_reply": "2023-07-28T11:06:48.998141Z"
    },
    "id": "EE481oMbT-H0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_data_validation/utils/artifacts_io_impl.py:93: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_data_validation/utils/artifacts_io_impl.py:93: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    }
   ],
   "source": [
    "train_stats = tfdv.generate_statistics_from_csv(data_location=TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhXQSxJ2dB_6"
   },
   "source": [
    "Now let's use [`tfdv.visualize_statistics`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics), which uses [Facets](https://pair-code.github.io/facets/) to create a succinct visualization of our training data:\n",
    "\n",
    "* Notice that numeric features and catagorical features are visualized separately, and that charts are displayed showing the distributions for each feature.\n",
    "* Notice that features with missing or zero values display a percentage in red as a visual indicator that there may be issues with examples in those features.  The percentage is the percentage of examples that have missing or zero values for that feature.\n",
    "* Notice that there are no examples with values for `pickup_census_tract`.  This is an opportunity for dimensionality reduction!\n",
    "* Try clicking \"expand\" above the charts to change the display\n",
    "* Try hovering over bars in the charts to display bucket ranges and counts\n",
    "* Try switching between the log and linear scales, and notice how the log scale reveals much more detail about the `payment_type` categorical feature\n",
    "* Try selecting \"quantiles\" from the \"Chart to show\" menu, and hover over the markers to show the quantile percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3tUKgh7Up3x"
   },
   "outputs": [],
   "source": [
    "# docs-infra: no-execute\n",
    "tfdv.visualize_statistics(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xoc0ijE5LYeQ"
   },
   "source": [
    "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/statistics.png\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVR02-y4V0uM"
   },
   "source": [
    "## Infer a schema\n",
    "\n",
    "Now let's use [`tfdv.infer_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema) to create a schema for our data.  A schema defines constraints for the data that are relevant for ML. Example constraints include the data type of each feature, whether it's numerical or categorical, or the frequency of its presence in the data.  For categorical features the schema also defines the domain - the list of acceptable values.  Since writing a schema can be a tedious task, especially for datasets with lots of features, TFDV provides a method to generate an initial version of the schema based on the descriptive statistics.\n",
    "\n",
    "Getting the schema right is important because the rest of our production pipeline will be relying on the schema that TFDV generates to be correct.  The schema also provides documentation for the data, and so is useful when different developers work on the same data.  Let's use [`tfdv.display_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/display_schema) to display the inferred schema so that we can review it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:49.003438Z",
     "iopub.status.busy": "2023-07-28T11:06:49.003074Z",
     "iopub.status.idle": "2023-07-28T11:06:49.024390Z",
     "shell.execute_reply": "2023-07-28T11:06:49.023502Z"
    },
    "id": "6LLkRJThVr9m"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Presence</th>\n",
       "      <th>Valency</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'pickup_community_area'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'fare'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'trip_start_month'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'trip_start_hour'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'trip_start_day'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'trip_start_timestamp'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'pickup_latitude'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'pickup_longitude'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'dropoff_latitude'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'dropoff_longitude'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'trip_miles'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'pickup_census_tract'</th>\n",
       "      <td>BYTES</td>\n",
       "      <td>optional</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'dropoff_census_tract'</th>\n",
       "      <td>INT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'payment_type'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>'payment_type'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'company'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>'company'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'trip_seconds'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'dropoff_community_area'</th>\n",
       "      <td>INT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'tips'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Type  Presence Valency          Domain\n",
       "Feature name                                                      \n",
       "'pickup_community_area'      INT  required                       -\n",
       "'fare'                     FLOAT  required                       -\n",
       "'trip_start_month'           INT  required                       -\n",
       "'trip_start_hour'            INT  required                       -\n",
       "'trip_start_day'             INT  required                       -\n",
       "'trip_start_timestamp'       INT  required                       -\n",
       "'pickup_latitude'          FLOAT  required                       -\n",
       "'pickup_longitude'         FLOAT  required                       -\n",
       "'dropoff_latitude'         FLOAT  optional  single               -\n",
       "'dropoff_longitude'        FLOAT  optional  single               -\n",
       "'trip_miles'               FLOAT  required                       -\n",
       "'pickup_census_tract'      BYTES  optional                       -\n",
       "'dropoff_census_tract'       INT  optional  single               -\n",
       "'payment_type'            STRING  required          'payment_type'\n",
       "'company'                 STRING  optional  single       'company'\n",
       "'trip_seconds'               INT  required                       -\n",
       "'dropoff_community_area'     INT  optional  single               -\n",
       "'tips'                     FLOAT  required                       -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'payment_type'</th>\n",
       "      <td>'Cash', 'Credit Card', 'Dispute', 'No Charge', 'Pcard', 'Unknown'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'company'</th>\n",
       "      <td>'0118 - 42111 Godfrey S.Awir', '0694 - 59280 Chinesco Trans Inc', '1085 - 72312 N and W Cab Co', '2733 - 74600 Benny Jona', '2809 - 95474 C &amp; D Cab Co Inc.', '3011 - 66308 JBL Cab Inc.', '3152 - 97284 Crystal Abernathy', '3201 - C&amp;D Cab Co Inc', '3201 - CID Cab Co Inc', '3253 - 91138 Gaither Cab Co.', '3385 - 23210 Eman Cab', '3623 - 72222 Arrington Enterprises', '3897 - Ilie Malec', '4053 - Adwar H. Nikola', '4197 - 41842 Royal Star', '4615 - 83503 Tyrone Henderson', '4615 - Tyrone Henderson', '4623 - Jay Kim', '5006 - 39261 Salifu Bawa', '5006 - Salifu Bawa', '5074 - 54002 Ahzmi Inc', '5074 - Ahzmi Inc', '5129 - 87128', '5129 - 98755 Mengisti Taxi', '5129 - Mengisti Taxi', '5724 - KYVI Cab Inc', '585 - Valley Cab Co', '5864 - 73614 Thomas Owusu', '5864 - Thomas Owusu', '5874 - 73628 Sergey Cab Corp.', '5997 - 65283 AW Services Inc.', '5997 - AW Services Inc.', '6488 - 83287 Zuha Taxi', '6743 - Luhak Corp', 'Blue Ribbon Taxi Association Inc.', 'C &amp; D Cab Co Inc', 'Chicago Elite Cab Corp.', 'Chicago Elite Cab Corp. (Chicago Carriag', 'Chicago Medallion Leasing INC', 'Chicago Medallion Management', 'Choice Taxi Association', 'Dispatch Taxi Affiliation', 'KOAM Taxi Association', 'Northwest Management LLC', 'Taxi Affiliation Services', 'Top Cab Affiliation'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Values\n",
       "Domain                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "'payment_type'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           'Cash', 'Credit Card', 'Dispute', 'No Charge', 'Pcard', 'Unknown'\n",
       "'company'       '0118 - 42111 Godfrey S.Awir', '0694 - 59280 Chinesco Trans Inc', '1085 - 72312 N and W Cab Co', '2733 - 74600 Benny Jona', '2809 - 95474 C & D Cab Co Inc.', '3011 - 66308 JBL Cab Inc.', '3152 - 97284 Crystal Abernathy', '3201 - C&D Cab Co Inc', '3201 - CID Cab Co Inc', '3253 - 91138 Gaither Cab Co.', '3385 - 23210 Eman Cab', '3623 - 72222 Arrington Enterprises', '3897 - Ilie Malec', '4053 - Adwar H. Nikola', '4197 - 41842 Royal Star', '4615 - 83503 Tyrone Henderson', '4615 - Tyrone Henderson', '4623 - Jay Kim', '5006 - 39261 Salifu Bawa', '5006 - Salifu Bawa', '5074 - 54002 Ahzmi Inc', '5074 - Ahzmi Inc', '5129 - 87128', '5129 - 98755 Mengisti Taxi', '5129 - Mengisti Taxi', '5724 - KYVI Cab Inc', '585 - Valley Cab Co', '5864 - 73614 Thomas Owusu', '5864 - Thomas Owusu', '5874 - 73628 Sergey Cab Corp.', '5997 - 65283 AW Services Inc.', '5997 - AW Services Inc.', '6488 - 83287 Zuha Taxi', '6743 - Luhak Corp', 'Blue Ribbon Taxi Association Inc.', 'C & D Cab Co Inc', 'Chicago Elite Cab Corp.', 'Chicago Elite Cab Corp. (Chicago Carriag', 'Chicago Medallion Leasing INC', 'Chicago Medallion Management', 'Choice Taxi Association', 'Dispatch Taxi Affiliation', 'KOAM Taxi Association', 'Northwest Management LLC', 'Taxi Affiliation Services', 'Top Cab Affiliation'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "schema = tfdv.infer_schema(statistics=train_stats)\n",
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVa3EXE8WEDE"
   },
   "source": [
    "## Check evaluation data for errors\n",
    "\n",
    "So far we've only been looking at the training data.  It's important that our evaluation data is consistent with our training data, including that it uses the same schema.  It's also important that the evaluation data includes examples of roughly the same ranges of values for our numerical features as our training data, so that our coverage of the loss surface during evaluation is roughly the same as during training.  The same is true for categorical features.  Otherwise, we may have training issues that are not identified during evaluation, because we didn't evaluate part of our loss surface.\n",
    "\n",
    "* Notice that each feature now includes statistics for both the training and evaluation datasets.\n",
    "* Notice that the charts now have both the training and evaluation datasets overlaid, making it easy to compare them.\n",
    "* Notice that the charts now include a percentages view, which can be combined with log or the default linear scales.\n",
    "* Notice that the mean and median for `trip_miles` are different for the training versus the evaluation datasets.  Will that cause problems?\n",
    "* Wow, the max `tips` is very different for the training versus the evaluation datasets.  Will that cause problems?\n",
    "* Click expand on the Numeric Features chart, and select the log scale.  Review the `trip_seconds` feature, and notice the difference in the max.  Will evaluation miss parts of the loss surface?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:49.028141Z",
     "iopub.status.busy": "2023-07-28T11:06:49.027894Z",
     "iopub.status.idle": "2023-07-28T11:06:51.667049Z",
     "shell.execute_reply": "2023-07-28T11:06:51.665979Z"
    },
    "id": "j_P0RLYlV6XG"
   },
   "outputs": [],
   "source": [
    "# Compute stats for evaluation data\n",
    "eval_stats = tfdv.generate_statistics_from_csv(data_location=EVAL_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qn-3fQWJLimn"
   },
   "outputs": [],
   "source": [
    "# docs-infra: no-execute\n",
    "# Compare evaluation data with training data\n",
    "tfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,\n",
    "                          lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MS4u82lzLeRh"
   },
   "source": [
    "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/statistics_eval.png\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycRRa4leHp84"
   },
   "source": [
    "## Check for evaluation anomalies\n",
    "\n",
    "Does our evaluation dataset match the schema from our training dataset?  This is especially important for categorical features, where we want to identify the range of acceptable values.\n",
    "\n",
    "Key Point: What would happen if we tried to evaluate using data with categorical feature values that were not in our training dataset?  What about numeric features that are outside the ranges in our training dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:51.671951Z",
     "iopub.status.busy": "2023-07-28T11:06:51.671619Z",
     "iopub.status.idle": "2023-07-28T11:06:51.684923Z",
     "shell.execute_reply": "2023-07-28T11:06:51.684099Z"
    },
    "id": "T7uGVeL2WOam"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly short description</th>\n",
       "      <th>Anomaly long description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'payment_type'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: Prcard (&lt;1%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'company'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: 2092 - 61288 Sbeih company (&lt;1%), 2192 - 73487 Zeymane Corp (&lt;1%), 2192 - Zeymane Corp (&lt;1%), 2823 - 73307 Seung Lee (&lt;1%), 3094 - 24059 G.L.B. Cab Co (&lt;1%), 3319 - CD Cab Co (&lt;1%), 3385 - Eman Cab (&lt;1%), 3897 - 57856 Ilie Malec (&lt;1%), 4053 - 40193 Adwar H. Nikola (&lt;1%), 4197 - Royal Star (&lt;1%), 585 - 88805 Valley Cab Co (&lt;1%), 5874 - Sergey Cab Corp. (&lt;1%), 6057 - 24657 Richard Addo (&lt;1%), 6574 - Babylon Express Inc. (&lt;1%), 6742 - 83735 Tasha ride inc (&lt;1%).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Anomaly short description  \\\n",
       "Feature name                               \n",
       "'payment_type'  Unexpected string values   \n",
       "'company'       Unexpected string values   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Anomaly long description  \n",
       "Feature name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "'payment_type'                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Examples contain values missing from the schema: Prcard (<1%).   \n",
       "'company'       Examples contain values missing from the schema: 2092 - 61288 Sbeih company (<1%), 2192 - 73487 Zeymane Corp (<1%), 2192 - Zeymane Corp (<1%), 2823 - 73307 Seung Lee (<1%), 3094 - 24059 G.L.B. Cab Co (<1%), 3319 - CD Cab Co (<1%), 3385 - Eman Cab (<1%), 3897 - 57856 Ilie Malec (<1%), 4053 - 40193 Adwar H. Nikola (<1%), 4197 - Royal Star (<1%), 585 - 88805 Valley Cab Co (<1%), 5874 - Sergey Cab Corp. (<1%), 6057 - 24657 Richard Addo (<1%), 6574 - Babylon Express Inc. (<1%), 6742 - 83735 Tasha ride inc (<1%).   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check eval data for errors by validating the eval data stats using the previously inferred schema.\n",
    "anomalies = tfdv.validate_statistics(statistics=eval_stats, schema=schema)\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzxx1gBpJIBa"
   },
   "source": [
    "## Fix evaluation anomalies in the schema\n",
    "\n",
    "Oops!  It looks like we have some new values for `company` in our evaluation data, that we didn't have in our training data.  We also have a new value for `payment_type`.  These should be considered anomalies, but what we decide to do about them depends on our domain knowledge of the data.  If an anomaly truly indicates a data error, then the underlying data should be fixed.  Otherwise, we can simply update the schema to include the values in the eval dataset.\n",
    "\n",
    "Key Point: How would our evaluation results be affected if we did not fix these problems?\n",
    "\n",
    "Unless we change our evaluation dataset we can't fix everything, but we can fix things in the schema that we're comfortable accepting.  That includes relaxing our view of what is and what is not an anomaly for particular features, as well as updating our schema to include missing values for categorical features.  TFDV has enabled us to discover what we need to fix.\n",
    "\n",
    "Let's make those fixes now, and then review one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:51.688836Z",
     "iopub.status.busy": "2023-07-28T11:06:51.688558Z",
     "iopub.status.idle": "2023-07-28T11:06:51.699542Z",
     "shell.execute_reply": "2023-07-28T11:06:51.698675Z"
    },
    "id": "legN2nXLWZAc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4 style=\"color:green;\">No anomalies found.</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Relax the minimum fraction of values that must come from the domain for feature company.\n",
    "company = tfdv.get_feature(schema, 'company')\n",
    "company.distribution_constraints.min_domain_mass = 0.9\n",
    "\n",
    "# Add new value to the domain of feature payment_type.\n",
    "payment_type_domain = tfdv.get_domain(schema, 'payment_type')\n",
    "payment_type_domain.value.append('Prcard')\n",
    "\n",
    "# Validate eval stats after updating the schema \n",
    "updated_anomalies = tfdv.validate_statistics(eval_stats, schema)\n",
    "tfdv.display_anomalies(updated_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNo72YP9LN98"
   },
   "source": [
    "Hey, look at that!  We verified that the training and evaluation data are now consistent!  Thanks TFDV ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZ1P4ucHJj5o"
   },
   "source": [
    "## Schema Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb179jczJppA"
   },
   "source": [
    "We also split off a 'serving' dataset for this example, so we should check that too.  By default all datasets in a pipeline should use the same schema, but there are often exceptions. For example, in supervised learning we need to include labels in our dataset, but when we serve the model for inference the labels will not be included. In some cases introducing slight schema variations is necessary.\n",
    "\n",
    "**Environments** can be used to express such requirements. In particular, features in schema can be associated with a set of environments using `default_environment`, `in_environment` and `not_in_environment`.\n",
    "\n",
    "For example, in this dataset the `tips` feature is included as the label for training, but it's missing in the serving data. Without environment specified, it will show up as an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:51.703540Z",
     "iopub.status.busy": "2023-07-28T11:06:51.703234Z",
     "iopub.status.idle": "2023-07-28T11:06:53.501068Z",
     "shell.execute_reply": "2023-07-28T11:06:53.500073Z"
    },
    "id": "wSZfbnifJuTA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly short description</th>\n",
       "      <th>Anomaly long description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'tips'</th>\n",
       "      <td>Column dropped</td>\n",
       "      <td>Column is completely missing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Anomaly short description      Anomaly long description\n",
       "Feature name                                                        \n",
       "'tips'                  Column dropped  Column is completely missing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "serving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA)\n",
    "serving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n",
    "\n",
    "tfdv.display_anomalies(serving_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDYHvZ09LfkT"
   },
   "source": [
    "We'll deal with the `tips` feature below.  We also have an INT value in our trip seconds, where our schema expected a FLOAT. By making us aware of that difference, TFDV helps uncover inconsistencies in the way the data is generated for training and serving. It's very easy to be unaware of problems like that until model performance suffers, sometimes catastrophically. It may or may not be a significant issue, but in any case this should be cause for further investigation.\n",
    "\n",
    "In this case, we can safely convert INT values to FLOATs, so we want to tell TFDV to use our schema to infer the type.  Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:53.505558Z",
     "iopub.status.busy": "2023-07-28T11:06:53.505269Z",
     "iopub.status.idle": "2023-07-28T11:06:55.151467Z",
     "shell.execute_reply": "2023-07-28T11:06:55.150530Z"
    },
    "id": "OhtYF8aAczpd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly short description</th>\n",
       "      <th>Anomaly long description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'tips'</th>\n",
       "      <td>Column dropped</td>\n",
       "      <td>Column is completely missing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Anomaly short description      Anomaly long description\n",
       "Feature name                                                        \n",
       "'tips'                  Column dropped  Column is completely missing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\n",
    "serving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA, stats_options=options)\n",
    "serving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n",
    "\n",
    "tfdv.display_anomalies(serving_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJjh5rigc5xy"
   },
   "source": [
    "Now we just have the `tips` feature (which is our label) showing up as an anomaly ('Column dropped').  Of course we don't expect to have labels in our serving data, so let's tell TFDV to ignore that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:55.155576Z",
     "iopub.status.busy": "2023-07-28T11:06:55.155261Z",
     "iopub.status.idle": "2023-07-28T11:06:55.165185Z",
     "shell.execute_reply": "2023-07-28T11:06:55.164367Z"
    },
    "id": "bnbnw8H6Lp2M"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4 style=\"color:green;\">No anomalies found.</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# All features are by default in both TRAINING and SERVING environments.\n",
    "schema.default_environment.append('TRAINING')\n",
    "schema.default_environment.append('SERVING')\n",
    "\n",
    "# Specify that 'tips' feature is not in SERVING environment.\n",
    "tfdv.get_feature(schema, 'tips').not_in_environment.append('SERVING')\n",
    "\n",
    "serving_anomalies_with_env = tfdv.validate_statistics(\n",
    "    serving_stats, schema, environment='SERVING')\n",
    "\n",
    "tfdv.display_anomalies(serving_anomalies_with_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yteMr3AGMYEp"
   },
   "source": [
    "## Check for drift and skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ftd5k6AMkPV"
   },
   "source": [
    "In addition to checking whether a dataset conforms to the expectations set in the schema, TFDV also provides functionalities to detect drift and skew.  TFDV performs this check by comparing the statistics of the different datasets based on the drift/skew comparators specified in the schema.\n",
    "\n",
    "### Drift\n",
    "\n",
    "Drift detection is supported for categorical features and between consecutive spans of data (i.e., between span N and span N+1), such as between different days of training data.  We express drift in terms of [L-infinity distance](https://en.wikipedia.org/wiki/Chebyshev_distance), and you can set the threshold distance so that you receive warnings when the drift is higher than is acceptable.  Setting the correct distance is typically an iterative process requiring domain knowledge and experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBFuLpXb6qSp"
   },
   "source": [
    "### Skew\n",
    "\n",
    "TFDV can detect three different kinds of skew in your data - schema skew, feature skew, and distribution skew.\n",
    "\n",
    "#### Schema Skew\n",
    "\n",
    "Schema skew occurs when the training and serving data do not conform to the same schema. Both training and serving data are expected to adhere to the same schema. Any expected deviations between the two (such as the label feature being only present in the training data but not in serving) should be specified through environments field in the schema.\n",
    "\n",
    "#### Feature Skew\n",
    "\n",
    "Feature skew occurs when the feature values that a model trains on are different from the feature values that it sees at serving time. For example, this can happen when:\n",
    "\n",
    "* A data source that provides some feature values is modified between training and serving time\n",
    "* There is different logic for generating features between training and serving. For example, if you apply some transformation only in one of the two code paths.\n",
    "\n",
    "#### Distribution Skew\n",
    "\n",
    "Distribution skew occurs when the distribution of the training dataset is significantly different from the distribution of the serving dataset. One of the key causes for distribution skew is using different code or different data sources to generate the training dataset. Another reason is a faulty sampling mechanism that chooses a non-representative subsample of the serving data to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:55.169208Z",
     "iopub.status.busy": "2023-07-28T11:06:55.168936Z",
     "iopub.status.idle": "2023-07-28T11:06:55.182401Z",
     "shell.execute_reply": "2023-07-28T11:06:55.181610Z"
    },
    "id": "wEUsZm_rOd1Q"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly short description</th>\n",
       "      <th>Anomaly long description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'company'</th>\n",
       "      <td>High Linfty distance between current and previous</td>\n",
       "      <td>The Linfty distance between current and previous is 0.00820891 (up to six significant digits), above the threshold 0.001. The feature value with maximum difference is: Blue Ribbon Taxi Association Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'payment_type'</th>\n",
       "      <td>High Linfty distance between training and serving</td>\n",
       "      <td>The Linfty distance between training and serving is 0.0225 (up to six significant digits), above the threshold 0.01. The feature value with maximum difference is: Credit Card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Anomaly short description  \\\n",
       "Feature name                                                        \n",
       "'company'       High Linfty distance between current and previous   \n",
       "'payment_type'  High Linfty distance between training and serving   \n",
       "\n",
       "                                                                                                                                                                                                 Anomaly long description  \n",
       "Feature name                                                                                                                                                                                                               \n",
       "'company'       The Linfty distance between current and previous is 0.00820891 (up to six significant digits), above the threshold 0.001. The feature value with maximum difference is: Blue Ribbon Taxi Association Inc.  \n",
       "'payment_type'                             The Linfty distance between training and serving is 0.0225 (up to six significant digits), above the threshold 0.01. The feature value with maximum difference is: Credit Card  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add skew comparator for 'payment_type' feature.\n",
    "payment_type = tfdv.get_feature(schema, 'payment_type')\n",
    "payment_type.skew_comparator.infinity_norm.threshold = 0.01\n",
    "\n",
    "# Add drift comparator for 'company' feature.\n",
    "company=tfdv.get_feature(schema, 'company')\n",
    "company.drift_comparator.infinity_norm.threshold = 0.001\n",
    "\n",
    "skew_anomalies = tfdv.validate_statistics(train_stats, schema,\n",
    "                                          previous_statistics=eval_stats,\n",
    "                                          serving_statistics=serving_stats)\n",
    "\n",
    "tfdv.display_anomalies(skew_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GzbbsPgf0Bg"
   },
   "source": [
    "In this example we do see some drift, but it is well below the threshold that we've set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ5saC9eWvHx"
   },
   "source": [
    "## Freeze the schema\n",
    "\n",
    "Now that the schema has been reviewed and curated, we will store it in a file to reflect its \"frozen\" state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T11:06:55.186273Z",
     "iopub.status.busy": "2023-07-28T11:06:55.185992Z",
     "iopub.status.idle": "2023-07-28T11:06:55.338659Z",
     "shell.execute_reply": "2023-07-28T11:06:55.337509Z"
    },
    "id": "ydkL4DkIWn18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\r\n",
      "  name: \"pickup_community_area\"\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"fare\"\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"trip_start_month\"\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"trip_start_hour\"\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"trip_start_day\"\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"trip_start_timestamp\"\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"pickup_latitude\"\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"pickup_longitude\"\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"dropoff_latitude\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"dropoff_longitude\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"trip_miles\"\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"pickup_census_tract\"\r\n",
      "  type: BYTES\r\n",
      "  presence {\r\n",
      "    min_count: 0\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"dropoff_census_tract\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"payment_type\"\r\n",
      "  type: BYTES\r\n",
      "  domain: \"payment_type\"\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  skew_comparator {\r\n",
      "    infinity_norm {\r\n",
      "      threshold: 0.01\r\n",
      "    }\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"company\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: BYTES\r\n",
      "  domain: \"company\"\r\n",
      "  presence {\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  distribution_constraints {\r\n",
      "    min_domain_mass: 0.9\r\n",
      "  }\r\n",
      "  drift_comparator {\r\n",
      "    infinity_norm {\r\n",
      "      threshold: 0.001\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"trip_seconds\"\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"dropoff_community_area\"\r\n",
      "  value_count {\r\n",
      "    min: 1\r\n",
      "    max: 1\r\n",
      "  }\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"tips\"\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  not_in_environment: \"SERVING\"\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "string_domain {\r\n",
      "  name: \"payment_type\"\r\n",
      "  value: \"Cash\"\r\n",
      "  value: \"Credit Card\"\r\n",
      "  value: \"Dispute\"\r\n",
      "  value: \"No Charge\"\r\n",
      "  value: \"Pcard\"\r\n",
      "  value: \"Unknown\"\r\n",
      "  value: \"Prcard\"\r\n",
      "}\r\n",
      "string_domain {\r\n",
      "  name: \"company\"\r\n",
      "  value: \"0118 - 42111 Godfrey S.Awir\"\r\n",
      "  value: \"0694 - 59280 Chinesco Trans Inc\"\r\n",
      "  value: \"1085 - 72312 N and W Cab Co\"\r\n",
      "  value: \"2733 - 74600 Benny Jona\"\r\n",
      "  value: \"2809 - 95474 C & D Cab Co Inc.\"\r\n",
      "  value: \"3011 - 66308 JBL Cab Inc.\"\r\n",
      "  value: \"3152 - 97284 Crystal Abernathy\"\r\n",
      "  value: \"3201 - C&D Cab Co Inc\"\r\n",
      "  value: \"3201 - CID Cab Co Inc\"\r\n",
      "  value: \"3253 - 91138 Gaither Cab Co.\"\r\n",
      "  value: \"3385 - 23210 Eman Cab\"\r\n",
      "  value: \"3623 - 72222 Arrington Enterprises\"\r\n",
      "  value: \"3897 - Ilie Malec\"\r\n",
      "  value: \"4053 - Adwar H. Nikola\"\r\n",
      "  value: \"4197 - 41842 Royal Star\"\r\n",
      "  value: \"4615 - 83503 Tyrone Henderson\"\r\n",
      "  value: \"4615 - Tyrone Henderson\"\r\n",
      "  value: \"4623 - Jay Kim\"\r\n",
      "  value: \"5006 - 39261 Salifu Bawa\"\r\n",
      "  value: \"5006 - Salifu Bawa\"\r\n",
      "  value: \"5074 - 54002 Ahzmi Inc\"\r\n",
      "  value: \"5074 - Ahzmi Inc\"\r\n",
      "  value: \"5129 - 87128\"\r\n",
      "  value: \"5129 - 98755 Mengisti Taxi\"\r\n",
      "  value: \"5129 - Mengisti Taxi\"\r\n",
      "  value: \"5724 - KYVI Cab Inc\"\r\n",
      "  value: \"585 - Valley Cab Co\"\r\n",
      "  value: \"5864 - 73614 Thomas Owusu\"\r\n",
      "  value: \"5864 - Thomas Owusu\"\r\n",
      "  value: \"5874 - 73628 Sergey Cab Corp.\"\r\n",
      "  value: \"5997 - 65283 AW Services Inc.\"\r\n",
      "  value: \"5997 - AW Services Inc.\"\r\n",
      "  value: \"6488 - 83287 Zuha Taxi\"\r\n",
      "  value: \"6743 - Luhak Corp\"\r\n",
      "  value: \"Blue Ribbon Taxi Association Inc.\"\r\n",
      "  value: \"C & D Cab Co Inc\"\r\n",
      "  value: \"Chicago Elite Cab Corp.\"\r\n",
      "  value: \"Chicago Elite Cab Corp. (Chicago Carriag\"\r\n",
      "  value: \"Chicago Medallion Leasing INC\"\r\n",
      "  value: \"Chicago Medallion Management\"\r\n",
      "  value: \"Choice Taxi Association\"\r\n",
      "  value: \"Dispatch Taxi Affiliation\"\r\n",
      "  value: \"KOAM Taxi Association\"\r\n",
      "  value: \"Northwest Management LLC\"\r\n",
      "  value: \"Taxi Affiliation Services\"\r\n",
      "  value: \"Top Cab Affiliation\"\r\n",
      "}\r\n",
      "default_environment: \"TRAINING\"\r\n",
      "default_environment: \"SERVING\"\r\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.lib.io import file_io\n",
    "from google.protobuf import text_format\n",
    "\n",
    "file_io.recursive_create_dir(OUTPUT_DIR)\n",
    "schema_file = os.path.join(OUTPUT_DIR, 'schema.pbtxt')\n",
    "tfdv.write_schema_text(schema, schema_file)\n",
    "\n",
    "!cat {schema_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8eC59yISdGB"
   },
   "source": [
    "## When to use TFDV\n",
    "\n",
    "It's easy to think of TFDV as only applying to the start of your training pipeline, as we did here, but in fact it has many uses.  Here's a few more:\n",
    "\n",
    "* Validating new data for inference to make sure that we haven't suddenly started receiving bad features\n",
    "* Validating new data for inference to make sure that our model has trained on that part of the decision surface\n",
    "* Validating our data after we've transformed it and done feature engineering (probably using [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started)) to make sure we haven't done something wrong"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tghWegsjhpkt"
   ],
   "name": "tfdv_basic.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
